{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search algorithms within Optuna\n",
    "\n",
    "\n",
    "\n",
    "We can select the search algorithm from the [optuna.study.create_study()](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html#optuna.study.create_study) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34579</th>\n",
       "      <td>35</td>\n",
       "      <td>admin.</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>92.893</td>\n",
       "      <td>-46.2</td>\n",
       "      <td>1.266</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>42</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>tue</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20173</th>\n",
       "      <td>36</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.444</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4.965</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18171</th>\n",
       "      <td>37</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jul</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.918</td>\n",
       "      <td>-42.7</td>\n",
       "      <td>4.963</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30128</th>\n",
       "      <td>31</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>apr</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>93.075</td>\n",
       "      <td>-47.1</td>\n",
       "      <td>1.365</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28870</th>\n",
       "      <td>45</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>apr</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>93.075</td>\n",
       "      <td>-47.1</td>\n",
       "      <td>1.410</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30452</th>\n",
       "      <td>60</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>92.893</td>\n",
       "      <td>-46.2</td>\n",
       "      <td>1.354</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28937</th>\n",
       "      <td>38</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>apr</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>93.075</td>\n",
       "      <td>-47.1</td>\n",
       "      <td>1.405</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16739</th>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>jul</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.918</td>\n",
       "      <td>-42.7</td>\n",
       "      <td>4.962</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16807</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jul</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.918</td>\n",
       "      <td>-42.7</td>\n",
       "      <td>4.962</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9280 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age          job  marital            education  default housing loan  \\\n",
       "34579   35       admin.   single    university.degree       no     yes   no   \n",
       "446     42   technician  married  professional.course       no      no   no   \n",
       "20173   36       admin.  married    university.degree       no      no   no   \n",
       "18171   37       admin.  married          high.school       no     yes  yes   \n",
       "30128   31   management   single    university.degree       no     yes   no   \n",
       "...    ...          ...      ...                  ...      ...     ...  ...   \n",
       "28870   45  blue-collar  married              unknown       no     yes   no   \n",
       "30452   60       admin.  married    university.degree       no     yes   no   \n",
       "28937   38  blue-collar  married             basic.9y       no     yes   no   \n",
       "16739   41   technician   single    university.degree       no      no   no   \n",
       "16807   56     services  married          high.school  unknown     yes   no   \n",
       "\n",
       "         contact month day_of_week  ...  campaign  pdays  previous  \\\n",
       "34579   cellular   may         thu  ...         1    999         1   \n",
       "446    telephone   may         tue  ...         1    999         0   \n",
       "20173   cellular   aug         mon  ...         2    999         0   \n",
       "18171  telephone   jul         wed  ...         2    999         0   \n",
       "30128   cellular   apr         thu  ...         1    999         0   \n",
       "...          ...   ...         ...  ...       ...    ...       ...   \n",
       "28870   cellular   apr         thu  ...         1    999         1   \n",
       "30452   cellular   may         mon  ...         2    999         0   \n",
       "28937   cellular   apr         fri  ...         1    999         1   \n",
       "16739   cellular   jul         thu  ...         2    999         0   \n",
       "16807  telephone   jul         thu  ...        11    999         0   \n",
       "\n",
       "          poutcome emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  \\\n",
       "34579      failure         -1.8          92.893          -46.2      1.266   \n",
       "446    nonexistent          1.1          93.994          -36.4      4.857   \n",
       "20173  nonexistent          1.4          93.444          -36.1      4.965   \n",
       "18171  nonexistent          1.4          93.918          -42.7      4.963   \n",
       "30128  nonexistent         -1.8          93.075          -47.1      1.365   \n",
       "...            ...          ...             ...            ...        ...   \n",
       "28870      failure         -1.8          93.075          -47.1      1.410   \n",
       "30452  nonexistent         -1.8          92.893          -46.2      1.354   \n",
       "28937      failure         -1.8          93.075          -47.1      1.405   \n",
       "16739  nonexistent          1.4          93.918          -42.7      4.962   \n",
       "16807  nonexistent          1.4          93.918          -42.7      4.962   \n",
       "\n",
       "       nr.employed    y  \n",
       "34579       5099.1   no  \n",
       "446         5191.0  yes  \n",
       "20173       5228.1  yes  \n",
       "18171       5228.1  yes  \n",
       "30128       5099.1   no  \n",
       "...            ...  ...  \n",
       "28870       5099.1  yes  \n",
       "30452       5099.1  yes  \n",
       "28937       5099.1   no  \n",
       "16739       5228.1   no  \n",
       "16807       5228.1   no  \n",
       "\n",
       "[9280 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('balanced_bank.csv', index_col = 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9280 entries, 34579 to 16807\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             9280 non-null   int64  \n",
      " 1   job             9280 non-null   object \n",
      " 2   marital         9280 non-null   object \n",
      " 3   education       9280 non-null   object \n",
      " 4   default         9280 non-null   object \n",
      " 5   housing         9280 non-null   object \n",
      " 6   loan            9280 non-null   object \n",
      " 7   contact         9280 non-null   object \n",
      " 8   month           9280 non-null   object \n",
      " 9   day_of_week     9280 non-null   object \n",
      " 10  duration        9280 non-null   int64  \n",
      " 11  campaign        9280 non-null   int64  \n",
      " 12  pdays           9280 non-null   int64  \n",
      " 13  previous        9280 non-null   int64  \n",
      " 14  poutcome        9280 non-null   object \n",
      " 15  emp.var.rate    9280 non-null   float64\n",
      " 16  cons.price.idx  9280 non-null   float64\n",
      " 17  cons.conf.idx   9280 non-null   float64\n",
      " 18  euribor3m       9280 non-null   float64\n",
      " 19  nr.employed     9280 non-null   float64\n",
      " 20  y               9280 non-null   object \n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34579</th>\n",
       "      <td>35</td>\n",
       "      <td>admin.</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>92.893</td>\n",
       "      <td>-46.2</td>\n",
       "      <td>1.266</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>42</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>professional.course</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>tue</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20173</th>\n",
       "      <td>36</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>aug</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.444</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4.965</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18171</th>\n",
       "      <td>37</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jul</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.918</td>\n",
       "      <td>-42.7</td>\n",
       "      <td>4.963</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30128</th>\n",
       "      <td>31</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>apr</td>\n",
       "      <td>thu</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>93.075</td>\n",
       "      <td>-47.1</td>\n",
       "      <td>1.365</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         job  marital            education default housing loan  \\\n",
       "34579   35      admin.   single    university.degree      no     yes   no   \n",
       "446     42  technician  married  professional.course      no      no   no   \n",
       "20173   36      admin.  married    university.degree      no      no   no   \n",
       "18171   37      admin.  married          high.school      no     yes  yes   \n",
       "30128   31  management   single    university.degree      no     yes   no   \n",
       "\n",
       "         contact month day_of_week  ...  campaign  pdays  previous  \\\n",
       "34579   cellular   may         thu  ...         1    999         1   \n",
       "446    telephone   may         tue  ...         1    999         0   \n",
       "20173   cellular   aug         mon  ...         2    999         0   \n",
       "18171  telephone   jul         wed  ...         2    999         0   \n",
       "30128   cellular   apr         thu  ...         1    999         0   \n",
       "\n",
       "          poutcome emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  \\\n",
       "34579      failure         -1.8          92.893          -46.2      1.266   \n",
       "446    nonexistent          1.1          93.994          -36.4      4.857   \n",
       "20173  nonexistent          1.4          93.444          -36.1      4.965   \n",
       "18171  nonexistent          1.4          93.918          -42.7      4.963   \n",
       "30128  nonexistent         -1.8          93.075          -47.1      1.365   \n",
       "\n",
       "       nr.employed    y  \n",
       "34579       5099.1   no  \n",
       "446         5191.0  yes  \n",
       "20173       5228.1  yes  \n",
       "18171       5228.1  yes  \n",
       "30128       5099.1   no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
       "       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n",
       "       'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
       "       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no         7691\n",
       "yes        1371\n",
       "unknown     218\n",
       "Name: loan, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loan.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp.var.rate      0\n",
       "cons.price.idx    0\n",
       "cons.conf.idx     0\n",
       "euribor3m         0\n",
       "nr.employed       0\n",
       "y                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['job','marital','education','month','day_of_week'] \n",
    "data_encoded = pd.get_dummies(data, drop_first=True,columns = categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cat_cols = ['contact','housing','loan','poutcome','default']\n",
    "\n",
    "for i in binary_cat_cols:\n",
    "    if len(data_encoded[i].unique())==2:\n",
    "        data_encoded[i].replace({data_encoded[i].unique()[0]: 0, data_encoded[i].unique()[1]: 1, 'unknown':-1}, inplace=True)\n",
    "\n",
    "    else:\n",
    "        data_encoded[i].replace({data_encoded[i].unique()[0]: 0, data_encoded[i].unique()[1]: 1, data_encoded[i].unique()[2]: 2, 'unknown':-1}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_encoded['y'] = label_encoder.fit_transform(data_encoded['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                                int64\n",
       "default                            int64\n",
       "housing                            int64\n",
       "loan                               int64\n",
       "contact                            int64\n",
       "duration                           int64\n",
       "campaign                           int64\n",
       "pdays                              int64\n",
       "previous                           int64\n",
       "poutcome                           int64\n",
       "emp.var.rate                     float64\n",
       "cons.price.idx                   float64\n",
       "cons.conf.idx                    float64\n",
       "euribor3m                        float64\n",
       "nr.employed                      float64\n",
       "y                                  int32\n",
       "job_blue-collar                    uint8\n",
       "job_entrepreneur                   uint8\n",
       "job_housemaid                      uint8\n",
       "job_management                     uint8\n",
       "job_retired                        uint8\n",
       "job_self-employed                  uint8\n",
       "job_services                       uint8\n",
       "job_student                        uint8\n",
       "job_technician                     uint8\n",
       "job_unemployed                     uint8\n",
       "job_unknown                        uint8\n",
       "marital_married                    uint8\n",
       "marital_single                     uint8\n",
       "marital_unknown                    uint8\n",
       "education_basic.6y                 uint8\n",
       "education_basic.9y                 uint8\n",
       "education_high.school              uint8\n",
       "education_illiterate               uint8\n",
       "education_professional.course      uint8\n",
       "education_university.degree        uint8\n",
       "education_unknown                  uint8\n",
       "month_aug                          uint8\n",
       "month_dec                          uint8\n",
       "month_jul                          uint8\n",
       "month_jun                          uint8\n",
       "month_mar                          uint8\n",
       "month_may                          uint8\n",
       "month_nov                          uint8\n",
       "month_oct                          uint8\n",
       "month_sep                          uint8\n",
       "day_of_week_mon                    uint8\n",
       "day_of_week_thu                    uint8\n",
       "day_of_week_tue                    uint8\n",
       "day_of_week_wed                    uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9280, 50)\n",
      "(9280, 49)\n",
      "(9280,)\n"
     ]
    }
   ],
   "source": [
    "print(data_encoded.shape)\n",
    "\n",
    "X = data_encoded.loc[:,data_encoded.columns!='y']\n",
    "print(X.shape)\n",
    "\n",
    "y = data_encoded.loc[:,'y']\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8854064642507345"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(nthread=-1)  \n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter optimization with Optuna\n",
    "\n",
    "In this notebook, I will demo how to select the search algorithm with Optuna. We will compare the use of:\n",
    "\n",
    "- Randomized search\n",
    "- Tree-structured Parzen Estimators\n",
    "- CMA-ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function\n",
    "\n",
    "This is the hyperparameter response space, the function we want to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    \n",
    "    param = {\n",
    "        \"objective\": 'binary:logistic',\n",
    "        'reg_lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'gamma':trial.suggest_loguniform('gamma', 1e-3,1),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [i/10.0 for i in range(4,11)]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [i/10.0 for i in range(4,11)]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02,0.300000012]),\n",
    "        'n_estimators': trial.suggest_int('n_estimators',100,500),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5,6,7,9,11,13,15,17,20]),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param, nthread=-1)  \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:20:31,616]\u001b[0m A new study created in memory with name: no-name-e6a918c6-54ae-4b43-8177-65ab0a3f90f4\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:31,631]\u001b[0m A new study created in memory with name: no-name-e12436fe-5f8c-4faa-9499-67bb083d2f3b\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:31,634]\u001b[0m A new study created in memory with name: no-name-2d39f5ef-7326-4545-872b-e6dfe4683366\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:33,695]\u001b[0m Trial 0 finished with value: 0.871041462618348 and parameters: {'lambda': 0.0024397246556234964, 'alpha': 0.007382954871191506, 'gamma': 0.04316971959666753, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.014, 'n_estimators': 395, 'max_depth': 11, 'min_child_weight': 213}. Best is trial 0 with value: 0.871041462618348.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:33,736]\u001b[0m The parameter 'colsample_bytree' in trial#1 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:33,736]\u001b[0m The parameter 'subsample' in trial#1 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:33,736]\u001b[0m The parameter 'learning_rate' in trial#1 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:33,736]\u001b[0m The parameter 'max_depth' in trial#1 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:35,625]\u001b[0m Trial 1 finished with value: 0.8628795298726738 and parameters: {'lambda': 0.19904672176382754, 'alpha': 0.014330135407775661, 'gamma': 0.14624160110148507, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.009, 'n_estimators': 299, 'max_depth': 20, 'min_child_weight': 151}. Best is trial 0 with value: 0.871041462618348.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:35,625]\u001b[0m The parameter 'colsample_bytree' in trial#2 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:35,625]\u001b[0m The parameter 'subsample' in trial#2 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:35,633]\u001b[0m The parameter 'learning_rate' in trial#2 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:35,635]\u001b[0m The parameter 'max_depth' in trial#2 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:37,169]\u001b[0m Trial 2 finished with value: 0.871694417238002 and parameters: {'lambda': 0.03732961357474213, 'alpha': 0.07820918795584841, 'gamma': 0.05029266860699954, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 299, 'max_depth': 13, 'min_child_weight': 151}. Best is trial 2 with value: 0.871694417238002.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:37,177]\u001b[0m The parameter 'colsample_bytree' in trial#3 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:37,179]\u001b[0m The parameter 'subsample' in trial#3 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:37,179]\u001b[0m The parameter 'learning_rate' in trial#3 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:37,179]\u001b[0m The parameter 'max_depth' in trial#3 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:39,222]\u001b[0m Trial 3 finished with value: 0.8700620306888671 and parameters: {'lambda': 0.3835629614110805, 'alpha': 0.3219478397643868, 'gamma': 0.19632656004870355, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.02, 'n_estimators': 300, 'max_depth': 17, 'min_child_weight': 149}. Best is trial 2 with value: 0.871694417238002.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:39,230]\u001b[0m The parameter 'colsample_bytree' in trial#4 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:39,232]\u001b[0m The parameter 'subsample' in trial#4 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:20:39,232]\u001b[0m The parameter 'learning_rate' in trial#4 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:39,232]\u001b[0m The parameter 'max_depth' in trial#4 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:40,961]\u001b[0m Trial 4 finished with value: 0.8566764609859615 and parameters: {'lambda': 0.17565209709760063, 'alpha': 0.5389875960213921, 'gamma': 0.06442330067467497, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.016, 'n_estimators': 299, 'max_depth': 20, 'min_child_weight': 150}. Best is trial 2 with value: 0.871694417238002.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:40,971]\u001b[0m The parameter 'colsample_bytree' in trial#5 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:40,971]\u001b[0m The parameter 'subsample' in trial#5 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:40,971]\u001b[0m The parameter 'learning_rate' in trial#5 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:40,971]\u001b[0m The parameter 'max_depth' in trial#5 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:42,488]\u001b[0m Trial 5 finished with value: 0.8681031668299053 and parameters: {'lambda': 0.004425902990740224, 'alpha': 0.353850977481585, 'gamma': 0.018644400609703286, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.016, 'n_estimators': 300, 'max_depth': 9, 'min_child_weight': 150}. Best is trial 2 with value: 0.871694417238002.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:42,498]\u001b[0m The parameter 'colsample_bytree' in trial#6 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:42,498]\u001b[0m The parameter 'subsample' in trial#6 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:42,498]\u001b[0m The parameter 'learning_rate' in trial#6 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:42,498]\u001b[0m The parameter 'max_depth' in trial#6 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:44,562]\u001b[0m Trial 6 finished with value: 0.8570029382957884 and parameters: {'lambda': 0.12098373160432328, 'alpha': 0.12933173482518873, 'gamma': 0.1477091682462579, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.008, 'n_estimators': 299, 'max_depth': 17, 'min_child_weight': 151}. Best is trial 2 with value: 0.871694417238002.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:44,572]\u001b[0m The parameter 'colsample_bytree' in trial#7 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:44,572]\u001b[0m The parameter 'subsample' in trial#7 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:44,572]\u001b[0m The parameter 'learning_rate' in trial#7 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:44,572]\u001b[0m The parameter 'max_depth' in trial#7 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:46,321]\u001b[0m Trial 7 finished with value: 0.8726738491674828 and parameters: {'lambda': 0.018649235157683613, 'alpha': 0.21979369731050355, 'gamma': 0.0156547269755242, 'colsample_bytree': 0.9, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 299, 'max_depth': 15, 'min_child_weight': 149}. Best is trial 7 with value: 0.8726738491674828.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:46,323]\u001b[0m The parameter 'colsample_bytree' in trial#8 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:20:46,323]\u001b[0m The parameter 'subsample' in trial#8 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:46,331]\u001b[0m The parameter 'learning_rate' in trial#8 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:46,333]\u001b[0m The parameter 'max_depth' in trial#8 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:48,225]\u001b[0m Trial 8 finished with value: 0.8628795298726738 and parameters: {'lambda': 0.12523246132437987, 'alpha': 0.09264155864599624, 'gamma': 0.014447727407753795, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 300, 'max_depth': 20, 'min_child_weight': 149}. Best is trial 7 with value: 0.8726738491674828.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:48,235]\u001b[0m The parameter 'colsample_bytree' in trial#9 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:48,235]\u001b[0m The parameter 'subsample' in trial#9 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:48,235]\u001b[0m The parameter 'learning_rate' in trial#9 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:48,235]\u001b[0m The parameter 'max_depth' in trial#9 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:49,963]\u001b[0m Trial 9 finished with value: 0.8697355533790402 and parameters: {'lambda': 0.12482622925065384, 'alpha': 0.38661250324689694, 'gamma': 0.0522301456630472, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.02, 'n_estimators': 300, 'max_depth': 5, 'min_child_weight': 150}. Best is trial 7 with value: 0.8726738491674828.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:49,963]\u001b[0m The parameter 'colsample_bytree' in trial#10 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:49,973]\u001b[0m The parameter 'subsample' in trial#10 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:49,973]\u001b[0m The parameter 'learning_rate' in trial#10 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:49,973]\u001b[0m The parameter 'max_depth' in trial#10 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:51,942]\u001b[0m Trial 10 finished with value: 0.8641854391119818 and parameters: {'lambda': 0.23002495230317546, 'alpha': 0.10986536463884568, 'gamma': 0.02770873963237148, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.01, 'n_estimators': 299, 'max_depth': 13, 'min_child_weight': 150}. Best is trial 7 with value: 0.8726738491674828.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:51,951]\u001b[0m The parameter 'colsample_bytree' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:51,951]\u001b[0m The parameter 'subsample' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:51,953]\u001b[0m The parameter 'learning_rate' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:51,953]\u001b[0m The parameter 'max_depth' in trial#11 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:53,527]\u001b[0m Trial 11 finished with value: 0.8684296441397323 and parameters: {'lambda': 0.10558036964524253, 'alpha': 0.5225515829033104, 'gamma': 0.09796356997393661, 'colsample_bytree': 0.5, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 300, 'max_depth': 5, 'min_child_weight': 150}. Best is trial 7 with value: 0.8726738491674828.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:20:53,529]\u001b[0m The parameter 'colsample_bytree' in trial#12 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:53,529]\u001b[0m The parameter 'subsample' in trial#12 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:53,529]\u001b[0m The parameter 'learning_rate' in trial#12 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:53,537]\u001b[0m The parameter 'max_depth' in trial#12 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:55,416]\u001b[0m Trial 12 finished with value: 0.8645119164218087 and parameters: {'lambda': 0.05463051148580205, 'alpha': 0.0489521779942131, 'gamma': 0.22201925240460568, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 299, 'max_depth': 11, 'min_child_weight': 150}. Best is trial 7 with value: 0.8726738491674828.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:55,426]\u001b[0m The parameter 'colsample_bytree' in trial#13 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:55,426]\u001b[0m The parameter 'subsample' in trial#13 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:55,426]\u001b[0m The parameter 'learning_rate' in trial#13 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:55,426]\u001b[0m The parameter 'max_depth' in trial#13 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:56,994]\u001b[0m Trial 13 finished with value: 0.8756121449559255 and parameters: {'lambda': 0.025689568946503285, 'alpha': 0.17814788707014284, 'gamma': 0.014904918711001419, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 298, 'max_depth': 11, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:57,002]\u001b[0m The parameter 'colsample_bytree' in trial#14 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:57,004]\u001b[0m The parameter 'subsample' in trial#14 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:57,004]\u001b[0m The parameter 'learning_rate' in trial#14 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:57,004]\u001b[0m The parameter 'max_depth' in trial#14 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:20:59,040]\u001b[0m Trial 14 finished with value: 0.8687561214495593 and parameters: {'lambda': 0.05130034692666741, 'alpha': 0.24637313705228733, 'gamma': 0.07273123604623276, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 298, 'max_depth': 6, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:59,042]\u001b[0m The parameter 'colsample_bytree' in trial#15 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:59,042]\u001b[0m The parameter 'subsample' in trial#15 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:59,042]\u001b[0m The parameter 'learning_rate' in trial#15 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:20:59,042]\u001b[0m The parameter 'max_depth' in trial#15 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:21:01,118]\u001b[0m Trial 15 finished with value: 0.8677766895200784 and parameters: {'lambda': 0.08237934266337686, 'alpha': 0.06922198290147676, 'gamma': 0.018010658245755644, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.012, 'n_estimators': 299, 'max_depth': 13, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:01,128]\u001b[0m The parameter 'colsample_bytree' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:01,128]\u001b[0m The parameter 'subsample' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:01,128]\u001b[0m The parameter 'learning_rate' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:01,136]\u001b[0m The parameter 'max_depth' in trial#16 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:02,618]\u001b[0m Trial 16 finished with value: 0.8488410055501142 and parameters: {'lambda': 0.01106675153310806, 'alpha': 0.38470317568755075, 'gamma': 0.28581415902012625, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.01, 'n_estimators': 299, 'max_depth': 5, 'min_child_weight': 151}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:02,626]\u001b[0m The parameter 'colsample_bytree' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:02,628]\u001b[0m The parameter 'subsample' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:02,628]\u001b[0m The parameter 'learning_rate' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:02,628]\u001b[0m The parameter 'max_depth' in trial#17 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:03,983]\u001b[0m Trial 17 finished with value: 0.8739797584067908 and parameters: {'lambda': 0.10652402980180259, 'alpha': 0.2405169298627021, 'gamma': 0.11687539682802504, 'colsample_bytree': 0.4, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 298, 'max_depth': 13, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:03,993]\u001b[0m The parameter 'colsample_bytree' in trial#18 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:03,993]\u001b[0m The parameter 'subsample' in trial#18 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:03,993]\u001b[0m The parameter 'learning_rate' in trial#18 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:03,993]\u001b[0m The parameter 'max_depth' in trial#18 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:05,772]\u001b[0m Trial 18 finished with value: 0.8566764609859615 and parameters: {'lambda': 0.015228953178787674, 'alpha': 0.16132862460799066, 'gamma': 0.015004164429109545, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.016, 'n_estimators': 298, 'max_depth': 5, 'min_child_weight': 148}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:05,780]\u001b[0m The parameter 'colsample_bytree' in trial#19 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:05,782]\u001b[0m The parameter 'subsample' in trial#19 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:05,782]\u001b[0m The parameter 'learning_rate' in trial#19 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:21:05,782]\u001b[0m The parameter 'max_depth' in trial#19 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:07,348]\u001b[0m Trial 19 finished with value: 0.85145282402873 and parameters: {'lambda': 0.33078066516465676, 'alpha': 0.0840604449937396, 'gamma': 0.030386771558898244, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.02, 'n_estimators': 298, 'max_depth': 6, 'min_child_weight': 151}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:07,350]\u001b[0m The parameter 'colsample_bytree' in trial#20 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:07,350]\u001b[0m The parameter 'subsample' in trial#20 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:07,350]\u001b[0m The parameter 'learning_rate' in trial#20 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:07,350]\u001b[0m The parameter 'max_depth' in trial#20 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:09,151]\u001b[0m Trial 20 finished with value: 0.8707149853085211 and parameters: {'lambda': 0.04558751113592101, 'alpha': 0.08005481439526446, 'gamma': 0.026042325398617196, 'colsample_bytree': 0.5, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 299, 'max_depth': 11, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:09,162]\u001b[0m The parameter 'colsample_bytree' in trial#21 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:09,162]\u001b[0m The parameter 'subsample' in trial#21 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:09,162]\u001b[0m The parameter 'learning_rate' in trial#21 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:09,162]\u001b[0m The parameter 'max_depth' in trial#21 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:11,136]\u001b[0m Trial 21 finished with value: 0.861247143323539 and parameters: {'lambda': 0.07126819723676932, 'alpha': 0.05863979912160843, 'gamma': 0.003478181871086246, 'colsample_bytree': 0.6, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 296, 'max_depth': 20, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:11,146]\u001b[0m The parameter 'colsample_bytree' in trial#22 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:11,146]\u001b[0m The parameter 'subsample' in trial#22 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:11,146]\u001b[0m The parameter 'learning_rate' in trial#22 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:11,146]\u001b[0m The parameter 'max_depth' in trial#22 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:13,019]\u001b[0m Trial 22 finished with value: 0.861900097943193 and parameters: {'lambda': 0.01857999620651971, 'alpha': 0.26698109559017424, 'gamma': 0.027078418973433868, 'colsample_bytree': 0.5, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 300, 'max_depth': 20, 'min_child_weight': 152}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:13,021]\u001b[0m The parameter 'colsample_bytree' in trial#23 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:13,021]\u001b[0m The parameter 'subsample' in trial#23 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:21:13,029]\u001b[0m The parameter 'learning_rate' in trial#23 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:13,031]\u001b[0m The parameter 'max_depth' in trial#23 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:14,925]\u001b[0m Trial 23 finished with value: 0.8638589618021547 and parameters: {'lambda': 0.24972410041327198, 'alpha': 0.4135384166120134, 'gamma': 0.04889423263678402, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.008, 'n_estimators': 299, 'max_depth': 11, 'min_child_weight': 150}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:14,925]\u001b[0m The parameter 'colsample_bytree' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:14,925]\u001b[0m The parameter 'subsample' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:14,933]\u001b[0m The parameter 'learning_rate' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:14,935]\u001b[0m The parameter 'max_depth' in trial#24 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:16,977]\u001b[0m Trial 24 finished with value: 0.8700620306888671 and parameters: {'lambda': 0.03267193380824136, 'alpha': 0.4573481670898536, 'gamma': 0.06434633487703872, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.02, 'n_estimators': 298, 'max_depth': 11, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:16,987]\u001b[0m The parameter 'colsample_bytree' in trial#25 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:16,987]\u001b[0m The parameter 'subsample' in trial#25 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:16,987]\u001b[0m The parameter 'learning_rate' in trial#25 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:16,987]\u001b[0m The parameter 'max_depth' in trial#25 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:18,717]\u001b[0m Trial 25 finished with value: 0.8746327130264446 and parameters: {'lambda': 0.08570651032679194, 'alpha': 0.16740821135425937, 'gamma': 0.028706888399281098, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 297, 'max_depth': 17, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:18,727]\u001b[0m The parameter 'colsample_bytree' in trial#26 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:18,727]\u001b[0m The parameter 'subsample' in trial#26 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:18,727]\u001b[0m The parameter 'learning_rate' in trial#26 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:18,727]\u001b[0m The parameter 'max_depth' in trial#26 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:20,326]\u001b[0m Trial 26 finished with value: 0.871367939928175 and parameters: {'lambda': 0.043258554331472195, 'alpha': 0.0956848668885918, 'gamma': 0.12596993855639113, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.02, 'n_estimators': 297, 'max_depth': 11, 'min_child_weight': 150}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:20,326]\u001b[0m The parameter 'colsample_bytree' in trial#27 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:21:20,336]\u001b[0m The parameter 'subsample' in trial#27 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:20,336]\u001b[0m The parameter 'learning_rate' in trial#27 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:20,336]\u001b[0m The parameter 'max_depth' in trial#27 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:22,226]\u001b[0m Trial 27 finished with value: 0.861900097943193 and parameters: {'lambda': 0.05250524259600229, 'alpha': 0.29668984978340085, 'gamma': 0.1067430002264812, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.009, 'n_estimators': 298, 'max_depth': 5, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:22,237]\u001b[0m The parameter 'colsample_bytree' in trial#28 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:22,237]\u001b[0m The parameter 'subsample' in trial#28 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:22,237]\u001b[0m The parameter 'learning_rate' in trial#28 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:22,245]\u001b[0m The parameter 'max_depth' in trial#28 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:23,979]\u001b[0m Trial 28 finished with value: 0.8720208945478289 and parameters: {'lambda': 0.060096472896798175, 'alpha': 0.33408688763875144, 'gamma': 0.19153414292957466, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.016, 'n_estimators': 299, 'max_depth': 6, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:23,989]\u001b[0m The parameter 'colsample_bytree' in trial#29 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:23,989]\u001b[0m The parameter 'subsample' in trial#29 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:23,989]\u001b[0m The parameter 'learning_rate' in trial#29 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:23,989]\u001b[0m The parameter 'max_depth' in trial#29 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:26,235]\u001b[0m Trial 29 finished with value: 0.8687561214495593 and parameters: {'lambda': 0.1887116333471148, 'alpha': 0.1522861561185572, 'gamma': 0.042618216721582725, 'colsample_bytree': 0.9, 'subsample': 1.0, 'learning_rate': 0.008, 'n_estimators': 297, 'max_depth': 17, 'min_child_weight': 150}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:26,245]\u001b[0m The parameter 'colsample_bytree' in trial#30 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:26,245]\u001b[0m The parameter 'subsample' in trial#30 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:26,245]\u001b[0m The parameter 'learning_rate' in trial#30 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:26,245]\u001b[0m The parameter 'max_depth' in trial#30 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:28,721]\u001b[0m Trial 30 finished with value: 0.8677766895200784 and parameters: {'lambda': 0.05807670927055555, 'alpha': 0.36246058735544606, 'gamma': 0.12633080672303157, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 298, 'max_depth': 20, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:21:28,721]\u001b[0m The parameter 'colsample_bytree' in trial#31 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:28,729]\u001b[0m The parameter 'subsample' in trial#31 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:28,731]\u001b[0m The parameter 'learning_rate' in trial#31 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:28,731]\u001b[0m The parameter 'max_depth' in trial#31 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:30,757]\u001b[0m Trial 31 finished with value: 0.8498204374795951 and parameters: {'lambda': 0.0993562477019671, 'alpha': 0.07431347571552879, 'gamma': 0.04629880794969977, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.008, 'n_estimators': 301, 'max_depth': 17, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:30,767]\u001b[0m The parameter 'colsample_bytree' in trial#32 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:30,767]\u001b[0m The parameter 'subsample' in trial#32 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:30,767]\u001b[0m The parameter 'learning_rate' in trial#32 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:30,767]\u001b[0m The parameter 'max_depth' in trial#32 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:32,540]\u001b[0m Trial 32 finished with value: 0.8488410055501142 and parameters: {'lambda': 0.049118193156400446, 'alpha': 0.8098775369769997, 'gamma': 0.03506328047704602, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.012, 'n_estimators': 299, 'max_depth': 20, 'min_child_weight': 148}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:32,542]\u001b[0m The parameter 'colsample_bytree' in trial#33 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:32,542]\u001b[0m The parameter 'subsample' in trial#33 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:32,542]\u001b[0m The parameter 'learning_rate' in trial#33 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:32,553]\u001b[0m The parameter 'max_depth' in trial#33 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:34,624]\u001b[0m Trial 33 finished with value: 0.8700620306888671 and parameters: {'lambda': 0.08024648304341682, 'alpha': 0.4124485620645373, 'gamma': 0.04215905674624258, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.016, 'n_estimators': 298, 'max_depth': 5, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:34,632]\u001b[0m The parameter 'colsample_bytree' in trial#34 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:34,634]\u001b[0m The parameter 'subsample' in trial#34 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:34,634]\u001b[0m The parameter 'learning_rate' in trial#34 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:34,634]\u001b[0m The parameter 'max_depth' in trial#34 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:21:36,605]\u001b[0m Trial 34 finished with value: 0.8651648710414627 and parameters: {'lambda': 0.08248218496003933, 'alpha': 0.11302791891057633, 'gamma': 0.020203916676034755, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.012, 'n_estimators': 298, 'max_depth': 17, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:36,611]\u001b[0m The parameter 'colsample_bytree' in trial#35 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:36,613]\u001b[0m The parameter 'subsample' in trial#35 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:36,615]\u001b[0m The parameter 'learning_rate' in trial#35 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:36,617]\u001b[0m The parameter 'max_depth' in trial#35 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:38,545]\u001b[0m Trial 35 finished with value: 0.8609206660137121 and parameters: {'lambda': 0.04693456129240169, 'alpha': 0.05371733578894994, 'gamma': 0.08745743271745211, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.300000012, 'n_estimators': 297, 'max_depth': 6, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:38,550]\u001b[0m The parameter 'colsample_bytree' in trial#36 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:38,552]\u001b[0m The parameter 'subsample' in trial#36 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:38,553]\u001b[0m The parameter 'learning_rate' in trial#36 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:38,555]\u001b[0m The parameter 'max_depth' in trial#36 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:39,905]\u001b[0m Trial 36 finished with value: 0.8527587332680379 and parameters: {'lambda': 0.030495980389083886, 'alpha': 0.4478487785848505, 'gamma': 0.04676195012780979, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 299, 'max_depth': 15, 'min_child_weight': 150}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:39,905]\u001b[0m The parameter 'colsample_bytree' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:39,905]\u001b[0m The parameter 'subsample' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:39,915]\u001b[0m The parameter 'learning_rate' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:39,915]\u001b[0m The parameter 'max_depth' in trial#37 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:41,212]\u001b[0m Trial 37 finished with value: 0.871367939928175 and parameters: {'lambda': 0.28027390113372624, 'alpha': 0.7205984657744103, 'gamma': 0.04648448330125342, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.300000012, 'n_estimators': 297, 'max_depth': 15, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:41,217]\u001b[0m The parameter 'colsample_bytree' in trial#38 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:41,219]\u001b[0m The parameter 'subsample' in trial#38 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:41,220]\u001b[0m The parameter 'learning_rate' in trial#38 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:21:41,222]\u001b[0m The parameter 'max_depth' in trial#38 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:42,900]\u001b[0m Trial 38 finished with value: 0.871367939928175 and parameters: {'lambda': 2.3951300173956764, 'alpha': 0.20429264598812372, 'gamma': 0.10471410595616715, 'colsample_bytree': 0.4, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 298, 'max_depth': 13, 'min_child_weight': 148}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:42,905]\u001b[0m The parameter 'colsample_bytree' in trial#39 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:42,907]\u001b[0m The parameter 'subsample' in trial#39 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:42,909]\u001b[0m The parameter 'learning_rate' in trial#39 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:42,911]\u001b[0m The parameter 'max_depth' in trial#39 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:45,199]\u001b[0m Trial 39 finished with value: 0.851779301338557 and parameters: {'lambda': 0.02265013285665763, 'alpha': 1.3480296837822632, 'gamma': 0.0536058674734749, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.016, 'n_estimators': 297, 'max_depth': 5, 'min_child_weight': 148}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:45,205]\u001b[0m The parameter 'colsample_bytree' in trial#40 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:45,206]\u001b[0m The parameter 'subsample' in trial#40 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:45,208]\u001b[0m The parameter 'learning_rate' in trial#40 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:45,210]\u001b[0m The parameter 'max_depth' in trial#40 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:47,229]\u001b[0m Trial 40 finished with value: 0.8746327130264446 and parameters: {'lambda': 0.2135108890729461, 'alpha': 0.48952724615923754, 'gamma': 0.10562981724394546, 'colsample_bytree': 0.9, 'subsample': 1.0, 'learning_rate': 0.02, 'n_estimators': 296, 'max_depth': 6, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:47,236]\u001b[0m The parameter 'colsample_bytree' in trial#41 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:47,238]\u001b[0m The parameter 'subsample' in trial#41 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:47,239]\u001b[0m The parameter 'learning_rate' in trial#41 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:47,241]\u001b[0m The parameter 'max_depth' in trial#41 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:50,762]\u001b[0m Trial 41 finished with value: 0.8651648710414627 and parameters: {'lambda': 0.49745417359934946, 'alpha': 0.15023078755178987, 'gamma': 0.10129854445115621, 'colsample_bytree': 0.8, 'subsample': 0.9, 'learning_rate': 0.01, 'n_estimators': 299, 'max_depth': 13, 'min_child_weight': 150}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:50,768]\u001b[0m The parameter 'colsample_bytree' in trial#42 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:50,770]\u001b[0m The parameter 'subsample' in trial#42 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:21:50,772]\u001b[0m The parameter 'learning_rate' in trial#42 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:50,773]\u001b[0m The parameter 'max_depth' in trial#42 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:53,984]\u001b[0m Trial 42 finished with value: 0.8733268037871368 and parameters: {'lambda': 0.9264421842510355, 'alpha': 0.33718036733045814, 'gamma': 0.13482178059196215, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 297, 'max_depth': 7, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:53,990]\u001b[0m The parameter 'colsample_bytree' in trial#43 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:53,992]\u001b[0m The parameter 'subsample' in trial#43 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:53,994]\u001b[0m The parameter 'learning_rate' in trial#43 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:53,996]\u001b[0m The parameter 'max_depth' in trial#43 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:21:59,232]\u001b[0m Trial 43 finished with value: 0.8694090760692131 and parameters: {'lambda': 0.09963696150861515, 'alpha': 0.38658516341496296, 'gamma': 0.6346900501265591, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.02, 'n_estimators': 296, 'max_depth': 17, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:59,239]\u001b[0m The parameter 'colsample_bytree' in trial#44 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:59,241]\u001b[0m The parameter 'subsample' in trial#44 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:59,244]\u001b[0m The parameter 'learning_rate' in trial#44 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:21:59,246]\u001b[0m The parameter 'max_depth' in trial#44 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:03,970]\u001b[0m Trial 44 finished with value: 0.8635324844923278 and parameters: {'lambda': 0.05164593929006459, 'alpha': 0.38745658898919255, 'gamma': 0.05572763786666983, 'colsample_bytree': 0.8, 'subsample': 0.9, 'learning_rate': 0.01, 'n_estimators': 295, 'max_depth': 9, 'min_child_weight': 148}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:03,980]\u001b[0m The parameter 'colsample_bytree' in trial#45 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:03,980]\u001b[0m The parameter 'subsample' in trial#45 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:03,980]\u001b[0m The parameter 'learning_rate' in trial#45 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:03,980]\u001b[0m The parameter 'max_depth' in trial#45 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:05,781]\u001b[0m Trial 45 finished with value: 0.8733268037871368 and parameters: {'lambda': 0.3107849806163704, 'alpha': 0.33712192460984813, 'gamma': 0.04648080902516324, 'colsample_bytree': 0.5, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 297, 'max_depth': 9, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:05,786]\u001b[0m The parameter 'colsample_bytree' in trial#46 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:22:05,788]\u001b[0m The parameter 'subsample' in trial#46 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:05,789]\u001b[0m The parameter 'learning_rate' in trial#46 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:05,791]\u001b[0m The parameter 'max_depth' in trial#46 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:08,163]\u001b[0m Trial 46 finished with value: 0.8560235063663075 and parameters: {'lambda': 0.023323312658049912, 'alpha': 0.38088620395532546, 'gamma': 0.5728065099650556, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.008, 'n_estimators': 298, 'max_depth': 15, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:08,165]\u001b[0m The parameter 'colsample_bytree' in trial#47 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:08,165]\u001b[0m The parameter 'subsample' in trial#47 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:08,173]\u001b[0m The parameter 'learning_rate' in trial#47 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:08,173]\u001b[0m The parameter 'max_depth' in trial#47 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:09,787]\u001b[0m Trial 47 finished with value: 0.8511263467189031 and parameters: {'lambda': 0.43856691789045515, 'alpha': 0.2749817718701611, 'gamma': 0.019517139411564244, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.014, 'n_estimators': 299, 'max_depth': 6, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:09,789]\u001b[0m The parameter 'colsample_bytree' in trial#48 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:09,789]\u001b[0m The parameter 'subsample' in trial#48 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:09,789]\u001b[0m The parameter 'learning_rate' in trial#48 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:09,797]\u001b[0m The parameter 'max_depth' in trial#48 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:11,629]\u001b[0m Trial 48 finished with value: 0.8635324844923278 and parameters: {'lambda': 0.16031514362721994, 'alpha': 0.5611735640827578, 'gamma': 0.0791829286045585, 'colsample_bytree': 1.0, 'subsample': 0.4, 'learning_rate': 0.300000012, 'n_estimators': 292, 'max_depth': 13, 'min_child_weight': 145}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:11,640]\u001b[0m The parameter 'colsample_bytree' in trial#49 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:11,640]\u001b[0m The parameter 'subsample' in trial#49 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:11,640]\u001b[0m The parameter 'learning_rate' in trial#49 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:11,648]\u001b[0m The parameter 'max_depth' in trial#49 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:13,545]\u001b[0m Trial 49 finished with value: 0.8645119164218087 and parameters: {'lambda': 0.026915984431739346, 'alpha': 0.566556179141757, 'gamma': 0.0223582359963139, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 298, 'max_depth': 20, 'min_child_weight': 146}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:22:13,547]\u001b[0m The parameter 'colsample_bytree' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:13,547]\u001b[0m The parameter 'subsample' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:13,547]\u001b[0m The parameter 'learning_rate' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:13,555]\u001b[0m The parameter 'max_depth' in trial#50 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:15,546]\u001b[0m Trial 50 finished with value: 0.871041462618348 and parameters: {'lambda': 3.708040087210852, 'alpha': 5.332101044234798, 'gamma': 0.25343364997285267, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.018, 'n_estimators': 296, 'max_depth': 6, 'min_child_weight': 145}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:15,554]\u001b[0m The parameter 'colsample_bytree' in trial#51 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:15,557]\u001b[0m The parameter 'subsample' in trial#51 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:15,557]\u001b[0m The parameter 'learning_rate' in trial#51 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:15,557]\u001b[0m The parameter 'max_depth' in trial#51 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:17,702]\u001b[0m Trial 51 finished with value: 0.870388507998694 and parameters: {'lambda': 0.13893853591001074, 'alpha': 0.42579799461121515, 'gamma': 0.019763767515788656, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.014, 'n_estimators': 298, 'max_depth': 7, 'min_child_weight': 150}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:17,708]\u001b[0m The parameter 'colsample_bytree' in trial#52 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:17,711]\u001b[0m The parameter 'subsample' in trial#52 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:17,713]\u001b[0m The parameter 'learning_rate' in trial#52 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:17,715]\u001b[0m The parameter 'max_depth' in trial#52 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:19,992]\u001b[0m Trial 52 finished with value: 0.8570029382957884 and parameters: {'lambda': 8.489044452087322, 'alpha': 0.4276953022586961, 'gamma': 0.9727539334831876, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.018, 'n_estimators': 298, 'max_depth': 5, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:20,003]\u001b[0m The parameter 'colsample_bytree' in trial#53 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:20,003]\u001b[0m The parameter 'subsample' in trial#53 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:20,003]\u001b[0m The parameter 'learning_rate' in trial#53 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:20,011]\u001b[0m The parameter 'max_depth' in trial#53 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:22:21,848]\u001b[0m Trial 53 finished with value: 0.8684296441397323 and parameters: {'lambda': 0.11486171595648884, 'alpha': 0.6009548210173279, 'gamma': 0.22669755779925255, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 297, 'max_depth': 6, 'min_child_weight': 146}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:21,858]\u001b[0m The parameter 'colsample_bytree' in trial#54 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:21,858]\u001b[0m The parameter 'subsample' in trial#54 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:21,858]\u001b[0m The parameter 'learning_rate' in trial#54 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:21,858]\u001b[0m The parameter 'max_depth' in trial#54 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:25,875]\u001b[0m Trial 54 finished with value: 0.8654913483512896 and parameters: {'lambda': 0.13681176272009155, 'alpha': 0.04810446950663393, 'gamma': 0.14939393297213083, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.01, 'n_estimators': 298, 'max_depth': 17, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:25,881]\u001b[0m The parameter 'colsample_bytree' in trial#55 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:25,883]\u001b[0m The parameter 'subsample' in trial#55 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:25,885]\u001b[0m The parameter 'learning_rate' in trial#55 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:25,887]\u001b[0m The parameter 'max_depth' in trial#55 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:28,143]\u001b[0m Trial 55 finished with value: 0.8739797584067908 and parameters: {'lambda': 0.20278451743757062, 'alpha': 0.1187582114592924, 'gamma': 0.18753872125492388, 'colsample_bytree': 0.7, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 297, 'max_depth': 15, 'min_child_weight': 146}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:28,149]\u001b[0m The parameter 'colsample_bytree' in trial#56 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:28,151]\u001b[0m The parameter 'subsample' in trial#56 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:28,153]\u001b[0m The parameter 'learning_rate' in trial#56 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:28,155]\u001b[0m The parameter 'max_depth' in trial#56 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:31,158]\u001b[0m Trial 56 finished with value: 0.8654913483512896 and parameters: {'lambda': 0.1065985196041671, 'alpha': 0.9713921098356297, 'gamma': 0.09161869777761535, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.01, 'n_estimators': 298, 'max_depth': 20, 'min_child_weight': 148}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:31,168]\u001b[0m The parameter 'colsample_bytree' in trial#57 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:31,172]\u001b[0m The parameter 'subsample' in trial#57 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:31,173]\u001b[0m The parameter 'learning_rate' in trial#57 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:22:31,175]\u001b[0m The parameter 'max_depth' in trial#57 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:33,391]\u001b[0m Trial 57 finished with value: 0.8524322559582109 and parameters: {'lambda': 0.614256936163209, 'alpha': 0.2948462073239085, 'gamma': 0.014726641793906587, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.01, 'n_estimators': 299, 'max_depth': 9, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:33,401]\u001b[0m The parameter 'colsample_bytree' in trial#58 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:33,401]\u001b[0m The parameter 'subsample' in trial#58 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:33,401]\u001b[0m The parameter 'learning_rate' in trial#58 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:33,409]\u001b[0m The parameter 'max_depth' in trial#58 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:35,429]\u001b[0m Trial 58 finished with value: 0.8501469147894222 and parameters: {'lambda': 1.9525174692264349, 'alpha': 0.7573551658005927, 'gamma': 0.027506759226317953, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.012, 'n_estimators': 296, 'max_depth': 17, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:35,435]\u001b[0m The parameter 'colsample_bytree' in trial#59 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:35,437]\u001b[0m The parameter 'subsample' in trial#59 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:35,439]\u001b[0m The parameter 'learning_rate' in trial#59 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:35,441]\u001b[0m The parameter 'max_depth' in trial#59 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:37,571]\u001b[0m Trial 59 finished with value: 0.871694417238002 and parameters: {'lambda': 0.5823882831201437, 'alpha': 0.2869940712962949, 'gamma': 0.40735509985125284, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.016, 'n_estimators': 298, 'max_depth': 15, 'min_child_weight': 146}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:37,579]\u001b[0m The parameter 'colsample_bytree' in trial#60 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:37,581]\u001b[0m The parameter 'subsample' in trial#60 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:37,581]\u001b[0m The parameter 'learning_rate' in trial#60 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:37,581]\u001b[0m The parameter 'max_depth' in trial#60 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:40,241]\u001b[0m Trial 60 finished with value: 0.8576558929154424 and parameters: {'lambda': 0.2095694012595696, 'alpha': 0.5357145655415482, 'gamma': 0.27707162648527117, 'colsample_bytree': 0.8, 'subsample': 0.9, 'learning_rate': 0.008, 'n_estimators': 296, 'max_depth': 11, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:40,252]\u001b[0m The parameter 'colsample_bytree' in trial#61 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:40,252]\u001b[0m The parameter 'subsample' in trial#61 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:22:40,252]\u001b[0m The parameter 'learning_rate' in trial#61 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:40,260]\u001b[0m The parameter 'max_depth' in trial#61 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:42,635]\u001b[0m Trial 61 finished with value: 0.8752856676460986 and parameters: {'lambda': 0.14280164430671446, 'alpha': 0.05490460778426666, 'gamma': 0.16253673061368784, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.300000012, 'n_estimators': 297, 'max_depth': 11, 'min_child_weight': 148}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:42,645]\u001b[0m The parameter 'colsample_bytree' in trial#62 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:42,645]\u001b[0m The parameter 'subsample' in trial#62 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:42,645]\u001b[0m The parameter 'learning_rate' in trial#62 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:42,645]\u001b[0m The parameter 'max_depth' in trial#62 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:45,111]\u001b[0m Trial 62 finished with value: 0.8694090760692131 and parameters: {'lambda': 0.04429624535396544, 'alpha': 0.41454707840812427, 'gamma': 0.5476607422300679, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.014, 'n_estimators': 297, 'max_depth': 6, 'min_child_weight': 148}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:45,121]\u001b[0m The parameter 'colsample_bytree' in trial#63 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:45,121]\u001b[0m The parameter 'subsample' in trial#63 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:45,121]\u001b[0m The parameter 'learning_rate' in trial#63 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:45,121]\u001b[0m The parameter 'max_depth' in trial#63 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:47,370]\u001b[0m Trial 63 finished with value: 0.8527587332680379 and parameters: {'lambda': 0.36878067332333286, 'alpha': 0.5921927710441638, 'gamma': 0.034741197383261875, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.016, 'n_estimators': 297, 'max_depth': 7, 'min_child_weight': 146}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:47,378]\u001b[0m The parameter 'colsample_bytree' in trial#64 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:47,380]\u001b[0m The parameter 'subsample' in trial#64 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:47,380]\u001b[0m The parameter 'learning_rate' in trial#64 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:47,380]\u001b[0m The parameter 'max_depth' in trial#64 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:50,263]\u001b[0m Trial 64 finished with value: 0.8681031668299053 and parameters: {'lambda': 0.20711144445783203, 'alpha': 0.8370344196944829, 'gamma': 0.043979266776396644, 'colsample_bytree': 0.6, 'subsample': 1.0, 'learning_rate': 0.01, 'n_estimators': 297, 'max_depth': 11, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:50,270]\u001b[0m The parameter 'colsample_bytree' in trial#65 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:22:50,272]\u001b[0m The parameter 'subsample' in trial#65 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:50,274]\u001b[0m The parameter 'learning_rate' in trial#65 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:50,275]\u001b[0m The parameter 'max_depth' in trial#65 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:52,799]\u001b[0m Trial 65 finished with value: 0.8651648710414627 and parameters: {'lambda': 0.5533399958491114, 'alpha': 0.4435731229658347, 'gamma': 0.6052213157024304, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 300, 'max_depth': 17, 'min_child_weight': 149}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:52,804]\u001b[0m The parameter 'colsample_bytree' in trial#66 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:52,806]\u001b[0m The parameter 'subsample' in trial#66 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:52,807]\u001b[0m The parameter 'learning_rate' in trial#66 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:52,809]\u001b[0m The parameter 'max_depth' in trial#66 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:55,048]\u001b[0m Trial 66 finished with value: 0.8481880509304603 and parameters: {'lambda': 0.04038670154562869, 'alpha': 0.08543483482272321, 'gamma': 0.04521957718226175, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.01, 'n_estimators': 295, 'max_depth': 6, 'min_child_weight': 147}. Best is trial 13 with value: 0.8756121449559255.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:55,054]\u001b[0m The parameter 'colsample_bytree' in trial#67 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:55,056]\u001b[0m The parameter 'subsample' in trial#67 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:55,057]\u001b[0m The parameter 'learning_rate' in trial#67 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:55,059]\u001b[0m The parameter 'max_depth' in trial#67 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:57,299]\u001b[0m Trial 67 finished with value: 0.8759386222657525 and parameters: {'lambda': 0.05867512052715696, 'alpha': 0.06551898052445691, 'gamma': 0.373890926535133, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.300000012, 'n_estimators': 299, 'max_depth': 11, 'min_child_weight': 147}. Best is trial 67 with value: 0.8759386222657525.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:57,299]\u001b[0m The parameter 'colsample_bytree' in trial#68 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:57,307]\u001b[0m The parameter 'subsample' in trial#68 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:57,309]\u001b[0m The parameter 'learning_rate' in trial#68 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:57,309]\u001b[0m The parameter 'max_depth' in trial#68 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:22:59,465]\u001b[0m Trial 68 finished with value: 0.8707149853085211 and parameters: {'lambda': 0.015095223811970796, 'alpha': 0.12141121931196253, 'gamma': 0.27415704209026287, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.018, 'n_estimators': 297, 'max_depth': 17, 'min_child_weight': 148}. Best is trial 67 with value: 0.8759386222657525.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:22:59,471]\u001b[0m The parameter 'colsample_bytree' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:59,472]\u001b[0m The parameter 'subsample' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:59,474]\u001b[0m The parameter 'learning_rate' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:22:59,476]\u001b[0m The parameter 'max_depth' in trial#69 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:01,616]\u001b[0m Trial 69 finished with value: 0.8524322559582109 and parameters: {'lambda': 0.1659151918494345, 'alpha': 0.04509623420142597, 'gamma': 0.255011593356664, 'colsample_bytree': 1.0, 'subsample': 0.4, 'learning_rate': 0.01, 'n_estimators': 298, 'max_depth': 15, 'min_child_weight': 146}. Best is trial 67 with value: 0.8759386222657525.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:01,627]\u001b[0m The parameter 'colsample_bytree' in trial#70 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:01,627]\u001b[0m The parameter 'subsample' in trial#70 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:01,627]\u001b[0m The parameter 'learning_rate' in trial#70 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:01,635]\u001b[0m The parameter 'max_depth' in trial#70 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:04,020]\u001b[0m Trial 70 finished with value: 0.8690825987593862 and parameters: {'lambda': 0.14438171457829355, 'alpha': 0.5778064535586755, 'gamma': 0.21876027903459738, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.014, 'n_estimators': 298, 'max_depth': 15, 'min_child_weight': 148}. Best is trial 67 with value: 0.8759386222657525.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:04,025]\u001b[0m The parameter 'colsample_bytree' in trial#71 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:04,028]\u001b[0m The parameter 'subsample' in trial#71 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:04,031]\u001b[0m The parameter 'learning_rate' in trial#71 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:04,033]\u001b[0m The parameter 'max_depth' in trial#71 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:06,049]\u001b[0m Trial 71 finished with value: 0.8700620306888671 and parameters: {'lambda': 0.025610682774875215, 'alpha': 0.048236976119451415, 'gamma': 0.0723134180377019, 'colsample_bytree': 0.6, 'subsample': 1.0, 'learning_rate': 0.012, 'n_estimators': 298, 'max_depth': 6, 'min_child_weight': 147}. Best is trial 67 with value: 0.8759386222657525.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:06,057]\u001b[0m The parameter 'colsample_bytree' in trial#72 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:06,058]\u001b[0m The parameter 'subsample' in trial#72 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:06,061]\u001b[0m The parameter 'learning_rate' in trial#72 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:06,064]\u001b[0m The parameter 'max_depth' in trial#72 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:23:09,418]\u001b[0m Trial 72 finished with value: 0.8576558929154424 and parameters: {'lambda': 0.09419693583424919, 'alpha': 1.2783748877511596, 'gamma': 0.22183628433990946, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.008, 'n_estimators': 297, 'max_depth': 11, 'min_child_weight': 148}. Best is trial 67 with value: 0.8759386222657525.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:09,418]\u001b[0m The parameter 'colsample_bytree' in trial#73 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:09,429]\u001b[0m The parameter 'subsample' in trial#73 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:09,429]\u001b[0m The parameter 'learning_rate' in trial#73 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:09,429]\u001b[0m The parameter 'max_depth' in trial#73 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:11,779]\u001b[0m Trial 73 finished with value: 0.8677766895200784 and parameters: {'lambda': 0.25803563549138414, 'alpha': 0.3345174540026794, 'gamma': 0.3767898882157782, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.012, 'n_estimators': 297, 'max_depth': 13, 'min_child_weight': 146}. Best is trial 67 with value: 0.8759386222657525.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:11,785]\u001b[0m The parameter 'colsample_bytree' in trial#74 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:11,787]\u001b[0m The parameter 'subsample' in trial#74 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:11,791]\u001b[0m The parameter 'learning_rate' in trial#74 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:11,794]\u001b[0m The parameter 'max_depth' in trial#74 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:13,383]\u001b[0m Trial 74 finished with value: 0.8775710088148874 and parameters: {'lambda': 0.015103111656482985, 'alpha': 0.010564431698273933, 'gamma': 0.6129460130775625, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.300000012, 'n_estimators': 301, 'max_depth': 15, 'min_child_weight': 147}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:13,383]\u001b[0m The parameter 'colsample_bytree' in trial#75 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:13,391]\u001b[0m The parameter 'subsample' in trial#75 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:13,391]\u001b[0m The parameter 'learning_rate' in trial#75 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:13,393]\u001b[0m The parameter 'max_depth' in trial#75 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:15,513]\u001b[0m Trial 75 finished with value: 0.8648383937316356 and parameters: {'lambda': 0.18382988222503147, 'alpha': 0.26526626269359116, 'gamma': 0.35730606769584644, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.009, 'n_estimators': 297, 'max_depth': 9, 'min_child_weight': 145}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:15,521]\u001b[0m The parameter 'colsample_bytree' in trial#76 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:15,523]\u001b[0m The parameter 'subsample' in trial#76 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:15,526]\u001b[0m The parameter 'learning_rate' in trial#76 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:23:15,529]\u001b[0m The parameter 'max_depth' in trial#76 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:17,283]\u001b[0m Trial 76 finished with value: 0.8648383937316356 and parameters: {'lambda': 0.04637355426168627, 'alpha': 0.0884434231851581, 'gamma': 0.357886372317136, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.009, 'n_estimators': 298, 'max_depth': 9, 'min_child_weight': 149}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:17,289]\u001b[0m The parameter 'colsample_bytree' in trial#77 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:17,289]\u001b[0m The parameter 'subsample' in trial#77 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:17,289]\u001b[0m The parameter 'learning_rate' in trial#77 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:17,289]\u001b[0m The parameter 'max_depth' in trial#77 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:19,534]\u001b[0m Trial 77 finished with value: 0.8641854391119818 and parameters: {'lambda': 0.3118253082064173, 'alpha': 0.46259176426998994, 'gamma': 0.3093708721154684, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.01, 'n_estimators': 299, 'max_depth': 6, 'min_child_weight': 147}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:19,534]\u001b[0m The parameter 'colsample_bytree' in trial#78 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:19,534]\u001b[0m The parameter 'subsample' in trial#78 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:19,542]\u001b[0m The parameter 'learning_rate' in trial#78 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:19,544]\u001b[0m The parameter 'max_depth' in trial#78 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:21,377]\u001b[0m Trial 78 finished with value: 0.8674502122102514 and parameters: {'lambda': 0.07998560184430027, 'alpha': 0.2131988794850851, 'gamma': 0.13571467465877385, 'colsample_bytree': 0.6, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 300, 'max_depth': 11, 'min_child_weight': 149}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:21,386]\u001b[0m The parameter 'colsample_bytree' in trial#79 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:21,388]\u001b[0m The parameter 'subsample' in trial#79 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:21,388]\u001b[0m The parameter 'learning_rate' in trial#79 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:21,388]\u001b[0m The parameter 'max_depth' in trial#79 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:23,217]\u001b[0m Trial 79 finished with value: 0.8677766895200784 and parameters: {'lambda': 0.004477331261207158, 'alpha': 0.17682026634682965, 'gamma': 0.09972628817898004, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.02, 'n_estimators': 299, 'max_depth': 20, 'min_child_weight': 148}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:23,225]\u001b[0m The parameter 'colsample_bytree' in trial#80 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:23,227]\u001b[0m The parameter 'subsample' in trial#80 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:23:23,227]\u001b[0m The parameter 'learning_rate' in trial#80 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:23,227]\u001b[0m The parameter 'max_depth' in trial#80 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:25,321]\u001b[0m Trial 80 finished with value: 0.8700620306888671 and parameters: {'lambda': 0.045099273183777336, 'alpha': 0.07677964750469697, 'gamma': 0.6213174118286122, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.01, 'n_estimators': 296, 'max_depth': 17, 'min_child_weight': 143}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:25,328]\u001b[0m The parameter 'colsample_bytree' in trial#81 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:25,331]\u001b[0m The parameter 'subsample' in trial#81 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:25,332]\u001b[0m The parameter 'learning_rate' in trial#81 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:25,335]\u001b[0m The parameter 'max_depth' in trial#81 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:27,621]\u001b[0m Trial 81 finished with value: 0.8743062357166177 and parameters: {'lambda': 0.0011688367094946604, 'alpha': 0.013592185161314286, 'gamma': 0.09408169956422723, 'colsample_bytree': 0.8, 'subsample': 1.0, 'learning_rate': 0.016, 'n_estimators': 302, 'max_depth': 7, 'min_child_weight': 149}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:27,627]\u001b[0m The parameter 'colsample_bytree' in trial#82 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:27,628]\u001b[0m The parameter 'subsample' in trial#82 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:27,630]\u001b[0m The parameter 'learning_rate' in trial#82 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:27,632]\u001b[0m The parameter 'max_depth' in trial#82 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:31,594]\u001b[0m Trial 82 finished with value: 0.8602677113940581 and parameters: {'lambda': 0.20050458806298965, 'alpha': 0.04705308546496664, 'gamma': 0.277683407939607, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.009, 'n_estimators': 299, 'max_depth': 11, 'min_child_weight': 144}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:31,595]\u001b[0m The parameter 'colsample_bytree' in trial#83 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:31,595]\u001b[0m The parameter 'subsample' in trial#83 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:31,603]\u001b[0m The parameter 'learning_rate' in trial#83 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:31,605]\u001b[0m The parameter 'max_depth' in trial#83 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:33,742]\u001b[0m Trial 83 finished with value: 0.871041462618348 and parameters: {'lambda': 0.056947345909306286, 'alpha': 0.09379392640565325, 'gamma': 0.8077454836798763, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.018, 'n_estimators': 301, 'max_depth': 6, 'min_child_weight': 145}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:33,743]\u001b[0m The parameter 'colsample_bytree' in trial#84 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:23:33,750]\u001b[0m The parameter 'subsample' in trial#84 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:33,750]\u001b[0m The parameter 'learning_rate' in trial#84 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:33,752]\u001b[0m The parameter 'max_depth' in trial#84 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:35,245]\u001b[0m Trial 84 finished with value: 0.8723473718576559 and parameters: {'lambda': 0.009455524756639784, 'alpha': 0.0011760137424436452, 'gamma': 0.4834436788748708, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.300000012, 'n_estimators': 301, 'max_depth': 17, 'min_child_weight': 146}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:35,253]\u001b[0m The parameter 'colsample_bytree' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:35,253]\u001b[0m The parameter 'subsample' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:35,255]\u001b[0m The parameter 'learning_rate' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:35,255]\u001b[0m The parameter 'max_depth' in trial#85 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:37,189]\u001b[0m Trial 85 finished with value: 0.861247143323539 and parameters: {'lambda': 0.025458290980208388, 'alpha': 0.2624732238579597, 'gamma': 0.5599611431629923, 'colsample_bytree': 0.6, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 297, 'max_depth': 15, 'min_child_weight': 144}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:37,194]\u001b[0m The parameter 'colsample_bytree' in trial#86 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:37,197]\u001b[0m The parameter 'subsample' in trial#86 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:37,198]\u001b[0m The parameter 'learning_rate' in trial#86 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:37,201]\u001b[0m The parameter 'max_depth' in trial#86 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:39,807]\u001b[0m Trial 86 finished with value: 0.8707149853085211 and parameters: {'lambda': 0.0039030113989041238, 'alpha': 0.01629843824682658, 'gamma': 0.1466821475779838, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.02, 'n_estimators': 300, 'max_depth': 11, 'min_child_weight': 147}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:39,815]\u001b[0m The parameter 'colsample_bytree' in trial#87 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:39,817]\u001b[0m The parameter 'subsample' in trial#87 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:39,819]\u001b[0m The parameter 'learning_rate' in trial#87 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:39,821]\u001b[0m The parameter 'max_depth' in trial#87 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:41,931]\u001b[0m Trial 87 finished with value: 0.8609206660137121 and parameters: {'lambda': 0.022188181644063363, 'alpha': 0.02151172680221825, 'gamma': 0.31732145408448287, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.008, 'n_estimators': 301, 'max_depth': 9, 'min_child_weight': 145}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:23:41,931]\u001b[0m The parameter 'colsample_bytree' in trial#88 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:41,939]\u001b[0m The parameter 'subsample' in trial#88 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:41,941]\u001b[0m The parameter 'learning_rate' in trial#88 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:41,941]\u001b[0m The parameter 'max_depth' in trial#88 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:43,982]\u001b[0m Trial 88 finished with value: 0.8576558929154424 and parameters: {'lambda': 0.11472924671501478, 'alpha': 0.20836748439928543, 'gamma': 0.7528974327153374, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.008, 'n_estimators': 295, 'max_depth': 11, 'min_child_weight': 144}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:43,992]\u001b[0m The parameter 'colsample_bytree' in trial#89 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:43,992]\u001b[0m The parameter 'subsample' in trial#89 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:43,992]\u001b[0m The parameter 'learning_rate' in trial#89 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:44,000]\u001b[0m The parameter 'max_depth' in trial#89 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:46,032]\u001b[0m Trial 89 finished with value: 0.871694417238002 and parameters: {'lambda': 0.0014332278104456939, 'alpha': 0.004999224396574558, 'gamma': 0.33651622666952624, 'colsample_bytree': 1.0, 'subsample': 1.0, 'learning_rate': 0.012, 'n_estimators': 302, 'max_depth': 11, 'min_child_weight': 148}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:46,041]\u001b[0m The parameter 'colsample_bytree' in trial#90 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:46,042]\u001b[0m The parameter 'subsample' in trial#90 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:46,042]\u001b[0m The parameter 'learning_rate' in trial#90 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:46,042]\u001b[0m The parameter 'max_depth' in trial#90 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:47,554]\u001b[0m Trial 90 finished with value: 0.8547175971269997 and parameters: {'lambda': 0.007645784130594248, 'alpha': 0.017032550832029418, 'gamma': 0.1794888986433565, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 301, 'max_depth': 9, 'min_child_weight': 148}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:47,561]\u001b[0m The parameter 'colsample_bytree' in trial#91 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:47,562]\u001b[0m The parameter 'subsample' in trial#91 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:47,566]\u001b[0m The parameter 'learning_rate' in trial#91 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:47,569]\u001b[0m The parameter 'max_depth' in trial#91 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:23:49,152]\u001b[0m Trial 91 finished with value: 0.8730003264773099 and parameters: {'lambda': 0.003060601386322018, 'alpha': 0.012221175398363563, 'gamma': 0.07072369071016088, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.300000012, 'n_estimators': 301, 'max_depth': 20, 'min_child_weight': 149}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:49,157]\u001b[0m The parameter 'colsample_bytree' in trial#92 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:49,160]\u001b[0m The parameter 'subsample' in trial#92 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:49,164]\u001b[0m The parameter 'learning_rate' in trial#92 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:49,166]\u001b[0m The parameter 'max_depth' in trial#92 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:51,274]\u001b[0m Trial 92 finished with value: 0.871041462618348 and parameters: {'lambda': 0.06662871823575925, 'alpha': 0.7877760250186964, 'gamma': 0.14832586106410833, 'colsample_bytree': 0.8, 'subsample': 1.0, 'learning_rate': 0.009, 'n_estimators': 299, 'max_depth': 7, 'min_child_weight': 145}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:51,282]\u001b[0m The parameter 'colsample_bytree' in trial#93 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:51,282]\u001b[0m The parameter 'subsample' in trial#93 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:51,284]\u001b[0m The parameter 'learning_rate' in trial#93 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:51,284]\u001b[0m The parameter 'max_depth' in trial#93 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:52,988]\u001b[0m Trial 93 finished with value: 0.851779301338557 and parameters: {'lambda': 0.0062848165335570375, 'alpha': 0.01592671309484264, 'gamma': 0.009333030999393045, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.014, 'n_estimators': 301, 'max_depth': 15, 'min_child_weight': 149}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:52,998]\u001b[0m The parameter 'colsample_bytree' in trial#94 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:52,998]\u001b[0m The parameter 'subsample' in trial#94 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:52,998]\u001b[0m The parameter 'learning_rate' in trial#94 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:52,998]\u001b[0m The parameter 'max_depth' in trial#94 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:55,236]\u001b[0m Trial 94 finished with value: 0.8707149853085211 and parameters: {'lambda': 0.0025671379647436215, 'alpha': 0.0033122198930409267, 'gamma': 0.38987669473572595, 'colsample_bytree': 0.8, 'subsample': 0.9, 'learning_rate': 0.014, 'n_estimators': 303, 'max_depth': 11, 'min_child_weight': 144}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:55,242]\u001b[0m The parameter 'colsample_bytree' in trial#95 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:55,244]\u001b[0m The parameter 'subsample' in trial#95 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:55,246]\u001b[0m The parameter 'learning_rate' in trial#95 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:23:55,248]\u001b[0m The parameter 'max_depth' in trial#95 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:57,216]\u001b[0m Trial 95 finished with value: 0.8609206660137121 and parameters: {'lambda': 0.08654395432273246, 'alpha': 0.05875074154183072, 'gamma': 0.5893825626549539, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.018, 'n_estimators': 298, 'max_depth': 11, 'min_child_weight': 142}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:57,221]\u001b[0m The parameter 'colsample_bytree' in trial#96 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:57,225]\u001b[0m The parameter 'subsample' in trial#96 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:57,227]\u001b[0m The parameter 'learning_rate' in trial#96 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:57,230]\u001b[0m The parameter 'max_depth' in trial#96 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:23:59,882]\u001b[0m Trial 96 finished with value: 0.8635324844923278 and parameters: {'lambda': 0.04228520099827825, 'alpha': 0.18185873019505733, 'gamma': 0.39620276409855415, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.01, 'n_estimators': 300, 'max_depth': 7, 'min_child_weight': 147}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:59,890]\u001b[0m The parameter 'colsample_bytree' in trial#97 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:59,892]\u001b[0m The parameter 'subsample' in trial#97 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:59,895]\u001b[0m The parameter 'learning_rate' in trial#97 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:23:59,898]\u001b[0m The parameter 'max_depth' in trial#97 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:02,233]\u001b[0m Trial 97 finished with value: 0.851779301338557 and parameters: {'lambda': 0.004729865026291097, 'alpha': 0.006473287730797607, 'gamma': 0.133674682425445, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.01, 'n_estimators': 304, 'max_depth': 20, 'min_child_weight': 149}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:24:02,240]\u001b[0m The parameter 'colsample_bytree' in trial#98 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:24:02,242]\u001b[0m The parameter 'subsample' in trial#98 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:24:02,246]\u001b[0m The parameter 'learning_rate' in trial#98 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:24:02,247]\u001b[0m The parameter 'max_depth' in trial#98 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:04,656]\u001b[0m Trial 98 finished with value: 0.8488410055501142 and parameters: {'lambda': 0.0037956080089780317, 'alpha': 0.03346831855464631, 'gamma': 0.0702109925745578, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.01, 'n_estimators': 301, 'max_depth': 13, 'min_child_weight': 150}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:24:04,662]\u001b[0m The parameter 'colsample_bytree' in trial#99 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:24:04,664]\u001b[0m The parameter 'subsample' in trial#99 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-12-15 14:24:04,668]\u001b[0m The parameter 'learning_rate' in trial#99 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[33m[W 2021-12-15 14:24:04,670]\u001b[0m The parameter 'max_depth' in trial#99 is sampled independently by using `RandomSampler` instead of `CmaEsSampler` (optimization performance may be degraded). `CmaEsSampler` does not support dynamic search space or `CategoricalDistribution`. You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `CmaEsSampler`, if this independent sampling is intended behavior.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:08,318]\u001b[0m Trial 99 finished with value: 0.8700620306888671 and parameters: {'lambda': 0.010473249196019304, 'alpha': 0.013444296157671657, 'gamma': 0.11352424120828924, 'colsample_bytree': 1.0, 'subsample': 1.0, 'learning_rate': 0.008, 'n_estimators': 302, 'max_depth': 6, 'min_child_weight': 148}. Best is trial 74 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:12,398]\u001b[0m Trial 0 finished with value: 0.881488736532811 and parameters: {'lambda': 0.12045869621351266, 'alpha': 0.0056726842355158845, 'gamma': 0.26217753908170144, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.012, 'n_estimators': 343, 'max_depth': 20, 'min_child_weight': 30}. Best is trial 0 with value: 0.881488736532811.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:13,398]\u001b[0m Trial 1 finished with value: 0.8622265752530199 and parameters: {'lambda': 0.0030791904302801295, 'alpha': 8.633684695630631, 'gamma': 0.43113948659985374, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.016, 'n_estimators': 180, 'max_depth': 13, 'min_child_weight': 124}. Best is trial 0 with value: 0.881488736532811.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:14,617]\u001b[0m Trial 2 finished with value: 0.8537381651975188 and parameters: {'lambda': 0.030698600979951318, 'alpha': 0.7391781152941252, 'gamma': 0.07566652302736167, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.012, 'n_estimators': 233, 'max_depth': 20, 'min_child_weight': 285}. Best is trial 0 with value: 0.881488736532811.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:15,626]\u001b[0m Trial 3 finished with value: 0.8498204374795951 and parameters: {'lambda': 0.0067157450552373715, 'alpha': 0.6374045447897868, 'gamma': 0.21019481131815915, 'colsample_bytree': 0.6, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 196, 'max_depth': 20, 'min_child_weight': 251}. Best is trial 0 with value: 0.881488736532811.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:18,461]\u001b[0m Trial 4 finished with value: 0.8772445315050604 and parameters: {'lambda': 4.865945890188443, 'alpha': 0.004663237416653837, 'gamma': 0.001036673940804618, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.016, 'n_estimators': 412, 'max_depth': 17, 'min_child_weight': 24}. Best is trial 0 with value: 0.881488736532811.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:21,760]\u001b[0m Trial 5 finished with value: 0.8762650995755795 and parameters: {'lambda': 0.17209123564442158, 'alpha': 0.054041495016660474, 'gamma': 0.03236001352275247, 'colsample_bytree': 0.8, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 413, 'max_depth': 20, 'min_child_weight': 86}. Best is trial 0 with value: 0.881488736532811.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:23,894]\u001b[0m Trial 6 finished with value: 0.8602677113940581 and parameters: {'lambda': 0.11185359354296846, 'alpha': 0.06222526662296766, 'gamma': 0.007071063359731426, 'colsample_bytree': 0.4, 'subsample': 0.7, 'learning_rate': 0.009, 'n_estimators': 429, 'max_depth': 6, 'min_child_weight': 237}. Best is trial 0 with value: 0.881488736532811.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:26,181]\u001b[0m Trial 7 finished with value: 0.881815213842638 and parameters: {'lambda': 0.0055193318622443815, 'alpha': 3.11940786998157, 'gamma': 0.0540229918648147, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.300000012, 'n_estimators': 373, 'max_depth': 17, 'min_child_weight': 86}. Best is trial 7 with value: 0.881815213842638.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:29,390]\u001b[0m Trial 8 finished with value: 0.8752856676460986 and parameters: {'lambda': 0.06289631546616316, 'alpha': 0.0031369209451026087, 'gamma': 0.6475703885277809, 'colsample_bytree': 0.7, 'subsample': 1.0, 'learning_rate': 0.014, 'n_estimators': 478, 'max_depth': 17, 'min_child_weight': 123}. Best is trial 7 with value: 0.881815213842638.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:31,920]\u001b[0m Trial 9 finished with value: 0.8485145282402873 and parameters: {'lambda': 0.04912151179029008, 'alpha': 0.10325619613693324, 'gamma': 0.0017647021054883764, 'colsample_bytree': 0.6, 'subsample': 0.6, 'learning_rate': 0.008, 'n_estimators': 467, 'max_depth': 20, 'min_child_weight': 232}. Best is trial 7 with value: 0.881815213842638.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:33,599]\u001b[0m Trial 10 finished with value: 0.8681031668299053 and parameters: {'lambda': 0.0011891084708886013, 'alpha': 4.4706035685397625, 'gamma': 0.009636670731679698, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.300000012, 'n_estimators': 298, 'max_depth': 11, 'min_child_weight': 180}. Best is trial 7 with value: 0.881815213842638.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:38,723]\u001b[0m Trial 11 finished with value: 0.8847535096310807 and parameters: {'lambda': 1.2102789458592274, 'alpha': 0.01258321331023716, 'gamma': 0.11299431841346942, 'colsample_bytree': 0.9, 'subsample': 1.0, 'learning_rate': 0.01, 'n_estimators': 324, 'max_depth': 9, 'min_child_weight': 1}. Best is trial 11 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:42,787]\u001b[0m Trial 12 finished with value: 0.8837740777015998 and parameters: {'lambda': 2.0976913099346692, 'alpha': 0.017361178469931456, 'gamma': 0.05941883000064466, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.01, 'n_estimators': 327, 'max_depth': 9, 'min_child_weight': 4}. Best is trial 11 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:46,188]\u001b[0m Trial 13 finished with value: 0.8857329415605616 and parameters: {'lambda': 2.6246347188019707, 'alpha': 0.01633981719113365, 'gamma': 0.12301617231597342, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.01, 'n_estimators': 284, 'max_depth': 9, 'min_child_weight': 2}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:46,960]\u001b[0m Trial 14 finished with value: 0.861900097943193 and parameters: {'lambda': 1.142319289166969, 'alpha': 0.0011215051534677195, 'gamma': 0.09442552863550004, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.01, 'n_estimators': 100, 'max_depth': 9, 'min_child_weight': 61}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:49,579]\u001b[0m Trial 15 finished with value: 0.880835781913157 and parameters: {'lambda': 0.7145688032426804, 'alpha': 0.02158079679875174, 'gamma': 0.012098363392474483, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.018, 'n_estimators': 259, 'max_depth': 7, 'min_child_weight': 47}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:51,601]\u001b[0m Trial 16 finished with value: 0.8844270323212536 and parameters: {'lambda': 9.175940175402271, 'alpha': 0.0126599739314841, 'gamma': 0.1678054216995209, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.02, 'n_estimators': 274, 'max_depth': 5, 'min_child_weight': 2}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:53,635]\u001b[0m Trial 17 finished with value: 0.8658178256611165 and parameters: {'lambda': 0.47698315006053926, 'alpha': 0.24339427668712196, 'gamma': 0.8971817083981077, 'colsample_bytree': 0.5, 'subsample': 0.7, 'learning_rate': 0.01, 'n_estimators': 361, 'max_depth': 15, 'min_child_weight': 180}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:55,220]\u001b[0m Trial 18 finished with value: 0.861247143323539 and parameters: {'lambda': 2.6028120940316772, 'alpha': 0.0011413832892731546, 'gamma': 0.016213968167071362, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.01, 'n_estimators': 218, 'max_depth': 9, 'min_child_weight': 75}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:24:56,294]\u001b[0m Trial 19 finished with value: 0.8566764609859615 and parameters: {'lambda': 0.33322330243456433, 'alpha': 0.029231797306920177, 'gamma': 0.13105530855683734, 'colsample_bytree': 0.8, 'subsample': 1.0, 'learning_rate': 0.01, 'n_estimators': 156, 'max_depth': 9, 'min_child_weight': 156}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:24:58,756]\u001b[0m Trial 20 finished with value: 0.8752856676460986 and parameters: {'lambda': 1.5788204916066804, 'alpha': 0.1796399380586137, 'gamma': 0.028266314693125114, 'colsample_bytree': 1.0, 'subsample': 1.0, 'learning_rate': 0.014, 'n_estimators': 303, 'max_depth': 9, 'min_child_weight': 105}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:00,769]\u001b[0m Trial 21 finished with value: 0.8854064642507345 and parameters: {'lambda': 9.242810850367846, 'alpha': 0.011999718214240992, 'gamma': 0.20584064294219295, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.02, 'n_estimators': 272, 'max_depth': 5, 'min_child_weight': 3}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:02,619]\u001b[0m Trial 22 finished with value: 0.8795298726738492 and parameters: {'lambda': 7.583873817705522, 'alpha': 0.009727259642968328, 'gamma': 0.3117708505700481, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.02, 'n_estimators': 267, 'max_depth': 5, 'min_child_weight': 34}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:04,762]\u001b[0m Trial 23 finished with value: 0.8824681684622919 and parameters: {'lambda': 3.385649443652953, 'alpha': 0.002445862283006811, 'gamma': 0.11842951396781527, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.02, 'n_estimators': 307, 'max_depth': 5, 'min_child_weight': 14}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:08,420]\u001b[0m Trial 24 finished with value: 0.880835781913157 and parameters: {'lambda': 0.9775654182166216, 'alpha': 0.03835567545843501, 'gamma': 0.43064979213275334, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.018, 'n_estimators': 382, 'max_depth': 7, 'min_child_weight': 45}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:09,977]\u001b[0m Trial 25 finished with value: 0.8697355533790402 and parameters: {'lambda': 5.065545130114307, 'alpha': 0.008836578313808546, 'gamma': 0.037993148624394464, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.008, 'n_estimators': 236, 'max_depth': 6, 'min_child_weight': 62}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:16,178]\u001b[0m Trial 26 finished with value: 0.8857329415605616 and parameters: {'lambda': 0.2621352310098622, 'alpha': 0.0025765480515791764, 'gamma': 0.15550206173494702, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.009, 'n_estimators': 330, 'max_depth': 15, 'min_child_weight': 1}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:18,732]\u001b[0m Trial 27 finished with value: 0.8762650995755795 and parameters: {'lambda': 0.2757003395679453, 'alpha': 0.0021706608880396887, 'gamma': 0.4192253532595673, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.009, 'n_estimators': 285, 'max_depth': 15, 'min_child_weight': 29}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:20,933]\u001b[0m Trial 28 finished with value: 0.871041462618348 and parameters: {'lambda': 0.02426346655407708, 'alpha': 0.004961645881728235, 'gamma': 0.18704892483343488, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.009, 'n_estimators': 255, 'max_depth': 15, 'min_child_weight': 48}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:24,299]\u001b[0m Trial 29 finished with value: 0.8798563499836761 and parameters: {'lambda': 9.883887492980794, 'alpha': 0.00574436906795196, 'gamma': 0.28935479553729154, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.009, 'n_estimators': 342, 'max_depth': 11, 'min_child_weight': 22}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:25,259]\u001b[0m Trial 30 finished with value: 0.8687561214495593 and parameters: {'lambda': 0.6127593308961883, 'alpha': 0.0018395458393704, 'gamma': 0.003622574825827136, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.02, 'n_estimators': 156, 'max_depth': 13, 'min_child_weight': 67}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:28,137]\u001b[0m Trial 31 finished with value: 0.8824681684622919 and parameters: {'lambda': 1.767799059007562, 'alpha': 0.007963160869604977, 'gamma': 0.0923300954602727, 'colsample_bytree': 0.9, 'subsample': 1.0, 'learning_rate': 0.01, 'n_estimators': 328, 'max_depth': 5, 'min_child_weight': 4}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:30,751]\u001b[0m Trial 32 finished with value: 0.8749591903362717 and parameters: {'lambda': 4.341891157641539, 'alpha': 0.01763402629394107, 'gamma': 0.14639587190898468, 'colsample_bytree': 0.5, 'subsample': 0.6, 'learning_rate': 0.01, 'n_estimators': 319, 'max_depth': 15, 'min_child_weight': 31}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:35,224]\u001b[0m Trial 33 finished with value: 0.8857329415605616 and parameters: {'lambda': 0.1921943786212156, 'alpha': 0.003781229434624576, 'gamma': 0.06415925565400485, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 354, 'max_depth': 9, 'min_child_weight': 4}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:38,429]\u001b[0m Trial 34 finished with value: 0.8792033953640223 and parameters: {'lambda': 0.13887536900721528, 'alpha': 0.003998072222241995, 'gamma': 0.054236873122558586, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 394, 'max_depth': 13, 'min_child_weight': 44}. Best is trial 13 with value: 0.8857329415605616.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:41,954]\u001b[0m Trial 35 finished with value: 0.8860594188703885 and parameters: {'lambda': 0.24463291297454626, 'alpha': 0.0016240861337152562, 'gamma': 0.01935974577484466, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 363, 'max_depth': 9, 'min_child_weight': 19}. Best is trial 35 with value: 0.8860594188703885.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:45,477]\u001b[0m Trial 36 finished with value: 0.8834476003917727 and parameters: {'lambda': 0.01872439023923046, 'alpha': 0.0018294393605537691, 'gamma': 0.026830922994067592, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 355, 'max_depth': 9, 'min_child_weight': 24}. Best is trial 35 with value: 0.8860594188703885.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:50,118]\u001b[0m Trial 37 finished with value: 0.8860594188703885 and parameters: {'lambda': 0.22006258453798766, 'alpha': 0.0011529010819865718, 'gamma': 0.01969968896253038, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 438, 'max_depth': 9, 'min_child_weight': 16}. Best is trial 35 with value: 0.8860594188703885.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:52,504]\u001b[0m Trial 38 finished with value: 0.8521057786483839 and parameters: {'lambda': 0.24800589086681746, 'alpha': 0.0010320618816172691, 'gamma': 0.01876651738375174, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 455, 'max_depth': 9, 'min_child_weight': 296}. Best is trial 35 with value: 0.8860594188703885.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:55,325]\u001b[0m Trial 39 finished with value: 0.8730003264773099 and parameters: {'lambda': 0.07709368146790686, 'alpha': 0.0032429233004183443, 'gamma': 0.004214000039124635, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 437, 'max_depth': 15, 'min_child_weight': 109}. Best is trial 35 with value: 0.8860594188703885.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:25:59,235]\u001b[0m Trial 40 finished with value: 0.8749591903362717 and parameters: {'lambda': 0.18286756659733447, 'alpha': 0.0016169703859441487, 'gamma': 0.0072788221198763015, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 399, 'max_depth': 6, 'min_child_weight': 103}. Best is trial 35 with value: 0.8860594188703885.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:26:04,522]\u001b[0m Trial 41 finished with value: 0.8873653281096964 and parameters: {'lambda': 0.33904349206947143, 'alpha': 0.00605983346487188, 'gamma': 0.04399019997940061, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.012, 'n_estimators': 497, 'max_depth': 9, 'min_child_weight': 20}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:09,291]\u001b[0m Trial 42 finished with value: 0.8841005550114267 and parameters: {'lambda': 0.43683051183903715, 'alpha': 0.006569508318742589, 'gamma': 0.04420429672117147, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.012, 'n_estimators': 441, 'max_depth': 9, 'min_child_weight': 21}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:12,728]\u001b[0m Trial 43 finished with value: 0.8785504407443683 and parameters: {'lambda': 0.046954624846145696, 'alpha': 0.003098780541335559, 'gamma': 0.01938051814172024, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.012, 'n_estimators': 468, 'max_depth': 9, 'min_child_weight': 83}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:16,156]\u001b[0m Trial 44 finished with value: 0.8795298726738492 and parameters: {'lambda': 0.10187713054824932, 'alpha': 0.004138537985352155, 'gamma': 0.0675806688541397, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 423, 'max_depth': 17, 'min_child_weight': 57}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:19,326]\u001b[0m Trial 45 finished with value: 0.8762650995755795 and parameters: {'lambda': 0.18010722383309966, 'alpha': 0.0013905176542121092, 'gamma': 0.023043638591957634, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.012, 'n_estimators': 496, 'max_depth': 20, 'min_child_weight': 39}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:24,291]\u001b[0m Trial 46 finished with value: 0.8863858961802155 and parameters: {'lambda': 0.3767805980776111, 'alpha': 0.6398103862821004, 'gamma': 0.011720731178154194, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 498, 'max_depth': 9, 'min_child_weight': 17}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:29,146]\u001b[0m Trial 47 finished with value: 0.8863858961802155 and parameters: {'lambda': 0.40082041443711935, 'alpha': 0.6083157479231792, 'gamma': 0.01192757585801047, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 482, 'max_depth': 9, 'min_child_weight': 17}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:31,869]\u001b[0m Trial 48 finished with value: 0.861573620633366 and parameters: {'lambda': 0.8008837503850581, 'alpha': 0.8445903953425282, 'gamma': 0.01156750179586304, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 499, 'max_depth': 9, 'min_child_weight': 205}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:34,807]\u001b[0m Trial 49 finished with value: 0.8501469147894222 and parameters: {'lambda': 0.403180072017698, 'alpha': 0.8590450561640606, 'gamma': 0.005023713577248271, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 484, 'max_depth': 9, 'min_child_weight': 265}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:39,690]\u001b[0m Trial 50 finished with value: 0.8834476003917727 and parameters: {'lambda': 0.08639683364786385, 'alpha': 1.5962031502458272, 'gamma': 0.014358786060964631, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 454, 'max_depth': 11, 'min_child_weight': 20}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:43,870]\u001b[0m Trial 51 finished with value: 0.8769180541952334 and parameters: {'lambda': 0.5585478262438383, 'alpha': 0.4743522999282004, 'gamma': 0.008556406582959756, 'colsample_bytree': 0.7, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 462, 'max_depth': 9, 'min_child_weight': 19}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:47,389]\u001b[0m Trial 52 finished with value: 0.8782239634345413 and parameters: {'lambda': 1.1980176594226504, 'alpha': 0.08216650324897498, 'gamma': 0.03772278720753697, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.012, 'n_estimators': 478, 'max_depth': 9, 'min_child_weight': 53}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:50,924]\u001b[0m Trial 53 finished with value: 0.8798563499836761 and parameters: {'lambda': 0.28349034924406724, 'alpha': 7.674596921301602, 'gamma': 0.005538421010823729, 'colsample_bytree': 0.6, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 417, 'max_depth': 9, 'min_child_weight': 13}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:55,064]\u001b[0m Trial 54 finished with value: 0.8795298726738492 and parameters: {'lambda': 0.13794607645859014, 'alpha': 0.43713893724476216, 'gamma': 0.01248366836722057, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.014, 'n_estimators': 484, 'max_depth': 7, 'min_child_weight': 69}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:26:59,147]\u001b[0m Trial 55 finished with value: 0.8798563499836761 and parameters: {'lambda': 0.05493046795432548, 'alpha': 2.1185870129985807, 'gamma': 0.0024895256920485507, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.008, 'n_estimators': 498, 'max_depth': 17, 'min_child_weight': 32}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:03,573]\u001b[0m Trial 56 finished with value: 0.8857329415605616 and parameters: {'lambda': 0.31330928720129264, 'alpha': 0.17297183024512158, 'gamma': 0.021216529904101403, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 400, 'max_depth': 20, 'min_child_weight': 12}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:07,233]\u001b[0m Trial 57 finished with value: 0.8801828272935031 and parameters: {'lambda': 0.7695728414625433, 'alpha': 1.2919747474701548, 'gamma': 0.008722871569139786, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 441, 'max_depth': 9, 'min_child_weight': 37}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:09,865]\u001b[0m Trial 58 finished with value: 0.8726738491674828 and parameters: {'lambda': 0.013472173742051495, 'alpha': 0.2889194793478567, 'gamma': 0.028169368770524103, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.016, 'n_estimators': 471, 'max_depth': 9, 'min_child_weight': 77}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:12,617]\u001b[0m Trial 59 finished with value: 0.8579823702252694 and parameters: {'lambda': 0.03849814980201473, 'alpha': 0.02647446171912943, 'gamma': 0.01508474640600017, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.018, 'n_estimators': 456, 'max_depth': 9, 'min_child_weight': 150}. Best is trial 41 with value: 0.8873653281096964.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:14,993]\u001b[0m Trial 60 finished with value: 0.8886712373490042 and parameters: {'lambda': 0.2392329798535532, 'alpha': 0.002621653839060247, 'gamma': 0.010115951465912081, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 370, 'max_depth': 15, 'min_child_weight': 95}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:18,376]\u001b[0m Trial 61 finished with value: 0.8827946457721189 and parameters: {'lambda': 0.36152019726114093, 'alpha': 0.12579459085526518, 'gamma': 0.02191707003950417, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 401, 'max_depth': 20, 'min_child_weight': 13}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:21,347]\u001b[0m Trial 62 finished with value: 0.880835781913157 and parameters: {'lambda': 0.21235491278682941, 'alpha': 0.30529028184884544, 'gamma': 0.011218251508499102, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 377, 'max_depth': 20, 'min_child_weight': 39}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:27:24,359]\u001b[0m Trial 63 finished with value: 0.8726738491674828 and parameters: {'lambda': 0.48512123069031216, 'alpha': 0.16882065989240042, 'gamma': 0.006616981003832304, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 431, 'max_depth': 20, 'min_child_weight': 134}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:29,718]\u001b[0m Trial 64 finished with value: 0.8824681684622919 and parameters: {'lambda': 0.14414401377666747, 'alpha': 0.002690342120682797, 'gamma': 0.015998825723829456, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 486, 'max_depth': 15, 'min_child_weight': 13}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:32,513]\u001b[0m Trial 65 finished with value: 0.8752856676460986 and parameters: {'lambda': 0.0012504093545690018, 'alpha': 0.0013621110943004163, 'gamma': 0.03558729049441122, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 339, 'max_depth': 15, 'min_child_weight': 52}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:36,218]\u001b[0m Trial 66 finished with value: 0.8824681684622919 and parameters: {'lambda': 0.10708488208894809, 'alpha': 0.053260316293086474, 'gamma': 0.08292761476047118, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 364, 'max_depth': 9, 'min_child_weight': 30}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:39,321]\u001b[0m Trial 67 finished with value: 0.8736532810969637 and parameters: {'lambda': 0.23358281721883128, 'alpha': 0.0019276820133284696, 'gamma': 0.010263325712140475, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.012, 'n_estimators': 385, 'max_depth': 15, 'min_child_weight': 90}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:42,836]\u001b[0m Trial 68 finished with value: 0.8831211230819458 and parameters: {'lambda': 0.06966756851110636, 'alpha': 3.477103329340555, 'gamma': 0.024229608363574403, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.014, 'n_estimators': 355, 'max_depth': 9, 'min_child_weight': 9}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:46,522]\u001b[0m Trial 69 finished with value: 0.8834476003917727 and parameters: {'lambda': 0.6186218215376846, 'alpha': 0.006640866493742817, 'gamma': 0.04505964708846957, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 444, 'max_depth': 6, 'min_child_weight': 24}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:56,093]\u001b[0m Trial 70 finished with value: 0.8834476003917727 and parameters: {'lambda': 0.8673315802148794, 'alpha': 0.0023703260440139356, 'gamma': 0.6026605213472656, 'colsample_bytree': 0.7, 'subsample': 1.0, 'learning_rate': 0.009, 'n_estimators': 475, 'max_depth': 13, 'min_child_weight': 1}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:27:58,729]\u001b[0m Trial 71 finished with value: 0.8805093046033301 and parameters: {'lambda': 0.2960643914271316, 'alpha': 0.40665898250045285, 'gamma': 0.019141111279036765, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.01, 'n_estimators': 290, 'max_depth': 9, 'min_child_weight': 13}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:01,145]\u001b[0m Trial 72 finished with value: 0.8720208945478289 and parameters: {'lambda': 0.17786971263541493, 'alpha': 0.0035418023673438664, 'gamma': 0.10639374017497129, 'colsample_bytree': 0.6, 'subsample': 0.5, 'learning_rate': 0.008, 'n_estimators': 316, 'max_depth': 9, 'min_child_weight': 43}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:04,370]\u001b[0m Trial 73 finished with value: 0.881488736532811 and parameters: {'lambda': 0.3582745423259425, 'alpha': 0.001025795448220505, 'gamma': 0.0692471555925831, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 338, 'max_depth': 15, 'min_child_weight': 28}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:09,568]\u001b[0m Trial 74 finished with value: 0.8857329415605616 and parameters: {'lambda': 0.22029883446958526, 'alpha': 0.005031580801781764, 'gamma': 0.030992384153718598, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.018, 'n_estimators': 371, 'max_depth': 11, 'min_child_weight': 6}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:13,222]\u001b[0m Trial 75 finished with value: 0.8867123734900424 and parameters: {'lambda': 0.4647770233326027, 'alpha': 0.5866535195372065, 'gamma': 0.013146775009164212, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 409, 'max_depth': 7, 'min_child_weight': 17}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:15,453]\u001b[0m Trial 76 finished with value: 0.8720208945478289 and parameters: {'lambda': 0.5014190759670731, 'alpha': 1.1105042326067296, 'gamma': 0.03278269774381002, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.018, 'n_estimators': 411, 'max_depth': 11, 'min_child_weight': 178}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:18,915]\u001b[0m Trial 77 finished with value: 0.8801828272935031 and parameters: {'lambda': 0.13221917622281354, 'alpha': 0.643043710057096, 'gamma': 0.012974795490002088, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.018, 'n_estimators': 370, 'max_depth': 7, 'min_child_weight': 58}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:22,869]\u001b[0m Trial 78 finished with value: 0.8863858961802155 and parameters: {'lambda': 0.3436188825180726, 'alpha': 0.20151853860491595, 'gamma': 0.0075933472097133616, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.016, 'n_estimators': 406, 'max_depth': 20, 'min_child_weight': 19}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:25,424]\u001b[0m Trial 79 finished with value: 0.8788769180541952 and parameters: {'lambda': 1.0556193717837228, 'alpha': 0.2359350458088803, 'gamma': 0.007957094796666708, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.016, 'n_estimators': 388, 'max_depth': 7, 'min_child_weight': 47}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:28,531]\u001b[0m Trial 80 finished with value: 0.8834476003917727 and parameters: {'lambda': 1.4182749260060104, 'alpha': 0.5676446688708209, 'gamma': 0.006543401999979729, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.016, 'n_estimators': 429, 'max_depth': 7, 'min_child_weight': 36}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:31,854]\u001b[0m Trial 81 finished with value: 0.8801828272935031 and parameters: {'lambda': 0.42736147970851246, 'alpha': 0.0013848591565808076, 'gamma': 0.009929243565623806, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.300000012, 'n_estimators': 415, 'max_depth': 15, 'min_child_weight': 20}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:35,807]\u001b[0m Trial 82 finished with value: 0.8824681684622919 and parameters: {'lambda': 0.2326675343625911, 'alpha': 1.1262914054326592, 'gamma': 0.016069264588715375, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.018, 'n_estimators': 450, 'max_depth': 7, 'min_child_weight': 27}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:39,366]\u001b[0m Trial 83 finished with value: 0.881488736532811 and parameters: {'lambda': 0.6615002694536738, 'alpha': 0.7039952395595847, 'gamma': 0.005527540792045876, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.016, 'n_estimators': 408, 'max_depth': 9, 'min_child_weight': 17}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:46,465]\u001b[0m Trial 84 finished with value: 0.8863858961802155 and parameters: {'lambda': 0.34779655063705833, 'alpha': 0.004762261226030331, 'gamma': 0.028564649685327907, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.012, 'n_estimators': 492, 'max_depth': 11, 'min_child_weight': 8}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:28:50,301]\u001b[0m Trial 85 finished with value: 0.8762650995755795 and parameters: {'lambda': 0.3691857799204996, 'alpha': 0.35431177618007276, 'gamma': 0.003293885290927137, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.012, 'n_estimators': 488, 'max_depth': 11, 'min_child_weight': 66}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:55,316]\u001b[0m Trial 86 finished with value: 0.880835781913157 and parameters: {'lambda': 0.16442811947763183, 'alpha': 1.9480998485313987, 'gamma': 0.017961061351518835, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.012, 'n_estimators': 468, 'max_depth': 11, 'min_child_weight': 26}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:28:59,655]\u001b[0m Trial 87 finished with value: 0.8788769180541952 and parameters: {'lambda': 0.5942599284646878, 'alpha': 0.0015589617300639331, 'gamma': 0.04585237857440323, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 491, 'max_depth': 5, 'min_child_weight': 36}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:02,591]\u001b[0m Trial 88 finished with value: 0.8648383937316356 and parameters: {'lambda': 0.27676637172923413, 'alpha': 0.5553679355827534, 'gamma': 0.013046190786106145, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 464, 'max_depth': 17, 'min_child_weight': 211}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:09,597]\u001b[0m Trial 89 finished with value: 0.8876918054195233 and parameters: {'lambda': 0.09148726513159973, 'alpha': 0.011403325649455325, 'gamma': 0.008424011895488664, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 426, 'max_depth': 13, 'min_child_weight': 6}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:16,112]\u001b[0m Trial 90 finished with value: 0.8850799869409076 and parameters: {'lambda': 0.08671526076446667, 'alpha': 0.011344244366503069, 'gamma': 0.004449907053368087, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 431, 'max_depth': 13, 'min_child_weight': 8}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:21,892]\u001b[0m Trial 91 finished with value: 0.8860594188703885 and parameters: {'lambda': 0.4285480480606724, 'alpha': 0.006805708857232531, 'gamma': 0.00886225017019991, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 479, 'max_depth': 13, 'min_child_weight': 18}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:27,627]\u001b[0m Trial 92 finished with value: 0.8850799869409076 and parameters: {'lambda': 0.5011564621028759, 'alpha': 0.007866640260445715, 'gamma': 0.009222472780186328, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 477, 'max_depth': 13, 'min_child_weight': 19}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:35,124]\u001b[0m Trial 93 finished with value: 0.8847535096310807 and parameters: {'lambda': 0.32987198761271763, 'alpha': 0.00541268479332304, 'gamma': 0.00708294587090425, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 494, 'max_depth': 13, 'min_child_weight': 7}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:40,262]\u001b[0m Trial 94 finished with value: 0.8788769180541952 and parameters: {'lambda': 0.8857104605126273, 'alpha': 0.004389125578100636, 'gamma': 0.01016910065550711, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 499, 'max_depth': 13, 'min_child_weight': 43}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:45,158]\u001b[0m Trial 95 finished with value: 0.880835781913157 and parameters: {'lambda': 0.3926584623070133, 'alpha': 0.015226149177238482, 'gamma': 0.005993498779571164, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 462, 'max_depth': 13, 'min_child_weight': 32}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:56,677]\u001b[0m Trial 96 finished with value: 0.8844270323212536 and parameters: {'lambda': 0.11232196392721934, 'alpha': 0.003044346172761578, 'gamma': 0.014360726021369407, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 421, 'max_depth': 20, 'min_child_weight': 1}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:29:59,874]\u001b[0m Trial 97 finished with value: 0.8723473718576559 and parameters: {'lambda': 0.15701698729204303, 'alpha': 0.10746325677792722, 'gamma': 0.00758779061171277, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.02, 'n_estimators': 449, 'max_depth': 13, 'min_child_weight': 125}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:03,997]\u001b[0m Trial 98 finished with value: 0.8782239634345413 and parameters: {'lambda': 0.6744468405485962, 'alpha': 0.009758529635241733, 'gamma': 0.02426809950703454, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 481, 'max_depth': 6, 'min_child_weight': 51}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:10,641]\u001b[0m Trial 99 finished with value: 0.8782239634345413 and parameters: {'lambda': 0.4308517184317856, 'alpha': 0.2319181706259152, 'gamma': 0.011172749073432676, 'colsample_bytree': 0.9, 'subsample': 1.0, 'learning_rate': 0.300000012, 'n_estimators': 473, 'max_depth': 13, 'min_child_weight': 23}. Best is trial 60 with value: 0.8886712373490042.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:13,556]\u001b[0m Trial 0 finished with value: 0.871041462618348 and parameters: {'lambda': 0.007015367488271135, 'alpha': 3.329304336700877, 'gamma': 0.03259431309478603, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 288, 'max_depth': 15, 'min_child_weight': 111}. Best is trial 0 with value: 0.871041462618348.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:16,280]\u001b[0m Trial 1 finished with value: 0.8628795298726738 and parameters: {'lambda': 4.921368736086061, 'alpha': 3.9395275547938846, 'gamma': 0.4929549142645528, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.009, 'n_estimators': 373, 'max_depth': 15, 'min_child_weight': 181}. Best is trial 0 with value: 0.871041462618348.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:17,931]\u001b[0m Trial 2 finished with value: 0.8589618021547503 and parameters: {'lambda': 0.9737607007456459, 'alpha': 7.059509349170303, 'gamma': 0.005810692447434319, 'colsample_bytree': 0.7, 'subsample': 1.0, 'learning_rate': 0.012, 'n_estimators': 209, 'max_depth': 17, 'min_child_weight': 158}. Best is trial 0 with value: 0.871041462618348.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:19,246]\u001b[0m Trial 3 finished with value: 0.8527587332680379 and parameters: {'lambda': 5.929780385507121, 'alpha': 0.02072203967138034, 'gamma': 0.34803999556561505, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.016, 'n_estimators': 186, 'max_depth': 7, 'min_child_weight': 133}. Best is trial 0 with value: 0.871041462618348.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:21,695]\u001b[0m Trial 4 finished with value: 0.8775710088148874 and parameters: {'lambda': 0.015216446338400145, 'alpha': 2.983680438365325, 'gamma': 0.0029801616176719182, 'colsample_bytree': 0.4, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 275, 'max_depth': 11, 'min_child_weight': 34}. Best is trial 4 with value: 0.8775710088148874.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:26,328]\u001b[0m Trial 5 finished with value: 0.8837740777015998 and parameters: {'lambda': 0.0016169832572977267, 'alpha': 0.6483849150261617, 'gamma': 0.02206455296204213, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 201, 'max_depth': 9, 'min_child_weight': 13}. Best is trial 5 with value: 0.8837740777015998.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:29,141]\u001b[0m Trial 6 finished with value: 0.8847535096310807 and parameters: {'lambda': 0.6544714101033807, 'alpha': 1.4882053751566868, 'gamma': 0.0842820997948797, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.02, 'n_estimators': 245, 'max_depth': 5, 'min_child_weight': 3}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:30:31,326]\u001b[0m Trial 7 finished with value: 0.8723473718576559 and parameters: {'lambda': 0.00232654229083491, 'alpha': 0.38891259290068325, 'gamma': 0.02234210197214249, 'colsample_bytree': 0.8, 'subsample': 1.0, 'learning_rate': 0.300000012, 'n_estimators': 349, 'max_depth': 9, 'min_child_weight': 278}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:32,965]\u001b[0m Trial 8 finished with value: 0.8579823702252694 and parameters: {'lambda': 1.5978550007679164, 'alpha': 0.0016078971817456464, 'gamma': 0.06100189351340322, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.009, 'n_estimators': 252, 'max_depth': 15, 'min_child_weight': 234}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:34,534]\u001b[0m Trial 9 finished with value: 0.881815213842638 and parameters: {'lambda': 5.895018672564375, 'alpha': 3.0289027575550445, 'gamma': 0.17204797748012074, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.300000012, 'n_estimators': 225, 'max_depth': 11, 'min_child_weight': 28}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:35,940]\u001b[0m Trial 10 finished with value: 0.8609206660137121 and parameters: {'lambda': 0.002895207628163635, 'alpha': 0.001853795295767013, 'gamma': 0.07685652491091594, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 203, 'max_depth': 5, 'min_child_weight': 169}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:38,562]\u001b[0m Trial 11 finished with value: 0.8772445315050604 and parameters: {'lambda': 0.394023054874695, 'alpha': 7.963674502195475, 'gamma': 0.007931094269171164, 'colsample_bytree': 0.7, 'subsample': 1.0, 'learning_rate': 0.01, 'n_estimators': 356, 'max_depth': 17, 'min_child_weight': 82}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:40,614]\u001b[0m Trial 12 finished with value: 0.7538361083904669 and parameters: {'lambda': 0.0010977863704338454, 'alpha': 0.02170583380386952, 'gamma': 0.19858989001284402, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.008, 'n_estimators': 423, 'max_depth': 5, 'min_child_weight': 280}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:42,230]\u001b[0m Trial 13 finished with value: 0.8625530525628469 and parameters: {'lambda': 0.002264603820587734, 'alpha': 0.003497072472700452, 'gamma': 0.002951527410796196, 'colsample_bytree': 0.5, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 305, 'max_depth': 11, 'min_child_weight': 185}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:43,664]\u001b[0m Trial 14 finished with value: 0.8592882794645772 and parameters: {'lambda': 0.002752433688599918, 'alpha': 0.0037069724315616816, 'gamma': 0.006309419662777145, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.008, 'n_estimators': 312, 'max_depth': 9, 'min_child_weight': 256}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:46,021]\u001b[0m Trial 15 finished with value: 0.8609206660137121 and parameters: {'lambda': 0.05290160810694518, 'alpha': 0.02206241773046426, 'gamma': 0.538443209274791, 'colsample_bytree': 0.7, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 398, 'max_depth': 5, 'min_child_weight': 199}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:49,155]\u001b[0m Trial 16 finished with value: 0.8775710088148874 and parameters: {'lambda': 0.006179248368370559, 'alpha': 2.5203169019965252, 'gamma': 0.1455993340325494, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 340, 'max_depth': 7, 'min_child_weight': 59}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:52,433]\u001b[0m Trial 17 finished with value: 0.8756121449559255 and parameters: {'lambda': 0.005301259891965367, 'alpha': 0.270204402083948, 'gamma': 0.0010492959506132713, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.009, 'n_estimators': 401, 'max_depth': 11, 'min_child_weight': 64}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:30:56,229]\u001b[0m Trial 18 finished with value: 0.8752856676460986 and parameters: {'lambda': 0.003376816854252999, 'alpha': 7.188758115157529, 'gamma': 0.12100451599481564, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.014, 'n_estimators': 477, 'max_depth': 13, 'min_child_weight': 87}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:00,144]\u001b[0m Trial 19 finished with value: 0.8739797584067908 and parameters: {'lambda': 0.0010441984498110934, 'alpha': 3.2468744023382086, 'gamma': 0.0010922133296641667, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.008, 'n_estimators': 483, 'max_depth': 6, 'min_child_weight': 93}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:01,224]\u001b[0m Trial 20 finished with value: 0.8733268037871368 and parameters: {'lambda': 0.016511349208410474, 'alpha': 0.009897163824739005, 'gamma': 0.03569705917500647, 'colsample_bytree': 1.0, 'subsample': 0.5, 'learning_rate': 0.018, 'n_estimators': 125, 'max_depth': 15, 'min_child_weight': 51}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:01,830]\u001b[0m Trial 21 finished with value: 0.860594188703885 and parameters: {'lambda': 0.0020504916859959013, 'alpha': 0.0073737873585987225, 'gamma': 0.31482149112465363, 'colsample_bytree': 0.4, 'subsample': 0.7, 'learning_rate': 0.016, 'n_estimators': 107, 'max_depth': 15, 'min_child_weight': 119}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:03,737]\u001b[0m Trial 22 finished with value: 0.8765915768854065 and parameters: {'lambda': 0.3088410901805364, 'alpha': 0.019209925092898796, 'gamma': 0.002386148173640758, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.014, 'n_estimators': 186, 'max_depth': 6, 'min_child_weight': 9}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:05,979]\u001b[0m Trial 23 finished with value: 0.8132549787789749 and parameters: {'lambda': 0.5202088533152043, 'alpha': 0.1272338399255172, 'gamma': 0.014137383733280499, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.01, 'n_estimators': 476, 'max_depth': 15, 'min_child_weight': 220}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:07,442]\u001b[0m Trial 24 finished with value: 0.8579823702252694 and parameters: {'lambda': 0.01943439785821804, 'alpha': 0.012103388011403125, 'gamma': 0.0017831018766678536, 'colsample_bytree': 0.5, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 305, 'max_depth': 5, 'min_child_weight': 219}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:08,362]\u001b[0m Trial 25 finished with value: 0.861900097943193 and parameters: {'lambda': 0.033424098125405206, 'alpha': 1.764638576049377, 'gamma': 0.002394369281347584, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.02, 'n_estimators': 171, 'max_depth': 20, 'min_child_weight': 284}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:09,141]\u001b[0m Trial 26 finished with value: 0.8553705517466537 and parameters: {'lambda': 0.006908402649884586, 'alpha': 0.021025885803874107, 'gamma': 0.7823274784067749, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.01, 'n_estimators': 135, 'max_depth': 17, 'min_child_weight': 259}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:11,789]\u001b[0m Trial 27 finished with value: 0.8736532810969637 and parameters: {'lambda': 0.018185296827024244, 'alpha': 2.338582570059917, 'gamma': 0.0010946258019362092, 'colsample_bytree': 0.6, 'subsample': 0.9, 'learning_rate': 0.02, 'n_estimators': 382, 'max_depth': 5, 'min_child_weight': 117}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:13,174]\u001b[0m Trial 28 finished with value: 0.8543911198171728 and parameters: {'lambda': 0.29369486191528715, 'alpha': 0.0013728456253277697, 'gamma': 0.004855146987420637, 'colsample_bytree': 0.5, 'subsample': 0.7, 'learning_rate': 0.012, 'n_estimators': 251, 'max_depth': 11, 'min_child_weight': 248}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:31:14,740]\u001b[0m Trial 29 finished with value: 0.8788769180541952 and parameters: {'lambda': 1.7519986648029857, 'alpha': 0.007436283645776847, 'gamma': 0.17698218241463312, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.300000012, 'n_estimators': 239, 'max_depth': 6, 'min_child_weight': 84}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:16,511]\u001b[0m Trial 30 finished with value: 0.85145282402873 and parameters: {'lambda': 0.038289437290317516, 'alpha': 0.0017650391031599217, 'gamma': 0.04109597762289261, 'colsample_bytree': 0.8, 'subsample': 0.9, 'learning_rate': 0.008, 'n_estimators': 206, 'max_depth': 15, 'min_child_weight': 173}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:17,817]\u001b[0m Trial 31 finished with value: 0.8566764609859615 and parameters: {'lambda': 0.6498845698545593, 'alpha': 0.0014134451066722944, 'gamma': 0.04312759023735436, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.018, 'n_estimators': 126, 'max_depth': 13, 'min_child_weight': 161}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:19,689]\u001b[0m Trial 32 finished with value: 0.8671237349004244 and parameters: {'lambda': 0.012111938744147953, 'alpha': 0.009128999041105758, 'gamma': 0.00881890749419511, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.01, 'n_estimators': 114, 'max_depth': 7, 'min_child_weight': 41}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:21,829]\u001b[0m Trial 33 finished with value: 0.8579823702252694 and parameters: {'lambda': 4.038759928183588, 'alpha': 0.0037404811578612687, 'gamma': 0.07652104819635556, 'colsample_bytree': 0.6, 'subsample': 1.0, 'learning_rate': 0.009, 'n_estimators': 272, 'max_depth': 20, 'min_child_weight': 219}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:23,944]\u001b[0m Trial 34 finished with value: 0.8638589618021547 and parameters: {'lambda': 0.0010513577941184918, 'alpha': 6.5214731530613035, 'gamma': 0.005164719421271809, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.01, 'n_estimators': 237, 'max_depth': 6, 'min_child_weight': 190}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:26,968]\u001b[0m Trial 35 finished with value: 0.8537381651975188 and parameters: {'lambda': 1.941397626070725, 'alpha': 0.22923279531633028, 'gamma': 0.015430203871878056, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.016, 'n_estimators': 331, 'max_depth': 17, 'min_child_weight': 129}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:29,103]\u001b[0m Trial 36 finished with value: 0.7460006529546197 and parameters: {'lambda': 0.03937672848048772, 'alpha': 0.5528547943564728, 'gamma': 0.1268055213823672, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 444, 'max_depth': 13, 'min_child_weight': 293}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:33,420]\u001b[0m Trial 37 finished with value: 0.8556970290564806 and parameters: {'lambda': 0.41636019965769405, 'alpha': 1.5910704050861029, 'gamma': 0.010475471234689391, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.009, 'n_estimators': 486, 'max_depth': 6, 'min_child_weight': 159}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:34,662]\u001b[0m Trial 38 finished with value: 0.7995429317662422 and parameters: {'lambda': 0.0064858233028268035, 'alpha': 0.07557026184922155, 'gamma': 0.0018348558229161322, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.008, 'n_estimators': 226, 'max_depth': 17, 'min_child_weight': 273}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:35,732]\u001b[0m Trial 39 finished with value: 0.8530852105778648 and parameters: {'lambda': 0.0027825485381753595, 'alpha': 0.004192460974067728, 'gamma': 0.9518945473466243, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.02, 'n_estimators': 209, 'max_depth': 7, 'min_child_weight': 149}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:36,612]\u001b[0m Trial 40 finished with value: 0.8573294156056154 and parameters: {'lambda': 0.05262563284944762, 'alpha': 2.816152921934444, 'gamma': 0.006254398808206097, 'colsample_bytree': 0.7, 'subsample': 1.0, 'learning_rate': 0.008, 'n_estimators': 111, 'max_depth': 15, 'min_child_weight': 128}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:37,423]\u001b[0m Trial 41 finished with value: 0.8540646425073457 and parameters: {'lambda': 0.00113609618578442, 'alpha': 0.007122265475711753, 'gamma': 0.021020143166936317, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.008, 'n_estimators': 112, 'max_depth': 6, 'min_child_weight': 225}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:40,411]\u001b[0m Trial 42 finished with value: 0.8700620306888671 and parameters: {'lambda': 0.0015615264496285695, 'alpha': 0.0035044069167025466, 'gamma': 0.0023996983446220414, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.009, 'n_estimators': 318, 'max_depth': 11, 'min_child_weight': 101}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:43,158]\u001b[0m Trial 43 finished with value: 0.8762650995755795 and parameters: {'lambda': 2.1948186856177685, 'alpha': 0.02960282370867583, 'gamma': 0.005413188722819579, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.016, 'n_estimators': 117, 'max_depth': 13, 'min_child_weight': 27}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:45,969]\u001b[0m Trial 44 finished with value: 0.8504733920992491 and parameters: {'lambda': 1.917579889745149, 'alpha': 3.6653461143653474, 'gamma': 0.011859698889890828, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.014, 'n_estimators': 473, 'max_depth': 15, 'min_child_weight': 151}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:48,040]\u001b[0m Trial 45 finished with value: 0.7541625857002938 and parameters: {'lambda': 0.005640911856771093, 'alpha': 0.28102680453177487, 'gamma': 0.0010434266845030142, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.009, 'n_estimators': 418, 'max_depth': 20, 'min_child_weight': 277}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:51,642]\u001b[0m Trial 46 finished with value: 0.871041462618348 and parameters: {'lambda': 0.010642019978758457, 'alpha': 1.1055258162800157, 'gamma': 0.1926445233550498, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.02, 'n_estimators': 452, 'max_depth': 20, 'min_child_weight': 149}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:52,158]\u001b[0m Trial 47 finished with value: 0.8700620306888671 and parameters: {'lambda': 1.6105321777620132, 'alpha': 0.013643981110692587, 'gamma': 0.0011646803833264118, 'colsample_bytree': 0.7, 'subsample': 1.0, 'learning_rate': 0.300000012, 'n_estimators': 103, 'max_depth': 7, 'min_child_weight': 266}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:54,717]\u001b[0m Trial 48 finished with value: 0.8788769180541952 and parameters: {'lambda': 0.0012623365538713763, 'alpha': 0.0017925210394263182, 'gamma': 0.28895197444534904, 'colsample_bytree': 0.8, 'subsample': 1.0, 'learning_rate': 0.300000012, 'n_estimators': 439, 'max_depth': 20, 'min_child_weight': 187}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:31:58,449]\u001b[0m Trial 49 finished with value: 0.881162259222984 and parameters: {'lambda': 0.28328705917682945, 'alpha': 0.02133861774566848, 'gamma': 0.004356447253475179, 'colsample_bytree': 1.0, 'subsample': 1.0, 'learning_rate': 0.016, 'n_estimators': 293, 'max_depth': 13, 'min_child_weight': 59}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:00,805]\u001b[0m Trial 50 finished with value: 0.8831211230819458 and parameters: {'lambda': 2.2577804861313187, 'alpha': 0.03030566670186043, 'gamma': 0.056038401349635496, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 232, 'max_depth': 6, 'min_child_weight': 5}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:32:03,786]\u001b[0m Trial 51 finished with value: 0.8743062357166177 and parameters: {'lambda': 0.006694720550492909, 'alpha': 0.03989989852880341, 'gamma': 0.0051067831679929825, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.018, 'n_estimators': 372, 'max_depth': 5, 'min_child_weight': 102}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:06,248]\u001b[0m Trial 52 finished with value: 0.8560235063663075 and parameters: {'lambda': 0.0025030496347307287, 'alpha': 0.1606713467891515, 'gamma': 0.15071539070444087, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.009, 'n_estimators': 409, 'max_depth': 7, 'min_child_weight': 124}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:09,380]\u001b[0m Trial 53 finished with value: 0.8566764609859615 and parameters: {'lambda': 0.0054164727590674125, 'alpha': 0.7776632067001685, 'gamma': 0.05167555390844606, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.008, 'n_estimators': 476, 'max_depth': 13, 'min_child_weight': 240}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:13,475]\u001b[0m Trial 54 finished with value: 0.8824681684622919 and parameters: {'lambda': 0.02975904250192092, 'alpha': 0.3045189288018325, 'gamma': 0.003216642912768535, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.02, 'n_estimators': 488, 'max_depth': 7, 'min_child_weight': 27}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:15,609]\u001b[0m Trial 55 finished with value: 0.7962781586679726 and parameters: {'lambda': 0.5420184660181188, 'alpha': 0.4794391366754027, 'gamma': 0.11463325621943196, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.009, 'n_estimators': 409, 'max_depth': 5, 'min_child_weight': 219}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:17,334]\u001b[0m Trial 56 finished with value: 0.8681031668299053 and parameters: {'lambda': 0.26795453610052306, 'alpha': 1.892461635795704, 'gamma': 0.22086790774215678, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.014, 'n_estimators': 290, 'max_depth': 7, 'min_child_weight': 96}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:18,625]\u001b[0m Trial 57 finished with value: 0.8723473718576559 and parameters: {'lambda': 5.614819463696719, 'alpha': 0.013510446732318961, 'gamma': 0.3126501075245503, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.014, 'n_estimators': 183, 'max_depth': 13, 'min_child_weight': 45}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:21,007]\u001b[0m Trial 58 finished with value: 0.8635324844923278 and parameters: {'lambda': 0.0017649804969689555, 'alpha': 6.397686535656922, 'gamma': 0.01913471585926218, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.012, 'n_estimators': 409, 'max_depth': 20, 'min_child_weight': 181}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:22,044]\u001b[0m Trial 59 finished with value: 0.871694417238002 and parameters: {'lambda': 0.03592970961777296, 'alpha': 0.005781845944633123, 'gamma': 0.2218395694405535, 'colsample_bytree': 0.5, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 238, 'max_depth': 13, 'min_child_weight': 204}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:24,385]\u001b[0m Trial 60 finished with value: 0.8752856676460986 and parameters: {'lambda': 0.007894860993630765, 'alpha': 0.9272451733300492, 'gamma': 0.22630042207945847, 'colsample_bytree': 0.4, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 450, 'max_depth': 15, 'min_child_weight': 76}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:27,057]\u001b[0m Trial 61 finished with value: 0.8730003264773099 and parameters: {'lambda': 0.005418071370136922, 'alpha': 0.0033328074781260883, 'gamma': 0.032354400783693435, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 463, 'max_depth': 7, 'min_child_weight': 97}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:28,690]\u001b[0m Trial 62 finished with value: 0.871367939928175 and parameters: {'lambda': 1.8750313922558755, 'alpha': 0.13505242886923766, 'gamma': 0.002982204901847155, 'colsample_bytree': 0.4, 'subsample': 0.7, 'learning_rate': 0.014, 'n_estimators': 354, 'max_depth': 11, 'min_child_weight': 185}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:31,750]\u001b[0m Trial 63 finished with value: 0.8821416911524649 and parameters: {'lambda': 0.002947945827603621, 'alpha': 7.056044207212661, 'gamma': 0.09266493143125097, 'colsample_bytree': 0.6, 'subsample': 0.7, 'learning_rate': 0.012, 'n_estimators': 413, 'max_depth': 6, 'min_child_weight': 6}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:35,333]\u001b[0m Trial 64 finished with value: 0.880835781913157 and parameters: {'lambda': 0.03925684406873479, 'alpha': 0.001876753158356866, 'gamma': 0.7855529070900894, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.016, 'n_estimators': 471, 'max_depth': 7, 'min_child_weight': 25}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:36,381]\u001b[0m Trial 65 finished with value: 0.8720208945478289 and parameters: {'lambda': 0.001011074227845754, 'alpha': 0.0024159796405683696, 'gamma': 0.4447728115676469, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.02, 'n_estimators': 150, 'max_depth': 9, 'min_child_weight': 75}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:38,468]\u001b[0m Trial 66 finished with value: 0.8769180541952334 and parameters: {'lambda': 9.070083283444164, 'alpha': 0.2013705205262006, 'gamma': 0.004295988481601996, 'colsample_bytree': 1.0, 'subsample': 1.0, 'learning_rate': 0.016, 'n_estimators': 257, 'max_depth': 17, 'min_child_weight': 71}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:39,137]\u001b[0m Trial 67 finished with value: 0.8589618021547503 and parameters: {'lambda': 5.266485237378492, 'alpha': 0.07997998120155735, 'gamma': 0.1579434800421412, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.02, 'n_estimators': 118, 'max_depth': 17, 'min_child_weight': 240}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:41,268]\u001b[0m Trial 68 finished with value: 0.8540646425073457 and parameters: {'lambda': 3.786778068804496, 'alpha': 0.309330543423952, 'gamma': 0.3820280028201364, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.012, 'n_estimators': 411, 'max_depth': 5, 'min_child_weight': 135}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:43,200]\u001b[0m Trial 69 finished with value: 0.8543911198171728 and parameters: {'lambda': 0.7754431565776088, 'alpha': 1.6674688786040597, 'gamma': 0.2976715662221009, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.018, 'n_estimators': 288, 'max_depth': 6, 'min_child_weight': 300}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:45,496]\u001b[0m Trial 70 finished with value: 0.8494939601697682 and parameters: {'lambda': 0.0033063905534760013, 'alpha': 0.006469286663396586, 'gamma': 0.00436360808922806, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.02, 'n_estimators': 375, 'max_depth': 6, 'min_child_weight': 273}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:48,523]\u001b[0m Trial 71 finished with value: 0.8697355533790402 and parameters: {'lambda': 0.028632071120111683, 'alpha': 0.018443891646608965, 'gamma': 0.04921566067077558, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 434, 'max_depth': 5, 'min_child_weight': 164}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:49,383]\u001b[0m Trial 72 finished with value: 0.8560235063663075 and parameters: {'lambda': 0.14588931019657359, 'alpha': 0.1142688896060402, 'gamma': 0.018499395708530104, 'colsample_bytree': 0.6, 'subsample': 1.0, 'learning_rate': 0.01, 'n_estimators': 103, 'max_depth': 5, 'min_child_weight': 224}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:32:50,565]\u001b[0m Trial 73 finished with value: 0.8589618021547503 and parameters: {'lambda': 0.01488942270301306, 'alpha': 0.005884173662608366, 'gamma': 0.0021581923210066685, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.014, 'n_estimators': 128, 'max_depth': 5, 'min_child_weight': 72}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:52,285]\u001b[0m Trial 74 finished with value: 0.8671237349004244 and parameters: {'lambda': 0.13771683296867934, 'alpha': 0.01950647248912753, 'gamma': 0.011078580356736093, 'colsample_bytree': 0.4, 'subsample': 0.7, 'learning_rate': 0.008, 'n_estimators': 242, 'max_depth': 11, 'min_child_weight': 104}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:53,243]\u001b[0m Trial 75 finished with value: 0.8775710088148874 and parameters: {'lambda': 0.38252917829620264, 'alpha': 0.01126131332832667, 'gamma': 0.0014039906865912467, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.300000012, 'n_estimators': 198, 'max_depth': 20, 'min_child_weight': 118}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:54,062]\u001b[0m Trial 76 finished with value: 0.8684296441397323 and parameters: {'lambda': 0.3909875491502517, 'alpha': 0.010980677020045683, 'gamma': 0.4935248772487784, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.02, 'n_estimators': 141, 'max_depth': 13, 'min_child_weight': 154}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:55,221]\u001b[0m Trial 77 finished with value: 0.8586353248449233 and parameters: {'lambda': 0.035896291517408044, 'alpha': 0.024501147131475016, 'gamma': 0.0010263587333510756, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.012, 'n_estimators': 185, 'max_depth': 6, 'min_child_weight': 142}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:56,333]\u001b[0m Trial 78 finished with value: 0.85145282402873 and parameters: {'lambda': 0.02344353221667329, 'alpha': 4.434557625983413, 'gamma': 0.09177315137077129, 'colsample_bytree': 1.0, 'subsample': 0.5, 'learning_rate': 0.01, 'n_estimators': 158, 'max_depth': 13, 'min_child_weight': 100}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:32:58,291]\u001b[0m Trial 79 finished with value: 0.8749591903362717 and parameters: {'lambda': 5.434508438354595, 'alpha': 0.0014308903072662977, 'gamma': 0.41355402684810877, 'colsample_bytree': 1.0, 'subsample': 0.5, 'learning_rate': 0.009, 'n_estimators': 214, 'max_depth': 17, 'min_child_weight': 33}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:00,672]\u001b[0m Trial 80 finished with value: 0.8383937316356513 and parameters: {'lambda': 0.335935041703913, 'alpha': 1.1531313391311062, 'gamma': 0.023241246089696063, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.01, 'n_estimators': 446, 'max_depth': 6, 'min_child_weight': 265}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:02,160]\u001b[0m Trial 81 finished with value: 0.861573620633366 and parameters: {'lambda': 1.2559675835950248, 'alpha': 0.0014197814423131201, 'gamma': 0.0026425053337430313, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.018, 'n_estimators': 231, 'max_depth': 9, 'min_child_weight': 142}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:03,976]\u001b[0m Trial 82 finished with value: 0.772118837740777 and parameters: {'lambda': 0.0034972006294786105, 'alpha': 0.05200132384087435, 'gamma': 0.750498492161482, 'colsample_bytree': 0.6, 'subsample': 0.4, 'learning_rate': 0.01, 'n_estimators': 401, 'max_depth': 9, 'min_child_weight': 252}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:05,703]\u001b[0m Trial 83 finished with value: 0.8543911198171728 and parameters: {'lambda': 0.0021869176865638284, 'alpha': 6.321088823644743, 'gamma': 0.03439157669834326, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 278, 'max_depth': 11, 'min_child_weight': 188}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:08,184]\u001b[0m Trial 84 finished with value: 0.8690825987593862 and parameters: {'lambda': 0.7720206259884356, 'alpha': 8.904717691387008, 'gamma': 0.0014266601411136819, 'colsample_bytree': 0.6, 'subsample': 0.5, 'learning_rate': 0.014, 'n_estimators': 451, 'max_depth': 11, 'min_child_weight': 112}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:10,450]\u001b[0m Trial 85 finished with value: 0.8687561214495593 and parameters: {'lambda': 0.005416772450999886, 'alpha': 0.0046673895579432394, 'gamma': 0.013439569697983551, 'colsample_bytree': 0.9, 'subsample': 1.0, 'learning_rate': 0.009, 'n_estimators': 355, 'max_depth': 7, 'min_child_weight': 210}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:12,247]\u001b[0m Trial 86 finished with value: 0.8707149853085211 and parameters: {'lambda': 2.065816232383767, 'alpha': 0.49065219525052994, 'gamma': 0.23044447355356068, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.02, 'n_estimators': 263, 'max_depth': 15, 'min_child_weight': 92}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:14,317]\u001b[0m Trial 87 finished with value: 0.8700620306888671 and parameters: {'lambda': 1.9806108215611173, 'alpha': 0.0026457169193033333, 'gamma': 0.02039567167193743, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.008, 'n_estimators': 297, 'max_depth': 20, 'min_child_weight': 123}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:15,387]\u001b[0m Trial 88 finished with value: 0.8743062357166177 and parameters: {'lambda': 2.8542102821904143, 'alpha': 0.022557090196796786, 'gamma': 0.11787012221268908, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.01, 'n_estimators': 129, 'max_depth': 9, 'min_child_weight': 21}. Best is trial 6 with value: 0.8847535096310807.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:20,944]\u001b[0m Trial 89 finished with value: 0.8863858961802155 and parameters: {'lambda': 0.0017966925626134247, 'alpha': 0.002196357078637659, 'gamma': 0.01539799506348639, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.014, 'n_estimators': 399, 'max_depth': 11, 'min_child_weight': 9}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:23,275]\u001b[0m Trial 90 finished with value: 0.8756121449559255 and parameters: {'lambda': 0.8138106086481933, 'alpha': 0.04304958595926792, 'gamma': 0.334087176648611, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.009, 'n_estimators': 326, 'max_depth': 15, 'min_child_weight': 97}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:24,344]\u001b[0m Trial 91 finished with value: 0.871367939928175 and parameters: {'lambda': 0.0841769924350261, 'alpha': 0.002945932741740701, 'gamma': 0.009973966073231989, 'colsample_bytree': 0.4, 'subsample': 0.7, 'learning_rate': 0.02, 'n_estimators': 205, 'max_depth': 20, 'min_child_weight': 160}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:26,151]\u001b[0m Trial 92 finished with value: 0.8723473718576559 and parameters: {'lambda': 3.9297051733594386, 'alpha': 0.1539460470243888, 'gamma': 0.002188964994832583, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.016, 'n_estimators': 410, 'max_depth': 7, 'min_child_weight': 159}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:28,099]\u001b[0m Trial 93 finished with value: 0.8442703232125367 and parameters: {'lambda': 6.466823090083628, 'alpha': 2.3504446127231744, 'gamma': 0.6137424095583558, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.02, 'n_estimators': 477, 'max_depth': 17, 'min_child_weight': 184}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:29,510]\u001b[0m Trial 94 finished with value: 0.850799869409076 and parameters: {'lambda': 3.749977722138479, 'alpha': 2.7536213327923895, 'gamma': 0.13934740839937784, 'colsample_bytree': 1.0, 'subsample': 0.5, 'learning_rate': 0.01, 'n_estimators': 248, 'max_depth': 7, 'min_child_weight': 193}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:33:31,986]\u001b[0m Trial 95 finished with value: 0.8694090760692131 and parameters: {'lambda': 0.002060028077120148, 'alpha': 0.011653499485161797, 'gamma': 0.0045826618290063246, 'colsample_bytree': 0.9, 'subsample': 0.7, 'learning_rate': 0.01, 'n_estimators': 375, 'max_depth': 17, 'min_child_weight': 132}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:36,569]\u001b[0m Trial 96 finished with value: 0.8841005550114267 and parameters: {'lambda': 9.149824562015786, 'alpha': 0.022412585387022708, 'gamma': 0.07582914163806408, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.008, 'n_estimators': 349, 'max_depth': 17, 'min_child_weight': 2}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:39,245]\u001b[0m Trial 97 finished with value: 0.8553705517466537 and parameters: {'lambda': 0.055734943294242646, 'alpha': 0.013427662898145263, 'gamma': 0.0020512225235654598, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.01, 'n_estimators': 468, 'max_depth': 17, 'min_child_weight': 243}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:40,740]\u001b[0m Trial 98 finished with value: 0.8599412340842312 and parameters: {'lambda': 1.5328238239706735, 'alpha': 0.011357309504068994, 'gamma': 0.02147954990986203, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 281, 'max_depth': 5, 'min_child_weight': 285}. Best is trial 89 with value: 0.8863858961802155.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:33:41,991]\u001b[0m Trial 99 finished with value: 0.8896506692784851 and parameters: {'lambda': 0.0011531391910478527, 'alpha': 0.023352619651264134, 'gamma': 0.07934417197895705, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 205, 'max_depth': 20, 'min_child_weight': 79}. Best is trial 99 with value: 0.8896506692784851.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "random_study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.RandomSampler(),\n",
    ")\n",
    "\n",
    "tpe_study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    ")\n",
    "\n",
    "cmaes_study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.CmaEsSampler(),\n",
    ")\n",
    "\n",
    "cmaes_study.optimize(objective, n_trials=100)\n",
    "tpe_study.optimize(objective, n_trials=100)\n",
    "random_study.optimize(objective, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"n_estimators\": [100, 500, 1000],\n",
    "    \"max_depth\": [5,6,7,9,11,13,15,17,20],\n",
    "    \"min_samples_split\": [0.1, 1.0],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    \n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators',100,500),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5,6,7,9,11,13,15,17,20]),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param)  \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 14:53:33,439]\u001b[0m A new study created in memory with name: no-name-a7e77fbd-fb3b-4d59-a434-4c09c56dda64\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:53:35,891]\u001b[0m Trial 0 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 0 with value: 0.8844270323212536.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 14:54:25,143]\u001b[0m Trial 1 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 0 with value: 0.8844270323212536.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 14:55:12,098]\u001b[0m Trial 2 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 0 with value: 0.8844270323212536.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 14:55:50,217]\u001b[0m Trial 3 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 0 with value: 0.8844270323212536.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:55:52,586]\u001b[0m Trial 4 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:55:56,861]\u001b[0m Trial 5 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:55:59,026]\u001b[0m Trial 6 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:56:23,336]\u001b[0m Trial 7 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 14:57:00,727]\u001b[0m Trial 8 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:57:06,283]\u001b[0m Trial 9 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:57:08,357]\u001b[0m Trial 10 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:57:17,798]\u001b[0m Trial 11 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 14:57:38,108]\u001b[0m Trial 12 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:57:43,157]\u001b[0m Trial 13 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:57:50,003]\u001b[0m Trial 14 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:57:51,900]\u001b[0m Trial 15 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:58:01,253]\u001b[0m Trial 16 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:58:17,204]\u001b[0m Trial 17 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 14:58:35,420]\u001b[0m Trial 18 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:58:56,639]\u001b[0m Trial 19 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:59:09,416]\u001b[0m Trial 20 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:59:13,507]\u001b[0m Trial 21 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 14:59:38,596]\u001b[0m Trial 22 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:00:05,601]\u001b[0m Trial 23 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:00:36,417]\u001b[0m Trial 24 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:00:58,087]\u001b[0m Trial 25 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:01:04,246]\u001b[0m Trial 26 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:01:09,581]\u001b[0m Trial 27 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:01:11,904]\u001b[0m Trial 28 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:01:54,381]\u001b[0m Trial 29 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:02:00,473]\u001b[0m Trial 30 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:02:06,631]\u001b[0m Trial 31 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:02:08,891]\u001b[0m Trial 32 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:02:18,822]\u001b[0m Trial 33 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:02:37,878]\u001b[0m Trial 34 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:02:53,139]\u001b[0m Trial 35 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:03:37,072]\u001b[0m Trial 36 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:03:40,955]\u001b[0m Trial 37 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:04:00,716]\u001b[0m Trial 38 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:04:03,414]\u001b[0m Trial 39 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:04:44,455]\u001b[0m Trial 40 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:05:36,470]\u001b[0m Trial 41 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:06:36,586]\u001b[0m Trial 42 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:07:11,319]\u001b[0m Trial 43 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:08:04,627]\u001b[0m Trial 44 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:08:57,737]\u001b[0m Trial 45 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:09:56,362]\u001b[0m Trial 46 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:10:22,080]\u001b[0m Trial 47 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:10:47,678]\u001b[0m Trial 48 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 15:11:47,308]\u001b[0m Trial 49 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:12:35,678]\u001b[0m Trial 50 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:12:39,515]\u001b[0m Trial 51 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:12:42,116]\u001b[0m Trial 52 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:13:06,349]\u001b[0m Trial 53 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:13:08,687]\u001b[0m Trial 54 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:14:00,198]\u001b[0m Trial 55 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:14:14,723]\u001b[0m Trial 56 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:14:20,310]\u001b[0m Trial 57 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:14:23,151]\u001b[0m Trial 58 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:14:29,022]\u001b[0m Trial 59 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:14:35,951]\u001b[0m Trial 60 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:15:22,601]\u001b[0m Trial 61 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:15:57,410]\u001b[0m Trial 62 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:16:03,452]\u001b[0m Trial 63 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:16:05,890]\u001b[0m Trial 64 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:16:08,382]\u001b[0m Trial 65 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:16:21,111]\u001b[0m Trial 66 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:16:41,873]\u001b[0m Trial 67 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:16:48,304]\u001b[0m Trial 68 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:17:16,276]\u001b[0m Trial 69 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:17:18,386]\u001b[0m Trial 70 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:18:09,911]\u001b[0m Trial 71 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:18:17,042]\u001b[0m Trial 72 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:18:41,612]\u001b[0m Trial 73 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:19:09,709]\u001b[0m Trial 74 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:19:33,749]\u001b[0m Trial 75 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:20:01,740]\u001b[0m Trial 76 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:20:35,264]\u001b[0m Trial 77 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:20:38,363]\u001b[0m Trial 78 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:20:46,091]\u001b[0m Trial 79 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:20:53,101]\u001b[0m Trial 80 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:21:53,190]\u001b[0m Trial 81 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:22:06,568]\u001b[0m Trial 82 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:23:07,705]\u001b[0m Trial 83 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:23:42,347]\u001b[0m Trial 84 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:23:56,638]\u001b[0m Trial 85 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:23:59,999]\u001b[0m Trial 86 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:24:06,466]\u001b[0m Trial 87 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:24:11,022]\u001b[0m Trial 88 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:24:15,858]\u001b[0m Trial 89 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:24:41,871]\u001b[0m Trial 90 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:25:43,911]\u001b[0m Trial 91 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:26:41,529]\u001b[0m Trial 92 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:27:17,118]\u001b[0m Trial 93 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:27:53,425]\u001b[0m Trial 94 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:27:58,832]\u001b[0m Trial 95 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:28:55,419]\u001b[0m Trial 96 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:29:37,881]\u001b[0m Trial 97 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:30:15,348]\u001b[0m Trial 98 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:30:24,900]\u001b[0m Trial 99 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:30:31,802]\u001b[0m Trial 100 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:30:38,920]\u001b[0m Trial 101 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:31:16,505]\u001b[0m Trial 102 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 15:31:55,748]\u001b[0m Trial 103 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:32:04,625]\u001b[0m Trial 104 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:32:08,436]\u001b[0m Trial 105 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:33:09,491]\u001b[0m Trial 106 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:33:59,356]\u001b[0m Trial 107 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:34:32,834]\u001b[0m Trial 108 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:34:38,463]\u001b[0m Trial 109 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:35:44,006]\u001b[0m Trial 110 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:35:49,035]\u001b[0m Trial 111 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:36:55,549]\u001b[0m Trial 112 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:37:01,987]\u001b[0m Trial 113 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:37:31,963]\u001b[0m Trial 114 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:37:50,255]\u001b[0m Trial 115 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:38:34,767]\u001b[0m Trial 116 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:39:05,760]\u001b[0m Trial 117 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:39:27,709]\u001b[0m Trial 118 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:39:33,532]\u001b[0m Trial 119 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:39:40,883]\u001b[0m Trial 120 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:40:16,382]\u001b[0m Trial 121 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:40:24,015]\u001b[0m Trial 122 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:41:07,637]\u001b[0m Trial 123 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:41:14,060]\u001b[0m Trial 124 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:41:23,782]\u001b[0m Trial 125 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:41:25,849]\u001b[0m Trial 126 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:41:30,490]\u001b[0m Trial 127 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:41:32,424]\u001b[0m Trial 128 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:41:37,524]\u001b[0m Trial 129 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 15:42:20,500]\u001b[0m Trial 130 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:42:29,393]\u001b[0m Trial 131 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:42:57,012]\u001b[0m Trial 132 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:43:24,719]\u001b[0m Trial 133 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:43:30,414]\u001b[0m Trial 134 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:43:32,259]\u001b[0m Trial 135 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:43:34,933]\u001b[0m Trial 136 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:43:37,200]\u001b[0m Trial 137 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:44:14,086]\u001b[0m Trial 138 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:44:24,371]\u001b[0m Trial 139 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:45:04,414]\u001b[0m Trial 140 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:45:06,544]\u001b[0m Trial 141 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:45:11,242]\u001b[0m Trial 142 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:45:13,189]\u001b[0m Trial 143 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:45:25,592]\u001b[0m Trial 144 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:46:14,465]\u001b[0m Trial 145 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:46:16,483]\u001b[0m Trial 146 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:47:00,151]\u001b[0m Trial 147 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:47:02,314]\u001b[0m Trial 148 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:47:45,961]\u001b[0m Trial 149 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:47:48,585]\u001b[0m Trial 150 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:47:54,019]\u001b[0m Trial 151 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:48:04,466]\u001b[0m Trial 152 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:48:06,927]\u001b[0m Trial 153 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:48:43,427]\u001b[0m Trial 154 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:49:25,724]\u001b[0m Trial 155 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:49:31,269]\u001b[0m Trial 156 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:49:33,985]\u001b[0m Trial 157 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:50:04,687]\u001b[0m Trial 158 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:50:07,171]\u001b[0m Trial 159 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:50:52,606]\u001b[0m Trial 160 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:51:32,462]\u001b[0m Trial 161 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:52:14,067]\u001b[0m Trial 162 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:52:57,575]\u001b[0m Trial 163 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:53:33,806]\u001b[0m Trial 164 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:53:35,600]\u001b[0m Trial 165 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:53:59,719]\u001b[0m Trial 166 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:54:20,631]\u001b[0m Trial 167 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:54:22,802]\u001b[0m Trial 168 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:54:25,305]\u001b[0m Trial 169 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:54:30,370]\u001b[0m Trial 170 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:54:42,429]\u001b[0m Trial 171 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:55:05,459]\u001b[0m Trial 172 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:55:49,481]\u001b[0m Trial 173 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:56:12,975]\u001b[0m Trial 174 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:56:37,506]\u001b[0m Trial 175 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:56:43,393]\u001b[0m Trial 176 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:56:54,026]\u001b[0m Trial 177 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:56:56,600]\u001b[0m Trial 178 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:57:09,221]\u001b[0m Trial 179 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:57:11,360]\u001b[0m Trial 180 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 15:57:31,896]\u001b[0m Trial 181 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:57:52,652]\u001b[0m Trial 182 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 15:58:13,751]\u001b[0m Trial 183 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:58:37,257]\u001b[0m Trial 184 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:58:49,074]\u001b[0m Trial 185 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:58:52,889]\u001b[0m Trial 186 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:58:58,867]\u001b[0m Trial 187 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 15:59:21,853]\u001b[0m Trial 188 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:00:05,487]\u001b[0m Trial 189 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:00:10,763]\u001b[0m Trial 190 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:00:34,807]\u001b[0m Trial 191 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:00:38,702]\u001b[0m Trial 192 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:00:41,839]\u001b[0m Trial 193 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:01:05,437]\u001b[0m Trial 194 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:01:34,859]\u001b[0m Trial 195 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:01:59,692]\u001b[0m Trial 196 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:02:05,869]\u001b[0m Trial 197 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:02:09,604]\u001b[0m Trial 198 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:02:44,093]\u001b[0m Trial 199 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:03:10,517]\u001b[0m Trial 200 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:03:23,744]\u001b[0m Trial 201 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:03:49,477]\u001b[0m Trial 202 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:05:07,963]\u001b[0m Trial 203 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:05:16,813]\u001b[0m Trial 204 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:05:45,671]\u001b[0m Trial 205 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:06:54,202]\u001b[0m Trial 206 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 16:07:02,855]\u001b[0m Trial 207 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:08:12,923]\u001b[0m Trial 208 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:09:13,809]\u001b[0m Trial 209 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:10:31,373]\u001b[0m Trial 210 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:10:37,599]\u001b[0m Trial 211 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:10:42,588]\u001b[0m Trial 212 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:11:29,564]\u001b[0m Trial 213 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:11:38,992]\u001b[0m Trial 214 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:11:44,572]\u001b[0m Trial 215 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:12:07,920]\u001b[0m Trial 216 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:12:28,350]\u001b[0m Trial 217 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:13:57,482]\u001b[0m Trial 218 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:14:07,977]\u001b[0m Trial 219 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:14:52,574]\u001b[0m Trial 220 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:16:16,897]\u001b[0m Trial 221 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:17:16,536]\u001b[0m Trial 222 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:17:22,841]\u001b[0m Trial 223 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:17:32,884]\u001b[0m Trial 224 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:18:17,075]\u001b[0m Trial 225 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:18:35,692]\u001b[0m Trial 226 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:19:33,034]\u001b[0m Trial 227 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:20:13,139]\u001b[0m Trial 228 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:20:46,960]\u001b[0m Trial 229 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:20:53,596]\u001b[0m Trial 230 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 16:21:30,402]\u001b[0m Trial 231 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:22:38,691]\u001b[0m Trial 232 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:23:15,702]\u001b[0m Trial 233 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:23:45,742]\u001b[0m Trial 234 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:23:49,940]\u001b[0m Trial 235 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:25:00,716]\u001b[0m Trial 236 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:25:08,475]\u001b[0m Trial 237 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:25:27,496]\u001b[0m Trial 238 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:25:48,419]\u001b[0m Trial 239 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:25:52,891]\u001b[0m Trial 240 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:26:30,963]\u001b[0m Trial 241 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:26:59,832]\u001b[0m Trial 242 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:28:10,354]\u001b[0m Trial 243 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:28:26,748]\u001b[0m Trial 244 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:29:03,766]\u001b[0m Trial 245 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:29:22,386]\u001b[0m Trial 246 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:29:28,231]\u001b[0m Trial 247 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:29:37,876]\u001b[0m Trial 248 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:30:18,720]\u001b[0m Trial 249 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:30:47,590]\u001b[0m Trial 250 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:31:35,964]\u001b[0m Trial 251 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:31:55,227]\u001b[0m Trial 252 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:32:28,973]\u001b[0m Trial 253 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:33:29,621]\u001b[0m Trial 254 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:33:51,302]\u001b[0m Trial 255 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:33:59,018]\u001b[0m Trial 256 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:35:11,455]\u001b[0m Trial 257 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:36:24,982]\u001b[0m Trial 258 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 16:37:28,040]\u001b[0m Trial 259 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:37:38,068]\u001b[0m Trial 260 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:37:45,101]\u001b[0m Trial 261 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:37:53,238]\u001b[0m Trial 262 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:38:26,117]\u001b[0m Trial 263 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:38:57,550]\u001b[0m Trial 264 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:39:01,344]\u001b[0m Trial 265 finished with value: 0.8844270323212536 and parameters: {'n_estimators': 100, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:39:43,786]\u001b[0m Trial 266 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:39:47,997]\u001b[0m Trial 267 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:39:53,572]\u001b[0m Trial 268 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:40:36,128]\u001b[0m Trial 269 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:40:45,859]\u001b[0m Trial 270 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:41:30,086]\u001b[0m Trial 271 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:41:36,182]\u001b[0m Trial 272 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:42:49,359]\u001b[0m Trial 273 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:42:53,800]\u001b[0m Trial 274 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:43:09,351]\u001b[0m Trial 275 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:44:30,464]\u001b[0m Trial 276 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:44:40,398]\u001b[0m Trial 277 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:45:19,020]\u001b[0m Trial 278 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:46:00,789]\u001b[0m Trial 279 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:46:53,095]\u001b[0m Trial 280 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:47:11,503]\u001b[0m Trial 281 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:47:37,534]\u001b[0m Trial 282 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:48:07,395]\u001b[0m Trial 283 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:48:50,465]\u001b[0m Trial 284 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:49:23,268]\u001b[0m Trial 285 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:49:33,713]\u001b[0m Trial 286 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:50:12,532]\u001b[0m Trial 287 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:50:21,741]\u001b[0m Trial 288 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:50:53,490]\u001b[0m Trial 289 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:51:11,883]\u001b[0m Trial 290 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:51:48,855]\u001b[0m Trial 291 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:52:19,817]\u001b[0m Trial 292 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:53:24,098]\u001b[0m Trial 293 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:54:27,944]\u001b[0m Trial 294 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:55:11,702]\u001b[0m Trial 295 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:55:27,335]\u001b[0m Trial 296 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:56:04,363]\u001b[0m Trial 297 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:56:34,097]\u001b[0m Trial 298 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 16:57:33,050]\u001b[0m Trial 299 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:58:01,900]\u001b[0m Trial 300 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:58:09,132]\u001b[0m Trial 301 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:58:13,807]\u001b[0m Trial 302 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:58:17,117]\u001b[0m Trial 303 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:58:49,365]\u001b[0m Trial 304 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:59:11,142]\u001b[0m Trial 305 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:59:15,851]\u001b[0m Trial 306 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 16:59:58,992]\u001b[0m Trial 307 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:01:13,122]\u001b[0m Trial 308 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:01:17,829]\u001b[0m Trial 309 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:01:48,182]\u001b[0m Trial 310 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 17:02:47,003]\u001b[0m Trial 311 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:02:56,535]\u001b[0m Trial 312 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:02:59,877]\u001b[0m Trial 313 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:03:28,514]\u001b[0m Trial 314 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:03:59,341]\u001b[0m Trial 315 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:04:29,390]\u001b[0m Trial 316 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:04:38,286]\u001b[0m Trial 317 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:04:56,460]\u001b[0m Trial 318 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:05:22,966]\u001b[0m Trial 319 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:06:40,773]\u001b[0m Trial 320 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:07:19,988]\u001b[0m Trial 321 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:07:28,316]\u001b[0m Trial 322 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:07:33,931]\u001b[0m Trial 323 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:07:42,112]\u001b[0m Trial 324 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:07:49,091]\u001b[0m Trial 325 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:08:16,131]\u001b[0m Trial 326 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:08:19,684]\u001b[0m Trial 327 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:08:26,497]\u001b[0m Trial 328 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:08:29,844]\u001b[0m Trial 329 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:08:38,301]\u001b[0m Trial 330 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:09:02,082]\u001b[0m Trial 331 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:09:27,819]\u001b[0m Trial 332 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:09:34,148]\u001b[0m Trial 333 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:09:39,694]\u001b[0m Trial 334 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:09:45,718]\u001b[0m Trial 335 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:10:33,004]\u001b[0m Trial 336 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:11:18,643]\u001b[0m Trial 337 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:11:44,319]\u001b[0m Trial 338 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:12:07,302]\u001b[0m Trial 339 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:12:31,084]\u001b[0m Trial 340 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:13:08,252]\u001b[0m Trial 341 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:13:36,138]\u001b[0m Trial 342 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:13:49,218]\u001b[0m Trial 343 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:14:05,737]\u001b[0m Trial 344 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:14:53,724]\u001b[0m Trial 345 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:15:38,099]\u001b[0m Trial 346 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:16:03,594]\u001b[0m Trial 347 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:16:18,810]\u001b[0m Trial 348 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:16:21,496]\u001b[0m Trial 349 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:17:00,279]\u001b[0m Trial 350 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:17:24,719]\u001b[0m Trial 351 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:17:35,360]\u001b[0m Trial 352 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:17:41,768]\u001b[0m Trial 353 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:17:44,609]\u001b[0m Trial 354 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:18:16,526]\u001b[0m Trial 355 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:18:18,870]\u001b[0m Trial 356 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:18:48,162]\u001b[0m Trial 357 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:18:55,053]\u001b[0m Trial 358 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:19:03,321]\u001b[0m Trial 359 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:19:31,510]\u001b[0m Trial 360 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:19:50,755]\u001b[0m Trial 361 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:20:03,970]\u001b[0m Trial 362 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 17:20:59,690]\u001b[0m Trial 363 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:21:51,423]\u001b[0m Trial 364 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:22:39,315]\u001b[0m Trial 365 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:22:54,753]\u001b[0m Trial 366 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:23:11,894]\u001b[0m Trial 367 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:23:14,498]\u001b[0m Trial 368 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:23:56,947]\u001b[0m Trial 369 finished with value: 0.880835781913157 and parameters: {'n_estimators': 1000, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:24:01,397]\u001b[0m Trial 370 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 100, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:24:06,623]\u001b[0m Trial 371 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:24:21,644]\u001b[0m Trial 372 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:25:09,635]\u001b[0m Trial 373 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:25:40,888]\u001b[0m Trial 374 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:26:29,265]\u001b[0m Trial 375 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:27:14,238]\u001b[0m Trial 376 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 1000, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:27:51,122]\u001b[0m Trial 377 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:28:07,731]\u001b[0m Trial 378 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:28:34,327]\u001b[0m Trial 379 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:28:37,593]\u001b[0m Trial 380 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:29:03,907]\u001b[0m Trial 381 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:29:25,524]\u001b[0m Trial 382 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:29:57,080]\u001b[0m Trial 383 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:30:16,114]\u001b[0m Trial 384 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:30:44,985]\u001b[0m Trial 385 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:30:51,019]\u001b[0m Trial 386 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:31:16,278]\u001b[0m Trial 387 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 17:31:45,164]\u001b[0m Trial 388 finished with value: 0.8765915768854065 and parameters: {'n_estimators': 1000, 'max_depth': 6}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:31:49,125]\u001b[0m Trial 389 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:33:11,690]\u001b[0m Trial 390 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:34:22,327]\u001b[0m Trial 391 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:36:04,650]\u001b[0m Trial 392 finished with value: 0.8788769180541952 and parameters: {'n_estimators': 1000, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:37:28,315]\u001b[0m Trial 393 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:38:10,397]\u001b[0m Trial 394 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:38:17,221]\u001b[0m Trial 395 finished with value: 0.8834476003917727 and parameters: {'n_estimators': 100, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:38:58,791]\u001b[0m Trial 396 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:39:07,394]\u001b[0m Trial 397 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:41:09,721]\u001b[0m Trial 398 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:42:01,143]\u001b[0m Trial 399 finished with value: 0.8782239634345413 and parameters: {'n_estimators': 500, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:42:10,376]\u001b[0m Trial 400 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:42:27,138]\u001b[0m Trial 401 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:43:34,745]\u001b[0m Trial 402 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:43:48,868]\u001b[0m Trial 403 finished with value: 0.881488736532811 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:44:43,537]\u001b[0m Trial 404 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:45:49,967]\u001b[0m Trial 405 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:46:56,099]\u001b[0m Trial 406 finished with value: 0.8778974861247143 and parameters: {'n_estimators': 500, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:47:23,878]\u001b[0m Trial 407 finished with value: 0.8827946457721189 and parameters: {'n_estimators': 500, 'max_depth': 7}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:49:04,844]\u001b[0m Trial 408 finished with value: 0.8795298726738492 and parameters: {'n_estimators': 1000, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:50:03,016]\u001b[0m Trial 409 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:51:28,148]\u001b[0m Trial 410 finished with value: 0.8801828272935031 and parameters: {'n_estimators': 1000, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:52:25,723]\u001b[0m Trial 411 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:52:32,990]\u001b[0m Trial 412 finished with value: 0.8798563499836761 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:53:22,586]\u001b[0m Trial 413 finished with value: 0.8792033953640223 and parameters: {'n_estimators': 1000, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:53:32,441]\u001b[0m Trial 414 finished with value: 0.8837740777015998 and parameters: {'n_estimators': 100, 'max_depth': 11}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:54:32,939]\u001b[0m Trial 415 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 15}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:55:28,743]\u001b[0m Trial 416 finished with value: 0.880835781913157 and parameters: {'n_estimators': 500, 'max_depth': 13}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:55:54,754]\u001b[0m Trial 417 finished with value: 0.881815213842638 and parameters: {'n_estimators': 500, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n",
      "\u001b[32m[I 2021-12-15 17:57:54,444]\u001b[0m Trial 418 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 1000, 'max_depth': 20}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:58:34,629]\u001b[0m Trial 419 finished with value: 0.8805093046033301 and parameters: {'n_estimators': 500, 'max_depth': 9}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:59:37,337]\u001b[0m Trial 420 finished with value: 0.8785504407443683 and parameters: {'n_estimators': 500, 'max_depth': 17}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "\u001b[32m[I 2021-12-15 17:59:42,758]\u001b[0m Trial 421 finished with value: 0.8850799869409076 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 4 with value: 0.8850799869409076.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\samplers\\_grid.py:173: UserWarning: The value `1000` is out of range of the parameter `n_estimators`. The value will be used but the actual distribution is: `IntUniformDistribution(high=500, low=100, step=1)`.\n",
      "  f\"The value `{param_value}` is out of range of the parameter `{param_name}`. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d72a1a9478ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgrid_study\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m         )\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mprogress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\root3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-97423b55b93e>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m     }\n\u001b[0;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1109\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1110\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.GridSampler(search_space),\n",
    ")\n",
    "\n",
    "grid_study.optimize(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstamp1 = cmaes_study.trials_dataframe()['datetime_complete'].min()\n",
    "tstamp2 = grid_study.trials_dataframe()['datetime_complete'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 03:39:09.064630')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = tstamp1 - tstamp2 if tstamp1 > tstamp2 else tstamp2 - tstamp1\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRID SEARCH:  0.8850799869409076\n",
      "CMAES:  0.8775710088148874\n",
      "TPE:  0.8886712373490042\n",
      "RANDOM SEARCH :  0.8896506692784851\n",
      "BASELINE MODEL:  0.8854064642507345\n"
     ]
    }
   ],
   "source": [
    "print('GRID SEARCH: ',grid_study.best_value)\n",
    "print('CMAES: ',cmaes_study.best_value)\n",
    "print('TPE: ',tpe_study.best_value)\n",
    "print('RANDOM SEARCH : ',random_study.best_value)\n",
    "print('BASELINE MODEL: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811d748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "# from sqlalchemy.pool import StaticPool\n",
    "# engine = create_engine('sqlite:///m.db',\n",
    "#                     poolclass=StaticPool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21fe52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_name = 'example-l'  # Unique identifier of the study.\n",
    "# study = optuna.create_study(study_name=study_name, storage='sqlite:///m.db', direction=\"maximize\",\n",
    "#     sampler=optuna.samplers.TPESampler(), load_if_exists=True\n",
    "# )\n",
    "# study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5fc6b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 00:10:26.128678')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstamp1 = study.trials_dataframe()['datetime_complete'].min()\n",
    "tstamp2 = study.trials_dataframe()['datetime_complete'].max()\n",
    "\n",
    "tstamp1 - tstamp2 if tstamp1 > tstamp2 else tstamp2 - tstamp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e88f6180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:27:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:27:48,772]\u001b[0m Trial 220 finished with value: 0.82 and parameters: {'lambda': 0.009730066153068524, 'alpha': 0.0014918614135806128, 'gamma': 0.007692681137278601, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 477, 'max_depth': 7, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:27:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:27:51,475]\u001b[0m Trial 221 finished with value: 0.88 and parameters: {'lambda': 0.02282858996099954, 'alpha': 0.0013505114662567812, 'gamma': 0.0017808609398968214, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 500, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:27:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:27:54,381]\u001b[0m Trial 222 finished with value: 0.88 and parameters: {'lambda': 0.01458769749851633, 'alpha': 0.0012778774424982845, 'gamma': 0.0017954801589204037, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 500, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:27:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:27:56,224]\u001b[0m Trial 223 finished with value: 0.84 and parameters: {'lambda': 0.014580116936612951, 'alpha': 0.0012681614895261122, 'gamma': 0.0021851263567222506, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 486, 'max_depth': 7, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:27:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:27:58,270]\u001b[0m Trial 224 finished with value: 0.84 and parameters: {'lambda': 0.018726088199312253, 'alpha': 0.001776442275856285, 'gamma': 0.002684167043193823, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 488, 'max_depth': 9, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:27:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:00,344]\u001b[0m Trial 225 finished with value: 0.84 and parameters: {'lambda': 0.02868623558878333, 'alpha': 0.0018447944436846572, 'gamma': 0.001313363112527714, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 457, 'max_depth': 20, 'min_child_weight': 13}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:03,003]\u001b[0m Trial 226 finished with value: 0.87 and parameters: {'lambda': 0.00877157772027989, 'alpha': 0.0014801601891894802, 'gamma': 0.0019801355780450156, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 481, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:04,525]\u001b[0m Trial 227 finished with value: 0.85 and parameters: {'lambda': 0.020482022257322628, 'alpha': 0.0019071602252465536, 'gamma': 0.0017650896406359651, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 489, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:06,470]\u001b[0m Trial 228 finished with value: 0.83 and parameters: {'lambda': 0.010635665329701815, 'alpha': 0.0015007170175399339, 'gamma': 0.0038336636355776783, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 473, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:08,035]\u001b[0m Trial 229 finished with value: 0.8 and parameters: {'lambda': 0.039089071542309804, 'alpha': 0.03538087402616211, 'gamma': 0.002553004049012786, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 470, 'max_depth': 9, 'min_child_weight': 17}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:09,778]\u001b[0m Trial 230 finished with value: 0.82 and parameters: {'lambda': 0.013325271915463406, 'alpha': 8.57210186587102, 'gamma': 0.0013587235462023117, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 484, 'max_depth': 7, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:12,980]\u001b[0m Trial 231 finished with value: 0.88 and parameters: {'lambda': 0.011602886042744573, 'alpha': 0.0011605740002200923, 'gamma': 0.0035627504184760125, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 462, 'max_depth': 9, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:16,768]\u001b[0m Trial 232 finished with value: 0.87 and parameters: {'lambda': 0.009846699208274258, 'alpha': 0.0011868943848319827, 'gamma': 0.0033718140310562733, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 491, 'max_depth': 9, 'min_child_weight': 2}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:19,771]\u001b[0m Trial 233 finished with value: 0.9 and parameters: {'lambda': 0.00822104503840727, 'alpha': 0.0010908058508964352, 'gamma': 0.004483581203328908, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 460, 'max_depth': 9, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:21,591]\u001b[0m Trial 234 finished with value: 0.85 and parameters: {'lambda': 0.013232347403408982, 'alpha': 0.001039300043166206, 'gamma': 0.0046574796476273675, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 452, 'max_depth': 9, 'min_child_weight': 10}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:24,321]\u001b[0m Trial 235 finished with value: 0.88 and parameters: {'lambda': 0.008990962053377103, 'alpha': 0.0016301289285423227, 'gamma': 0.0034097726889786486, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 460, 'max_depth': 15, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:26,193]\u001b[0m Trial 236 finished with value: 0.83 and parameters: {'lambda': 0.006339628945725685, 'alpha': 0.0010471175413011757, 'gamma': 0.005187708343948961, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 468, 'max_depth': 9, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:27,996]\u001b[0m Trial 237 finished with value: 0.89 and parameters: {'lambda': 0.016451967522026035, 'alpha': 0.0017070207918928877, 'gamma': 0.0034189261322165455, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.300000012, 'n_estimators': 461, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:30,223]\u001b[0m Trial 238 finished with value: 0.86 and parameters: {'lambda': 0.015550605912033073, 'alpha': 0.0010202515433704817, 'gamma': 0.00426713513134509, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 452, 'max_depth': 9, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:31,702]\u001b[0m Trial 239 finished with value: 0.8 and parameters: {'lambda': 0.019778472007713384, 'alpha': 0.0020525018693662534, 'gamma': 0.00152435061761414, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.300000012, 'n_estimators': 482, 'max_depth': 7, 'min_child_weight': 13}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:33,672]\u001b[0m Trial 240 finished with value: 0.77 and parameters: {'lambda': 0.05015119407941712, 'alpha': 0.0017896723414261692, 'gamma': 0.002852843764458603, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.300000012, 'n_estimators': 458, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:35,506]\u001b[0m Trial 241 finished with value: 0.85 and parameters: {'lambda': 0.011025319553581757, 'alpha': 0.0015176837259468542, 'gamma': 0.0032459408427950344, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.300000012, 'n_estimators': 461, 'max_depth': 9, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:37,425]\u001b[0m Trial 242 finished with value: 0.89 and parameters: {'lambda': 0.01676380371681038, 'alpha': 0.002374067769316505, 'gamma': 0.001200079959347504, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 467, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:39,302]\u001b[0m Trial 243 finished with value: 0.87 and parameters: {'lambda': 0.007869576348428033, 'alpha': 0.001371013131407805, 'gamma': 0.0015428049041844472, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 473, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:41,200]\u001b[0m Trial 244 finished with value: 0.86 and parameters: {'lambda': 0.005366771919845011, 'alpha': 0.001317531277016339, 'gamma': 0.0023510787450909012, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.300000012, 'n_estimators': 475, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:43,162]\u001b[0m Trial 245 finished with value: 0.86 and parameters: {'lambda': 0.016755981417628237, 'alpha': 0.002389937580170541, 'gamma': 0.0012621603249834794, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 493, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:45,132]\u001b[0m Trial 246 finished with value: 0.86 and parameters: {'lambda': 0.02057933464601895, 'alpha': 0.0017914666121948003, 'gamma': 0.001913745746943239, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 472, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:47,196]\u001b[0m Trial 247 finished with value: 0.79 and parameters: {'lambda': 0.00781159146619027, 'alpha': 0.002132618951148249, 'gamma': 0.0040838315958302285, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 441, 'max_depth': 9, 'min_child_weight': 14}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:49,393]\u001b[0m Trial 248 finished with value: 0.84 and parameters: {'lambda': 0.012746585068446569, 'alpha': 0.0011630779236162247, 'gamma': 0.003547424444433993, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 465, 'max_depth': 9, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:51,292]\u001b[0m Trial 249 finished with value: 0.91 and parameters: {'lambda': 0.021706879760131966, 'alpha': 0.001457497471697097, 'gamma': 0.001998294064834154, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 483, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:53,212]\u001b[0m Trial 250 finished with value: 0.81 and parameters: {'lambda': 0.015403950529035612, 'alpha': 0.0024933268361533997, 'gamma': 0.0025647417016239084, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.014, 'n_estimators': 478, 'max_depth': 7, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:54,815]\u001b[0m Trial 251 finished with value: 0.87 and parameters: {'lambda': 0.02694430655224188, 'alpha': 0.0015730747172431133, 'gamma': 0.002177679862762566, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 449, 'max_depth': 7, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:56,729]\u001b[0m Trial 252 finished with value: 0.9 and parameters: {'lambda': 0.017716509308014414, 'alpha': 0.0019175523991097395, 'gamma': 0.0010462612779764257, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 487, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:58,409]\u001b[0m Trial 253 finished with value: 0.79 and parameters: {'lambda': 0.018532105211100824, 'alpha': 0.002811137739865531, 'gamma': 0.0010971412640936663, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 18}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:28:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:28:59,704]\u001b[0m Trial 254 finished with value: 0.51 and parameters: {'lambda': 0.0020090512276871976, 'alpha': 0.00167809489340137, 'gamma': 0.003030998994699393, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 466, 'max_depth': 5, 'min_child_weight': 223}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:01,784]\u001b[0m Trial 255 finished with value: 0.8 and parameters: {'lambda': 0.017075491090398222, 'alpha': 0.0020975750463732862, 'gamma': 0.0018540911188561402, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.01, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:03,720]\u001b[0m Trial 256 finished with value: 0.83 and parameters: {'lambda': 0.01374435705948938, 'alpha': 3.0322117045886716, 'gamma': 0.0027235759893493765, 'colsample_bytree': 0.7, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 479, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:06,013]\u001b[0m Trial 257 finished with value: 0.82 and parameters: {'lambda': 0.024845238103609727, 'alpha': 0.0013886520091679396, 'gamma': 0.00662525265593683, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 494, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:07,173]\u001b[0m Trial 258 finished with value: 0.79 and parameters: {'lambda': 0.029682778779512302, 'alpha': 0.00278514832804131, 'gamma': 0.0010546861297710216, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 193, 'max_depth': 15, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:09,203]\u001b[0m Trial 259 finished with value: 0.82 and parameters: {'lambda': 0.01651265928830487, 'alpha': 0.002347920016489608, 'gamma': 0.0012322561104091737, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 454, 'max_depth': 7, 'min_child_weight': 21}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:11,308]\u001b[0m Trial 260 finished with value: 0.86 and parameters: {'lambda': 0.001822064254168542, 'alpha': 0.0018905408858915821, 'gamma': 0.003095628563312771, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 466, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:13,209]\u001b[0m Trial 261 finished with value: 0.87 and parameters: {'lambda': 0.01096282858644453, 'alpha': 0.0017350213385983193, 'gamma': 0.005600064814357481, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 470, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:15,172]\u001b[0m Trial 262 finished with value: 0.8 and parameters: {'lambda': 0.013523858277366801, 'alpha': 0.0010126973085547884, 'gamma': 0.005858856596232176, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 483, 'max_depth': 20, 'min_child_weight': 16}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:19,634]\u001b[0m Trial 263 finished with value: 0.89 and parameters: {'lambda': 0.01627500898911839, 'alpha': 0.0014867572538774419, 'gamma': 0.009936700608363028, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 474, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:21,114]\u001b[0m Trial 264 finished with value: 0.51 and parameters: {'lambda': 0.682520100652941, 'alpha': 0.0015411674633851202, 'gamma': 0.00930789295198829, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 481, 'max_depth': 7, 'min_child_weight': 189}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:23,323]\u001b[0m Trial 265 finished with value: 0.83 and parameters: {'lambda': 0.021832850239011686, 'alpha': 0.002041712467949099, 'gamma': 0.0019435103701166806, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 498, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:25,078]\u001b[0m Trial 266 finished with value: 0.84 and parameters: {'lambda': 0.0014210757760432262, 'alpha': 0.0015258086455215147, 'gamma': 0.00475579767098397, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 461, 'max_depth': 7, 'min_child_weight': 13}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:26,839]\u001b[0m Trial 267 finished with value: 0.85 and parameters: {'lambda': 0.01966726935502048, 'alpha': 0.002402495523164446, 'gamma': 0.0011626971300869285, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 473, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:29,515]\u001b[0m Trial 268 finished with value: 0.87 and parameters: {'lambda': 0.02119680558572309, 'alpha': 0.19909150599262768, 'gamma': 0.008798008371882143, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 486, 'max_depth': 13, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:31,388]\u001b[0m Trial 269 finished with value: 0.83 and parameters: {'lambda': 0.016096741306277813, 'alpha': 0.0013435807822551181, 'gamma': 0.010973677281723928, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 475, 'max_depth': 7, 'min_child_weight': 8}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:33,081]\u001b[0m Trial 270 finished with value: 0.78 and parameters: {'lambda': 0.013402862167568105, 'alpha': 0.07495936301648047, 'gamma': 0.0013664870210074193, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 479, 'max_depth': 7, 'min_child_weight': 15}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:35,206]\u001b[0m Trial 271 finished with value: 0.87 and parameters: {'lambda': 0.04478826852279146, 'alpha': 0.0016825654771395102, 'gamma': 0.0030980454784172206, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 448, 'max_depth': 15, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:37,542]\u001b[0m Trial 272 finished with value: 0.88 and parameters: {'lambda': 0.009265328211898446, 'alpha': 0.0016913803243980793, 'gamma': 0.0024761400651076306, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 458, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:39,466]\u001b[0m Trial 273 finished with value: 0.78 and parameters: {'lambda': 0.016034935937853396, 'alpha': 0.0012762390726141954, 'gamma': 0.014165732256668867, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 500, 'max_depth': 7, 'min_child_weight': 22}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:41,182]\u001b[0m Trial 274 finished with value: 0.8 and parameters: {'lambda': 0.0109309230428464, 'alpha': 0.0017994014774109844, 'gamma': 0.003553772069446932, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 488, 'max_depth': 15, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:44,429]\u001b[0m Trial 275 finished with value: 0.88 and parameters: {'lambda': 0.013417141726213453, 'alpha': 0.0014079932024322998, 'gamma': 0.0017140606773352089, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 500, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:46,528]\u001b[0m Trial 276 finished with value: 0.83 and parameters: {'lambda': 0.011657707560857407, 'alpha': 0.001177658127253789, 'gamma': 0.008074321718276121, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 467, 'max_depth': 9, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:48,505]\u001b[0m Trial 277 finished with value: 0.78 and parameters: {'lambda': 0.0030126631980294913, 'alpha': 0.0020332197096175415, 'gamma': 0.002126415482365083, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.01, 'n_estimators': 439, 'max_depth': 13, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:49,509]\u001b[0m Trial 278 finished with value: 0.84 and parameters: {'lambda': 0.018508660317210775, 'alpha': 0.0012019444131423148, 'gamma': 0.002665209833705038, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 206, 'max_depth': 5, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:53,163]\u001b[0m Trial 279 finished with value: 0.85 and parameters: {'lambda': 0.004507626036477698, 'alpha': 0.0015225302826418065, 'gamma': 0.002427489127926811, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 456, 'max_depth': 9, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:54,673]\u001b[0m Trial 280 finished with value: 0.79 and parameters: {'lambda': 0.011868726104344756, 'alpha': 0.0025092621125449585, 'gamma': 0.0041017343391374075, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 468, 'max_depth': 7, 'min_child_weight': 18}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:56,159]\u001b[0m Trial 281 finished with value: 0.51 and parameters: {'lambda': 0.009470842651684405, 'alpha': 0.0019134650907461869, 'gamma': 0.0037655248133257394, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 457, 'max_depth': 15, 'min_child_weight': 147}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:57,841]\u001b[0m Trial 282 finished with value: 0.79 and parameters: {'lambda': 0.01579011132473005, 'alpha': 0.0012302536156901364, 'gamma': 0.012824251772140182, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.018, 'n_estimators': 446, 'max_depth': 9, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:29:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:29:59,487]\u001b[0m Trial 283 finished with value: 0.88 and parameters: {'lambda': 0.03327681606233174, 'alpha': 0.0013930430024558299, 'gamma': 0.004997901640568103, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 483, 'max_depth': 9, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:01,837]\u001b[0m Trial 284 finished with value: 0.84 and parameters: {'lambda': 0.008752322684434889, 'alpha': 0.0012355913057319002, 'gamma': 0.009537509714575463, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 471, 'max_depth': 9, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:04,149]\u001b[0m Trial 285 finished with value: 0.89 and parameters: {'lambda': 0.023837452223666265, 'alpha': 0.0014624134465882302, 'gamma': 0.0020263779426250325, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 493, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:05,757]\u001b[0m Trial 286 finished with value: 0.82 and parameters: {'lambda': 0.025524250793563245, 'alpha': 0.00212957973606508, 'gamma': 0.0015659981624291496, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 487, 'max_depth': 7, 'min_child_weight': 16}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:07,735]\u001b[0m Trial 287 finished with value: 0.83 and parameters: {'lambda': 0.019400648314189965, 'alpha': 0.0018428544765872587, 'gamma': 0.0019998545239467732, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 480, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:09,151]\u001b[0m Trial 288 finished with value: 0.51 and parameters: {'lambda': 0.017760811825690268, 'alpha': 0.0030386065614351142, 'gamma': 0.0029074184403191984, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 490, 'max_depth': 7, 'min_child_weight': 82}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:10,932]\u001b[0m Trial 289 finished with value: 0.83 and parameters: {'lambda': 0.013757390473637847, 'alpha': 0.0028553446741000754, 'gamma': 0.002446043541461425, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 472, 'max_depth': 7, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:12,710]\u001b[0m Trial 290 finished with value: 0.9 and parameters: {'lambda': 0.023084029194166297, 'alpha': 0.0016226892706070605, 'gamma': 0.0020936838043649425, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 490, 'max_depth': 7, 'min_child_weight': 5}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:14,271]\u001b[0m Trial 291 finished with value: 0.78 and parameters: {'lambda': 0.031089940596953438, 'alpha': 0.0015288158293826786, 'gamma': 0.0021349907332333428, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.018, 'n_estimators': 493, 'max_depth': 7, 'min_child_weight': 24}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:15,644]\u001b[0m Trial 292 finished with value: 0.85 and parameters: {'lambda': 0.02362160122677014, 'alpha': 0.0017225649204098694, 'gamma': 0.0020075720493168904, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 108, 'max_depth': 7, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:17,360]\u001b[0m Trial 293 finished with value: 0.49 and parameters: {'lambda': 0.010281355575269484, 'alpha': 0.0015404684906676345, 'gamma': 0.002876087342219744, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 486, 'max_depth': 7, 'min_child_weight': 60}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:20,669]\u001b[0m Trial 294 finished with value: 0.82 and parameters: {'lambda': 0.5751322405965978, 'alpha': 0.0021512145270087425, 'gamma': 0.007023729653230574, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 480, 'max_depth': 7, 'min_child_weight': 15}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:22,726]\u001b[0m Trial 295 finished with value: 0.84 and parameters: {'lambda': 0.012602933850969963, 'alpha': 0.0013984988296875142, 'gamma': 0.0023453778786754554, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 460, 'max_depth': 7, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:24,128]\u001b[0m Trial 296 finished with value: 0.51 and parameters: {'lambda': 9.102833257889147, 'alpha': 0.0023210309633039435, 'gamma': 0.0026852855451539813, 'colsample_bytree': 0.5, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 492, 'max_depth': 7, 'min_child_weight': 116}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:26,881]\u001b[0m Trial 297 finished with value: 0.85 and parameters: {'lambda': 0.015965957369000083, 'alpha': 1.3988318170566323, 'gamma': 0.0023083061163154915, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 474, 'max_depth': 20, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:28,737]\u001b[0m Trial 298 finished with value: 0.83 and parameters: {'lambda': 0.01486723871818819, 'alpha': 0.0017276873390946463, 'gamma': 0.001674746065667595, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 494, 'max_depth': 7, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:30,485]\u001b[0m Trial 299 finished with value: 0.89 and parameters: {'lambda': 0.02188165770555279, 'alpha': 0.001085151992156813, 'gamma': 0.0038099661710065493, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 449, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:31,915]\u001b[0m Trial 300 finished with value: 0.86 and parameters: {'lambda': 0.024690982413817793, 'alpha': 0.001010241650651078, 'gamma': 0.0031522283136364015, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 443, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:33,447]\u001b[0m Trial 301 finished with value: 0.79 and parameters: {'lambda': 0.02085707660183978, 'alpha': 0.0014150102466880077, 'gamma': 0.0038588342891961126, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 452, 'max_depth': 7, 'min_child_weight': 18}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:34,880]\u001b[0m Trial 302 finished with value: 0.82 and parameters: {'lambda': 0.034756070627381265, 'alpha': 0.0017898961042552843, 'gamma': 0.001989271657951287, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 437, 'max_depth': 5, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:36,604]\u001b[0m Trial 303 finished with value: 0.87 and parameters: {'lambda': 0.027080759643168185, 'alpha': 0.0011719182520192593, 'gamma': 0.0027404805367449935, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 432, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:38,367]\u001b[0m Trial 304 finished with value: 0.83 and parameters: {'lambda': 0.018721293684355628, 'alpha': 0.001424201651519236, 'gamma': 0.001010190367231685, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 484, 'max_depth': 5, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:39,991]\u001b[0m Trial 305 finished with value: 0.89 and parameters: {'lambda': 0.021818293023023013, 'alpha': 0.002008460001078925, 'gamma': 0.0013974137673183002, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 477, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:41,961]\u001b[0m Trial 306 finished with value: 0.79 and parameters: {'lambda': 0.016623156955064132, 'alpha': 0.0016624935413142056, 'gamma': 0.010469546623286414, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.01, 'n_estimators': 452, 'max_depth': 5, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:43,853]\u001b[0m Trial 307 finished with value: 0.86 and parameters: {'lambda': 0.02357357815441962, 'alpha': 0.002017962078498288, 'gamma': 0.0014176914583799664, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.300000012, 'n_estimators': 476, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:45,923]\u001b[0m Trial 308 finished with value: 0.82 and parameters: {'lambda': 0.021421446840622155, 'alpha': 0.001284924659891134, 'gamma': 0.001391580911917433, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 485, 'max_depth': 6, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:47,242]\u001b[0m Trial 309 finished with value: 0.8 and parameters: {'lambda': 0.02779301069407406, 'alpha': 0.0026394243006862307, 'gamma': 0.0011344522504764664, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 469, 'max_depth': 5, 'min_child_weight': 18}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:49,206]\u001b[0m Trial 310 finished with value: 0.81 and parameters: {'lambda': 0.014232579395998387, 'alpha': 0.002023613238150934, 'gamma': 0.0017586473407369021, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.018, 'n_estimators': 489, 'max_depth': 17, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:51,185]\u001b[0m Trial 311 finished with value: 0.89 and parameters: {'lambda': 0.01953377259225725, 'alpha': 0.020950867659637492, 'gamma': 0.019790845625290973, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 461, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:53,305]\u001b[0m Trial 312 finished with value: 0.85 and parameters: {'lambda': 0.016972342167736843, 'alpha': 0.12630556509590257, 'gamma': 0.018170300579268175, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 464, 'max_depth': 7, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:54,672]\u001b[0m Trial 313 finished with value: 0.77 and parameters: {'lambda': 0.011970454616334166, 'alpha': 0.01596769257957686, 'gamma': 0.021105222180470094, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 452, 'max_depth': 7, 'min_child_weight': 23}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:56,325]\u001b[0m Trial 314 finished with value: 0.78 and parameters: {'lambda': 0.020477112326835824, 'alpha': 0.3744721686944466, 'gamma': 0.012798018811641481, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 460, 'max_depth': 5, 'min_child_weight': 14}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:30:58,274]\u001b[0m Trial 315 finished with value: 0.84 and parameters: {'lambda': 0.010304835867413459, 'alpha': 0.03660430734525572, 'gamma': 0.005983037145638143, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 487, 'max_depth': 5, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:30:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:00,564]\u001b[0m Trial 316 finished with value: 0.89 and parameters: {'lambda': 0.03180754178932644, 'alpha': 0.0011541291569282697, 'gamma': 0.001289041661221378, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 479, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:01,996]\u001b[0m Trial 317 finished with value: 0.91 and parameters: {'lambda': 0.030040354779568217, 'alpha': 0.021555209859005886, 'gamma': 0.0032611531945796576, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 468, 'max_depth': 5, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:03,489]\u001b[0m Trial 318 finished with value: 0.84 and parameters: {'lambda': 0.02782376582422479, 'alpha': 0.06363584900955055, 'gamma': 0.0032649705610516486, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 467, 'max_depth': 5, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:04,947]\u001b[0m Trial 319 finished with value: 0.78 and parameters: {'lambda': 0.03933259711464198, 'alpha': 0.0012178961028110663, 'gamma': 0.002406913937220558, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 443, 'max_depth': 5, 'min_child_weight': 17}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:07,025]\u001b[0m Trial 320 finished with value: 0.84 and parameters: {'lambda': 0.027702827667963614, 'alpha': 0.001380775776043675, 'gamma': 0.0020221629443857906, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 478, 'max_depth': 5, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:08,714]\u001b[0m Trial 321 finished with value: 0.51 and parameters: {'lambda': 0.030737929479412965, 'alpha': 0.010685686024553056, 'gamma': 0.01567206630937456, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 472, 'max_depth': 5, 'min_child_weight': 277}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:10,282]\u001b[0m Trial 322 finished with value: 0.84 and parameters: {'lambda': 0.02328551015550578, 'alpha': 0.0018080081207857174, 'gamma': 0.004517412967540536, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 493, 'max_depth': 9, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:11,868]\u001b[0m Trial 323 finished with value: 0.51 and parameters: {'lambda': 0.06504472736465779, 'alpha': 0.0011360583302391196, 'gamma': 0.0037893082032547454, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 478, 'max_depth': 5, 'min_child_weight': 135}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:13,775]\u001b[0m Trial 324 finished with value: 0.87 and parameters: {'lambda': 0.018941216147506955, 'alpha': 0.00201027441780054, 'gamma': 0.002946038041938624, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 463, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:15,163]\u001b[0m Trial 325 finished with value: 0.76 and parameters: {'lambda': 0.014079884696203198, 'alpha': 0.0015452734624512101, 'gamma': 0.003213559143977175, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 471, 'max_depth': 7, 'min_child_weight': 30}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:17,380]\u001b[0m Trial 326 finished with value: 0.85 and parameters: {'lambda': 0.03369747294702417, 'alpha': 0.018257998659273824, 'gamma': 0.024115776494499294, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 481, 'max_depth': 5, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:19,018]\u001b[0m Trial 327 finished with value: 0.81 and parameters: {'lambda': 0.04069116050848902, 'alpha': 0.0034308382734616625, 'gamma': 0.0016214350490584147, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.300000012, 'n_estimators': 484, 'max_depth': 5, 'min_child_weight': 17}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:22,845]\u001b[0m Trial 328 finished with value: 0.86 and parameters: {'lambda': 0.837431552861623, 'alpha': 0.02589009604577653, 'gamma': 0.001418343510894004, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 459, 'max_depth': 5, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:24,470]\u001b[0m Trial 329 finished with value: 0.81 and parameters: {'lambda': 0.006721857827803884, 'alpha': 0.0024207165873299397, 'gamma': 0.0011709741752098926, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 469, 'max_depth': 5, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:26,281]\u001b[0m Trial 330 finished with value: 0.78 and parameters: {'lambda': 0.008893533707284533, 'alpha': 0.0016258362753492905, 'gamma': 0.007672893615665493, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 494, 'max_depth': 7, 'min_child_weight': 22}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:28,046]\u001b[0m Trial 331 finished with value: 0.82 and parameters: {'lambda': 0.012392321721413718, 'alpha': 0.0010021772954211812, 'gamma': 0.0041025968376172395, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 448, 'max_depth': 9, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:29,963]\u001b[0m Trial 332 finished with value: 0.88 and parameters: {'lambda': 0.03460073325210229, 'alpha': 0.048017084269515446, 'gamma': 0.0021925093791700166, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 487, 'max_depth': 13, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:31,784]\u001b[0m Trial 333 finished with value: 0.89 and parameters: {'lambda': 0.02530369759513438, 'alpha': 0.020844286845683246, 'gamma': 0.0026553098943538503, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 477, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:33,240]\u001b[0m Trial 334 finished with value: 0.51 and parameters: {'lambda': 0.008632463713822182, 'alpha': 0.0020937211244000473, 'gamma': 0.00269192156298502, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.300000012, 'n_estimators': 478, 'max_depth': 11, 'min_child_weight': 300}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:35,308]\u001b[0m Trial 335 finished with value: 0.81 and parameters: {'lambda': 0.023356127956879563, 'alpha': 0.0027772525154694887, 'gamma': 0.002470676169005893, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 469, 'max_depth': 5, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:37,385]\u001b[0m Trial 336 finished with value: 0.84 and parameters: {'lambda': 0.023482576065075176, 'alpha': 0.0013847121886237507, 'gamma': 0.0021756134491459776, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 494, 'max_depth': 9, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:39,313]\u001b[0m Trial 337 finished with value: 0.89 and parameters: {'lambda': 0.01854894703320833, 'alpha': 0.023442776312928234, 'gamma': 0.0018338990991459696, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 476, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:41,357]\u001b[0m Trial 338 finished with value: 0.82 and parameters: {'lambda': 0.015932115732760786, 'alpha': 0.0015110356572256298, 'gamma': 0.0032522479699610444, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 500, 'max_depth': 7, 'min_child_weight': 14}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:43,374]\u001b[0m Trial 339 finished with value: 0.82 and parameters: {'lambda': 0.019253007639360307, 'alpha': 0.02379781805385409, 'gamma': 0.015194093209620344, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.01, 'n_estimators': 457, 'max_depth': 5, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:44,220]\u001b[0m Trial 340 finished with value: 0.51 and parameters: {'lambda': 0.017710230127537193, 'alpha': 0.0011673998541263727, 'gamma': 0.0018957421718214862, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 145, 'max_depth': 7, 'min_child_weight': 245}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:46,338]\u001b[0m Trial 341 finished with value: 0.79 and parameters: {'lambda': 0.01033979380095542, 'alpha': 0.0018681237297415861, 'gamma': 0.004564663684363229, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.008, 'n_estimators': 429, 'max_depth': 9, 'min_child_weight': 16}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:47,919]\u001b[0m Trial 342 finished with value: 0.87 and parameters: {'lambda': 0.015756764816100746, 'alpha': 0.017356608877014464, 'gamma': 0.0016791982839546346, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 466, 'max_depth': 20, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:50,867]\u001b[0m Trial 343 finished with value: 0.87 and parameters: {'lambda': 0.012536324489426072, 'alpha': 0.020573588804347263, 'gamma': 0.002881954402279729, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 476, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:52,434]\u001b[0m Trial 344 finished with value: 0.82 and parameters: {'lambda': 0.010614595151016221, 'alpha': 0.08549669065962125, 'gamma': 0.5288207260151238, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 475, 'max_depth': 7, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:53,891]\u001b[0m Trial 345 finished with value: 0.72 and parameters: {'lambda': 0.014804786223734257, 'alpha': 0.0016488197628901772, 'gamma': 0.0035595327523199405, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 488, 'max_depth': 9, 'min_child_weight': 20}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:55,912]\u001b[0m Trial 346 finished with value: 0.84 and parameters: {'lambda': 0.007660638103044032, 'alpha': 0.0012580611322696585, 'gamma': 0.02932049975208445, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 491, 'max_depth': 9, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:57,503]\u001b[0m Trial 347 finished with value: 0.86 and parameters: {'lambda': 0.02599032146489857, 'alpha': 0.0018375622463556172, 'gamma': 0.002322646848784692, 'colsample_bytree': 0.9, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 485, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:31:59,450]\u001b[0m Trial 348 finished with value: 0.82 and parameters: {'lambda': 0.013003571528551482, 'alpha': 0.0020603265348546375, 'gamma': 0.0054476806092746645, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 485, 'max_depth': 17, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:01,464]\u001b[0m Trial 349 finished with value: 0.85 and parameters: {'lambda': 0.011400401347096016, 'alpha': 0.6166465886856088, 'gamma': 0.0012612191904899312, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 477, 'max_depth': 9, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:03,143]\u001b[0m Trial 350 finished with value: 0.82 and parameters: {'lambda': 0.017023887810339998, 'alpha': 0.0013810784644378963, 'gamma': 0.007979657499147301, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 495, 'max_depth': 7, 'min_child_weight': 15}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:04,987]\u001b[0m Trial 351 finished with value: 0.83 and parameters: {'lambda': 0.02232032007698851, 'alpha': 0.0024587842931334527, 'gamma': 0.0036314369218575283, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.016, 'n_estimators': 463, 'max_depth': 6, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:06,398]\u001b[0m Trial 352 finished with value: 0.76 and parameters: {'lambda': 0.014069322108732784, 'alpha': 0.0010350797666043114, 'gamma': 0.006299659663439081, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 450, 'max_depth': 7, 'min_child_weight': 25}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:09,013]\u001b[0m Trial 353 finished with value: 0.9 and parameters: {'lambda': 0.01845343617801802, 'alpha': 0.03119504338039123, 'gamma': 0.001494151145923977, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 470, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:10,980]\u001b[0m Trial 354 finished with value: 0.83 and parameters: {'lambda': 4.462836823203251, 'alpha': 0.03332468699677714, 'gamma': 0.003951991519338599, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.018, 'n_estimators': 452, 'max_depth': 7, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:12,592]\u001b[0m Trial 355 finished with value: 0.51 and parameters: {'lambda': 0.015450573169769926, 'alpha': 0.01389467445156385, 'gamma': 0.0016622928760610292, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 500, 'max_depth': 9, 'min_child_weight': 165}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:14,201]\u001b[0m Trial 356 finished with value: 0.51 and parameters: {'lambda': 0.020779269270051238, 'alpha': 0.031101127579491638, 'gamma': 0.0010370911269029896, 'colsample_bytree': 0.4, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 463, 'max_depth': 9, 'min_child_weight': 68}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:18,707]\u001b[0m Trial 357 finished with value: 0.84 and parameters: {'lambda': 0.008389844227983377, 'alpha': 0.0012198339931815095, 'gamma': 0.01069582992186067, 'colsample_bytree': 0.5, 'subsample': 0.6, 'learning_rate': 0.012, 'n_estimators': 468, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:21,131]\u001b[0m Trial 358 finished with value: 0.84 and parameters: {'lambda': 0.005920883080645839, 'alpha': 0.0014842418524789232, 'gamma': 0.012018239905364232, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 458, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:22,424]\u001b[0m Trial 359 finished with value: 0.8 and parameters: {'lambda': 0.009829697588262442, 'alpha': 0.001648646741304103, 'gamma': 0.0021328906967288805, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 469, 'max_depth': 7, 'min_child_weight': 17}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:26,426]\u001b[0m Trial 360 finished with value: 0.51 and parameters: {'lambda': 0.030515895677965955, 'alpha': 0.0010061712923825406, 'gamma': 0.018733786293077145, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 480, 'max_depth': 5, 'min_child_weight': 102}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:28,159]\u001b[0m Trial 361 finished with value: 0.87 and parameters: {'lambda': 0.01892353610135759, 'alpha': 0.04164765450502254, 'gamma': 0.00134657286186368, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 473, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:29,914]\u001b[0m Trial 362 finished with value: 0.89 and parameters: {'lambda': 0.036707273383506774, 'alpha': 0.020434570701889317, 'gamma': 0.0026906086534567763, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 444, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:31,489]\u001b[0m Trial 363 finished with value: 0.83 and parameters: {'lambda': 0.012133838045571286, 'alpha': 0.05535120456083745, 'gamma': 0.003131148923195002, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 442, 'max_depth': 13, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:33,868]\u001b[0m Trial 364 finished with value: 0.86 and parameters: {'lambda': 0.02135935533705777, 'alpha': 0.1471052770381242, 'gamma': 0.0014683679896831442, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.014, 'n_estimators': 485, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:35,457]\u001b[0m Trial 365 finished with value: 0.87 and parameters: {'lambda': 0.013730142871895399, 'alpha': 0.0013940367747591993, 'gamma': 0.0019212817622564983, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 458, 'max_depth': 11, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:37,507]\u001b[0m Trial 366 finished with value: 0.87 and parameters: {'lambda': 0.01985356850357379, 'alpha': 0.001004460193689475, 'gamma': 0.004462284027637361, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 462, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:39,461]\u001b[0m Trial 367 finished with value: 0.82 and parameters: {'lambda': 0.017217678238767, 'alpha': 0.001823145423694871, 'gamma': 0.0013041225910161449, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 472, 'max_depth': 7, 'min_child_weight': 14}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:40,918]\u001b[0m Trial 368 finished with value: 0.81 and parameters: {'lambda': 0.02682860809672224, 'alpha': 0.01314606031910046, 'gamma': 0.0011370478997402456, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 465, 'max_depth': 9, 'min_child_weight': 19}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:42,994]\u001b[0m Trial 369 finished with value: 0.84 and parameters: {'lambda': 0.030240835103233173, 'alpha': 0.028487237239480205, 'gamma': 0.002572457987275793, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 483, 'max_depth': 5, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:44,823]\u001b[0m Trial 370 finished with value: 0.9 and parameters: {'lambda': 0.028613402030018078, 'alpha': 0.023736330818918003, 'gamma': 0.0018111558002716322, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 478, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:46,729]\u001b[0m Trial 371 finished with value: 0.81 and parameters: {'lambda': 0.02451604356237129, 'alpha': 0.025331589901557223, 'gamma': 0.0016732082353545738, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.01, 'n_estimators': 492, 'max_depth': 9, 'min_child_weight': 13}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:48,884]\u001b[0m Trial 372 finished with value: 0.84 and parameters: {'lambda': 0.032271221160626515, 'alpha': 0.28818135949570634, 'gamma': 0.0025294311275935303, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 481, 'max_depth': 5, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:51,401]\u001b[0m Trial 373 finished with value: 0.88 and parameters: {'lambda': 0.04352306426016105, 'alpha': 0.02821853433450462, 'gamma': 0.0014868131675971727, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 491, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:52,944]\u001b[0m Trial 374 finished with value: 0.79 and parameters: {'lambda': 0.01774738349497044, 'alpha': 0.017822218865147288, 'gamma': 0.0018512687989965648, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 468, 'max_depth': 7, 'min_child_weight': 10}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:54,953]\u001b[0m Trial 375 finished with value: 0.81 and parameters: {'lambda': 0.021768078466818865, 'alpha': 0.027828994015479194, 'gamma': 0.001980309499488661, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.300000012, 'n_estimators': 483, 'max_depth': 20, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:56,519]\u001b[0m Trial 376 finished with value: 0.77 and parameters: {'lambda': 0.056387459171497345, 'alpha': 0.021229136160437513, 'gamma': 0.0029015822554134367, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 475, 'max_depth': 5, 'min_child_weight': 21}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:32:58,466]\u001b[0m Trial 377 finished with value: 0.88 and parameters: {'lambda': 0.006696114531844805, 'alpha': 0.02496924226423555, 'gamma': 0.0022967798568980773, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 474, 'max_depth': 7, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:32:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:00,248]\u001b[0m Trial 378 finished with value: 0.78 and parameters: {'lambda': 0.027312495029348502, 'alpha': 0.003882091609616941, 'gamma': 0.002023024491508398, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 486, 'max_depth': 5, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:02,052]\u001b[0m Trial 379 finished with value: 0.86 and parameters: {'lambda': 0.009943679733009142, 'alpha': 0.0012549460443873925, 'gamma': 0.0014758435295748108, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.018, 'n_estimators': 478, 'max_depth': 7, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:03,825]\u001b[0m Trial 380 finished with value: 0.78 and parameters: {'lambda': 0.014158253124028792, 'alpha': 0.0021641913674575308, 'gamma': 0.0016421099683444221, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 469, 'max_depth': 7, 'min_child_weight': 16}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:05,217]\u001b[0m Trial 381 finished with value: 0.76 and parameters: {'lambda': 0.03545135256002098, 'alpha': 0.020168878313442343, 'gamma': 0.0022690776438001473, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 478, 'max_depth': 5, 'min_child_weight': 28}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:07,956]\u001b[0m Trial 382 finished with value: 0.9 and parameters: {'lambda': 0.03018182002486095, 'alpha': 0.0015113608518233974, 'gamma': 0.0017890765240503369, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 493, 'max_depth': 17, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:10,135]\u001b[0m Trial 383 finished with value: 0.86 and parameters: {'lambda': 0.036787665108579864, 'alpha': 0.03700628712668363, 'gamma': 0.9791735138623371, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.02, 'n_estimators': 495, 'max_depth': 17, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:12,380]\u001b[0m Trial 384 finished with value: 0.84 and parameters: {'lambda': 0.0491385025424217, 'alpha': 0.016252009697129734, 'gamma': 0.0026233210989083924, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 470, 'max_depth': 17, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:15,025]\u001b[0m Trial 385 finished with value: 0.88 and parameters: {'lambda': 0.024761221905734744, 'alpha': 0.009035798561578863, 'gamma': 0.0017392889904068907, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 454, 'max_depth': 17, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:17,011]\u001b[0m Trial 386 finished with value: 0.85 and parameters: {'lambda': 0.03044542264645555, 'alpha': 0.025488869798544086, 'gamma': 0.0018458655503594422, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 479, 'max_depth': 17, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:18,822]\u001b[0m Trial 387 finished with value: 0.79 and parameters: {'lambda': 0.007637636753913366, 'alpha': 0.02441873853763668, 'gamma': 0.0022232195850538238, 'colsample_bytree': 0.4, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 462, 'max_depth': 17, 'min_child_weight': 17}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:20,685]\u001b[0m Trial 388 finished with value: 0.83 and parameters: {'lambda': 0.9931254068185033, 'alpha': 0.0118846030763521, 'gamma': 0.0015052656393397463, 'colsample_bytree': 0.5, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 487, 'max_depth': 5, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:22,768]\u001b[0m Trial 389 finished with value: 0.85 and parameters: {'lambda': 0.025902933950314434, 'alpha': 0.001243960180490557, 'gamma': 0.003524509753717103, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.016, 'n_estimators': 448, 'max_depth': 17, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:24,895]\u001b[0m Trial 390 finished with value: 0.85 and parameters: {'lambda': 0.011071990558557385, 'alpha': 0.0015303428974215475, 'gamma': 0.0032510835625895822, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 494, 'max_depth': 7, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:26,410]\u001b[0m Trial 391 finished with value: 0.83 and parameters: {'lambda': 0.020722359541251387, 'alpha': 0.0011909482583238099, 'gamma': 0.00487273305987443, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.008, 'n_estimators': 174, 'max_depth': 6, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:28,783]\u001b[0m Trial 392 finished with value: 0.82 and parameters: {'lambda': 0.01805113726409647, 'alpha': 0.042125289212510755, 'gamma': 0.0017551157437880986, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 259, 'max_depth': 5, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:30,259]\u001b[0m Trial 393 finished with value: 0.86 and parameters: {'lambda': 0.029206848973957616, 'alpha': 0.01468209606161053, 'gamma': 0.0013797479467408688, 'colsample_bytree': 1.0, 'subsample': 0.4, 'learning_rate': 0.300000012, 'n_estimators': 463, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:32,006]\u001b[0m Trial 394 finished with value: 0.8 and parameters: {'lambda': 0.012633201553386647, 'alpha': 0.001592977683031921, 'gamma': 0.001966387933620646, 'colsample_bytree': 0.9, 'subsample': 0.9, 'learning_rate': 0.012, 'n_estimators': 490, 'max_depth': 7, 'min_child_weight': 20}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:33,074]\u001b[0m Trial 395 finished with value: 0.51 and parameters: {'lambda': 0.621135550929673, 'alpha': 0.0032741307750820832, 'gamma': 0.002793232960028087, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 231, 'max_depth': 9, 'min_child_weight': 292}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:34,749]\u001b[0m Trial 396 finished with value: 0.51 and parameters: {'lambda': 0.034946884623046744, 'alpha': 0.01972924063337698, 'gamma': 0.0028743541122066775, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 460, 'max_depth': 5, 'min_child_weight': 214}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:36,484]\u001b[0m Trial 397 finished with value: 0.79 and parameters: {'lambda': 0.00906431661386137, 'alpha': 0.0024449583674838, 'gamma': 0.005649504412837105, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.018, 'n_estimators': 451, 'max_depth': 9, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:38,759]\u001b[0m Trial 398 finished with value: 0.85 and parameters: {'lambda': 0.04206354637321148, 'alpha': 0.0017542490154986194, 'gamma': 0.003374338412561799, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 483, 'max_depth': 9, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:40,413]\u001b[0m Trial 399 finished with value: 0.81 and parameters: {'lambda': 0.014860302063053202, 'alpha': 0.0016652486220829243, 'gamma': 0.007562607330223559, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 497, 'max_depth': 9, 'min_child_weight': 16}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:43,069]\u001b[0m Trial 400 finished with value: 0.89 and parameters: {'lambda': 0.011764218328805469, 'alpha': 0.001177833268166237, 'gamma': 0.004082127783932676, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 487, 'max_depth': 13, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:45,261]\u001b[0m Trial 401 finished with value: 0.85 and parameters: {'lambda': 0.028571402783184912, 'alpha': 0.001157352959598116, 'gamma': 0.00222984874312967, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.014, 'n_estimators': 500, 'max_depth': 9, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:46,852]\u001b[0m Trial 402 finished with value: 0.91 and parameters: {'lambda': 0.03691426083106213, 'alpha': 0.0010018904555019205, 'gamma': 0.001156409720018862, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 441, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:48,628]\u001b[0m Trial 403 finished with value: 0.85 and parameters: {'lambda': 0.041389742190590134, 'alpha': 0.0203632862736937, 'gamma': 0.001206947304939073, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 475, 'max_depth': 5, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:50,888]\u001b[0m Trial 404 finished with value: 0.81 and parameters: {'lambda': 0.007805454053508866, 'alpha': 0.03126766109331976, 'gamma': 0.001798327089934505, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.009, 'n_estimators': 482, 'max_depth': 9, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:52,323]\u001b[0m Trial 405 finished with value: 0.77 and parameters: {'lambda': 0.033100895509842666, 'alpha': 0.0010107064508309534, 'gamma': 0.0013230296883372048, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 484, 'max_depth': 5, 'min_child_weight': 25}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:55,102]\u001b[0m Trial 406 finished with value: 0.88 and parameters: {'lambda': 0.01178804289890003, 'alpha': 0.0013980178460833634, 'gamma': 0.0020725692561576804, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.02, 'n_estimators': 486, 'max_depth': 13, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:56,597]\u001b[0m Trial 407 finished with value: 0.78 and parameters: {'lambda': 0.023036607751573047, 'alpha': 0.0013781773702367152, 'gamma': 0.0010109256945109715, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.300000012, 'n_estimators': 437, 'max_depth': 7, 'min_child_weight': 15}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:33:58,577]\u001b[0m Trial 408 finished with value: 0.83 and parameters: {'lambda': 0.026394280111325132, 'alpha': 0.0018568601329224572, 'gamma': 0.002440475276929818, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.01, 'n_estimators': 437, 'max_depth': 17, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:00,228]\u001b[0m Trial 409 finished with value: 0.79 and parameters: {'lambda': 0.016379609347888346, 'alpha': 0.002052606974512751, 'gamma': 0.0016020653627756753, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 426, 'max_depth': 7, 'min_child_weight': 12}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:01,952]\u001b[0m Trial 410 finished with value: 0.83 and parameters: {'lambda': 0.013059159791815614, 'alpha': 0.0013723760078945336, 'gamma': 0.001464612373914855, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 472, 'max_depth': 13, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:03,813]\u001b[0m Trial 411 finished with value: 0.9 and parameters: {'lambda': 0.021219946651105465, 'alpha': 0.02258478819776245, 'gamma': 0.0018152429872205598, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 473, 'max_depth': 13, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:05,256]\u001b[0m Trial 412 finished with value: 0.77 and parameters: {'lambda': 2.712755147898232, 'alpha': 0.0016903987048504366, 'gamma': 0.0018738389122736174, 'colsample_bytree': 0.7, 'subsample': 0.7, 'learning_rate': 0.300000012, 'n_estimators': 472, 'max_depth': 11, 'min_child_weight': 21}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:07,272]\u001b[0m Trial 413 finished with value: 0.8 and parameters: {'lambda': 0.03134244032531774, 'alpha': 0.0010001857191377602, 'gamma': 0.002641792957799458, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.008, 'n_estimators': 480, 'max_depth': 13, 'min_child_weight': 7}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:08,829]\u001b[0m Trial 414 finished with value: 0.85 and parameters: {'lambda': 0.024677040227719486, 'alpha': 0.022735839282996363, 'gamma': 0.002035359030819888, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 476, 'max_depth': 13, 'min_child_weight': 1}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:10,509]\u001b[0m Trial 415 finished with value: 0.83 and parameters: {'lambda': 0.015407002987533861, 'alpha': 0.20429756251473766, 'gamma': 0.0010878881301706234, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 380, 'max_depth': 13, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:12,210]\u001b[0m Trial 416 finished with value: 0.86 and parameters: {'lambda': 0.07422262660273543, 'alpha': 0.017971902681916313, 'gamma': 0.0015777268701968245, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 456, 'max_depth': 5, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:13,909]\u001b[0m Trial 417 finished with value: 0.81 and parameters: {'lambda': 0.01904414206789297, 'alpha': 0.001417293913532806, 'gamma': 0.00166433999722452, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 468, 'max_depth': 13, 'min_child_weight': 15}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:15,471]\u001b[0m Trial 418 finished with value: 0.87 and parameters: {'lambda': 0.05076038847824833, 'alpha': 0.016063664030951632, 'gamma': 0.0023396125132332745, 'colsample_bytree': 0.4, 'subsample': 0.6, 'learning_rate': 0.300000012, 'n_estimators': 462, 'max_depth': 20, 'min_child_weight': 6}. Best is trial 199 with value: 0.91.\u001b[0m\n",
      "C:\\Users\\004567\\Anaconda3\\envs\\root3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-15 01:34:17,943]\u001b[0m Trial 419 finished with value: 0.82 and parameters: {'lambda': 0.009533107800759453, 'alpha': 0.0012942438443647637, 'gamma': 0.004715182190083166, 'colsample_bytree': 0.9, 'subsample': 0.8, 'learning_rate': 0.014, 'n_estimators': 491, 'max_depth': 13, 'min_child_weight': 11}. Best is trial 199 with value: 0.91.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.load_study(study_name=study_name, storage='sqlite:///m.db',\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    ")\n",
    "study.optimize(objective, n_trials=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
